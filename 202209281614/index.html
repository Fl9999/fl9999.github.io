<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16-next.png">
  <link rel="mask-icon" href="../images/logo.svg" color="#222">

<link rel="stylesheet" href="../css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"csms.tech","root":"/","images":"../images","scheme":"Gemini","darkmode":true,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeIn","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"../search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="../js/config.js"></script>

    <meta name="description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 常见错误总结">
<meta property="og:url" content="http://csms.tech/202209281614/index.html">
<meta property="og:site_name" content="L B T">
<meta property="og:description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.csms.tech/img_102.png">
<meta property="og:image" content="https://i.csms.tech/img_103.png">
<meta property="og:image" content="https://i.csms.tech/img_104.png">
<meta property="og:image" content="https://i.csms.tech/img_110.png">
<meta property="article:published_time" content="2022-09-28T08:14:41.000Z">
<meta property="article:modified_time" content="2024-01-16T03:32:47.000Z">
<meta property="article:author" content="COSMOS">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.csms.tech/img_102.png">


<link rel="canonical" href="http://csms.tech/202209281614/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://csms.tech/202209281614/","path":"202209281614/","title":"kubernetes 常见错误总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kubernetes 常见错误总结 | L B T</title>
  








  <noscript>
    <link rel="stylesheet" href="../css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">L B T</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记 录 过 去 的 经 验</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="../index.html" rel="section"><i class="fa fa-earth-americas fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="../categories/" rel="section"><i class="fa fa-folder-tree fa-fw"></i>总目录<span class="badge">41</span></a></li><li class="menu-item menu-item-linux"><a href="../categories/Linux" rel="section"><i class="fa fa-brands fa-linux fa-fw"></i>Linux</a></li><li class="menu-item menu-item-python"><a href="../categories/Python" rel="section"><i class="fa fa-brands fa-python fa-fw"></i>Python</a></li><li class="menu-item menu-item-docker"><a href="../categories/Docker" rel="section"><i class="fa fa-brands fa-docker fa-fw"></i>Docker</a></li><li class="menu-item menu-item-kubernetes"><a href="../categories/Kubernetes" rel="section"><i class="fa fa-dharmachakra fa-fw"></i>Kubernetes</a></li><li class="menu-item menu-item-tags"><a href="../tags/" rel="section"><i class="fa fa-tornado fa-fw"></i>标签<span class="badge">76</span></a></li><li class="menu-item menu-item-archives"><a href="../archives/" rel="section"><i class="fa fa-rectangle-list fa-fw"></i>列表<span class="badge">201</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章总目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">1.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">2.</span> <span class="nav-text">POD 状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CrashLoopBackOff"><span class="nav-number">2.1.</span> <span class="nav-text">CrashLoopBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ImagePullBackOff"><span class="nav-number">2.2.</span> <span class="nav-text">ImagePullBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E4%B8%BA-InvalidImageName"><span class="nav-number">2.3.</span> <span class="nav-text">POD 状态为 InvalidImageName</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Error"><span class="nav-number">2.4.</span> <span class="nav-text">Pod 状态为 Error</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-node-was-low-on-resource-ephemeral-storage"><span class="nav-number">2.4.1.</span> <span class="nav-text">The node was low on resource: ephemeral-storage</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Init"><span class="nav-number">2.5.</span> <span class="nav-text">Pod 状态为 Init</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unable-to-attach-or-mount-volumes"><span class="nav-number">2.5.1.</span> <span class="nav-text">Unable to attach or mount volumes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ContainerCreating"><span class="nav-number">2.6.</span> <span class="nav-text">ContainerCreating</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dbus-connection-closed-by-user"><span class="nav-number">2.6.1.</span> <span class="nav-text">dbus: connection closed by user</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%9Ccni0%E2%80%9D-already-has-an-IP-address-different-from"><span class="nav-number">2.6.2.</span> <span class="nav-text">“cni0” already has an IP address different from</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PodInitializing"><span class="nav-number">2.7.</span> <span class="nav-text">PodInitializing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Failed-to-update-QoS-cgroup-configuration"><span class="nav-number">2.7.1.</span> <span class="nav-text">Failed to update QoS cgroup configuration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B8%AD-Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Pending"><span class="nav-number">2.8.</span> <span class="nav-text">集群中 Pod 状态为 Pending</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">网络问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84-Pod-%E4%B9%8B%E9%97%B4%E7%BD%91%E7%BB%9C%E4%B8%8D%E9%80%9A"><span class="nav-number">3.1.</span> <span class="nav-text">同一个节点上的 Pod 之间网络不通</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E5%88%B0%E5%A4%96%E9%83%A8-Internet-%E7%BD%91%E7%BB%9C"><span class="nav-number">3.2.</span> <span class="nav-text">Pod 无法访问到外部 Internet 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E9%97%B4%E6%AD%87%E6%80%A7%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">3.3.</span> <span class="nav-text">Pod 间歇性无法连接外部数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B7%A8%E8%8A%82%E7%82%B9-Pod-%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE"><span class="nav-number">3.4.</span> <span class="nav-text">跨节点 Pod 无法访问</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF-1"><span class="nav-number">3.4.1.</span> <span class="nav-text">环境信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coredns-%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D"><span class="nav-number">3.5.</span> <span class="nav-text">coredns 无法解析域名</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dns-%E6%96%87%E4%BB%B6%E5%AE%9A%E4%BD%8D%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"><span class="nav-number">3.5.1.</span> <span class="nav-text">dns 文件定位参考文档</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">4.</span> <span class="nav-text">集群状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81-NotReady"><span class="nav-number">4.1.</span> <span class="nav-text">节点状态 NotReady</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><span class="nav-number">4.1.1.</span> <span class="nav-text">PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E5%8E%9F%E5%9B%A0"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">异常原因</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#container-runtime-is-down-container-runtime-not-ready"><span class="nav-number">4.1.2.</span> <span class="nav-text">container runtime is down, container runtime not ready</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%9CContainer-runtime-network-not-ready%E2%80%9D-networkReady-x3D-%E2%80%9DNetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized%E2%80%9D"><span class="nav-number">4.1.3.</span> <span class="nav-text">“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF-2"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">环境信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Container-runtime-network-not-ready%E2%80%9D-networkReady-x3D-%E2%80%9DNetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized%E2%80%9D"><span class="nav-number">4.1.4.</span> <span class="nav-text">Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#api-server-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">4.2.</span> <span class="nav-text">api-server 启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#No-such-file-or-directory"><span class="nav-number">4.2.1.</span> <span class="nav-text">No such file or directory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#context-deadline-exceeded"><span class="nav-number">4.2.2.</span> <span class="nav-text">context deadline exceeded</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">4.3.</span> <span class="nav-text">kubelet 启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#failed-to-parse-kubelet-flag"><span class="nav-number">4.3.1.</span> <span class="nav-text">failed to parse kubelet flag</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-%E2%80%9C"><span class="nav-number">4.3.2.</span> <span class="nav-text">misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#misconfiguration"><span class="nav-number">4.3.3.</span> <span class="nav-text">misconfiguration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E6%95%85%E9%9A%9C"><span class="nav-number">4.4.</span> <span class="nav-text">其他控制平面故障</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E4%B8%AD%E4%B8%80%E4%B8%AA-master-%E8%8A%82%E7%82%B9%E5%BC%82%E5%B8%B8%E5%AF%BC%E8%87%B4%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E4%B8%8D%E5%8F%AF%E7%94%A8"><span class="nav-number">4.4.1.</span> <span class="nav-text">高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deployment-%E9%83%A8%E7%BD%B2%E5%90%8E%E6%9C%AA%E5%88%9B%E5%BB%BA%E5%AF%B9%E5%BA%94%E7%9A%84-Pod"><span class="nav-number">4.4.2.</span> <span class="nav-text">deployment 部署后未创建对应的 Pod</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#controller-manager-%E5%92%8C-scheduler-%E7%BB%84%E4%BB%B6%E7%8A%B6%E6%80%81-Unhealthy"><span class="nav-number">4.4.3.</span> <span class="nav-text">controller-manager 和 scheduler 组件状态 Unhealthy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF-3"><span class="nav-number">4.4.3.1.</span> <span class="nav-text">环境信息</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">Kubernetes 集群证书相关问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kubernetes-%E7%9B%B8%E5%85%B3%E8%AF%81%E4%B9%A6"><span class="nav-number">5.1.</span> <span class="nav-text">Kubernetes 相关证书</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kubernetes-%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6"><span class="nav-number">5.1.1.</span> <span class="nav-text">Kubernetes 集群证书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#etcd-%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6"><span class="nav-number">5.1.2.</span> <span class="nav-text">etcd 集群证书</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Found-multiple-CRI-endpoints-on-the-host"><span class="nav-number">5.2.</span> <span class="nav-text">Found multiple CRI endpoints on the host</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubeadm-%E6%9B%B4%E6%96%B0-apiserver-%E8%AF%81%E4%B9%A6%E6%8A%A5%E9%94%99"><span class="nav-number">5.3.</span> <span class="nav-text">kubeadm 更新 apiserver 证书报错</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E5%BC%82%E5%B8%B8%E5%AF%BC%E8%87%B4-etcd-%E5%92%8C-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8D%E5%8F%AF%E7%94%A8"><span class="nav-number">5.4.</span> <span class="nav-text">etcd 集群证书异常导致 etcd 和 Kubernetes 集群不可用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6%E5%90%8E%EF%BC%8Ckube-apiserver-%E6%8A%A5%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%94%99%E8%AF%AF"><span class="nav-number">5.4.1.</span> <span class="nav-text">更新证书后，kube-apiserver 报证书过期错误</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ingress-%E6%8E%A5%E5%85%A5%E5%BC%82%E5%B8%B8"><span class="nav-number">6.</span> <span class="nav-text">Ingress 接入异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#503-Service-Temporarily-Unavailable"><span class="nav-number">6.1.</span> <span class="nav-text">503 Service Temporarily Unavailable</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%94%99%E8%AF%AF"><span class="nav-number">7.</span> <span class="nav-text">其他错误</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#invalid-Host-header"><span class="nav-number">7.1.</span> <span class="nav-text">invalid Host header</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">8.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%9A%E6%B3%A8"><span class="nav-number">9.</span> <span class="nav-text">脚注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">COSMOS</p>
  <div class="site-description" itemprop="description">得 能 莫 忘</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="../archives/">
          <span class="site-state-item-count">201</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="../categories/">
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="../tags/">
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://csms.tech/202209281614/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="../images/avatar.gif">
      <meta itemprop="name" content="COSMOS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L B T">
      <meta itemprop="description" content="得 能 莫 忘">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kubernetes 常见错误总结 | L B T">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kubernetes 常见错误总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-28 16:14:41" itemprop="dateCreated datePublished" datetime="2022-09-28T16:14:41+08:00">2022-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-01-16 11:32:47" itemprop="dateModified" datetime="2024-01-16T11:32:47+08:00">2024-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">上层目录</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="../categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><ul>
<li>Centos7 5.4.212-1</li>
<li>Docker 20.10.18</li>
<li>containerd.io-1.6.8</li>
<li>kubectl-1.25.0</li>
<li>kubeadm-1.25.0</li>
<li>kubelet-1.25.0</li>
</ul>
<h1 id="POD-状态异常"><a href="#POD-状态异常" class="headerlink" title="POD 状态异常"></a>POD 状态异常</h1><h2 id="CrashLoopBackOff"><a href="#CrashLoopBackOff" class="headerlink" title="CrashLoopBackOff"></a>CrashLoopBackOff</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                                     READY   STATUS             RESTARTS       AGE</span><br><span class="line">test-centos7-7cc5dc6987-jz486            0/1     CrashLoopBackOff   8 (111s ago)   17m</span><br></pre></td></tr></table></figure>
<p>查看 <code>Pod</code> 详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod test-centos7-7cc5dc6987-jz486</span></span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                  From               Message</span><br><span class="line">  ----     ------     ----                 ----               -------</span><br><span class="line">  Normal   Scheduled  18m                  default-scheduler  Successfully assigned default/test-centos7-7cc5dc6987-jz486 to ops-kubernetes3</span><br><span class="line">  Normal   Pulled     16m (x5 over 18m)    kubelet            Container image &quot;centos:centos7.9.2009&quot; already present on machine</span><br><span class="line">  Normal   Created    16m (x5 over 18m)    kubelet            Created container centos7</span><br><span class="line">  Normal   Started    16m (x5 over 18m)    kubelet            Started container centos7</span><br><span class="line">  Warning  BackOff    3m3s (x71 over 18m)  kubelet            Back-off restarting failed container</span><br></pre></td></tr></table></figure>
<p>结果显示，<code>Reason</code> 为 <code>BackOff</code>，<code>Message</code> 显示 <code>Back-off restarting failed container</code></p>
<p><strong>可能原因</strong> ：</p>
<p><code>Back-off restarting failed container</code> 的原因，通常是因为，容器内 PID 为 1 的进程退出导致（通常用户在构建镜像执行 <code>CMD</code> 时，启动的程序，均是 PID 为1）<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Back-off restarting failed container 怎么办](https://cloud.tencent.com/developer/article/1931089)">[1]</span></a></sup></p>
<p>容器进程退出（命令执行结束或者进程异常结束），则容器生命周期结束。kubernetes 控制器检查到容器退出，会持续重启容器。针对此种情况，需要检查镜像，是否不存在常驻进程，或者常驻进程异常。</p>
<p>针对此种情况，可以单独使用 <code>docker</code> 客户端部署镜像，查看镜像的运行情况，如果部署后，容器中的进程立马结束或退出，则容器也会随之结束。</p>
<p>定位中也可以使用 <code>kubectl describe pod</code> 命令检查 Pod 的退出状态码。Kubernetes 中的 Pod ExitCode 状态码是容器退出时返回的退出状态码，这个状态码通常用来指示容器的执行结果，以便 Kubernetes 和相关工具可以根据它来采取后续的操作。以下是一些常见的 ExitCode 状态码说明：</p>
<ul>
<li><code>ExitCode 0</code> : 这表示容器正常退出，没有错误。这通常是期望的结果。</li>
<li><code>ExitCode 1</code> ： 通常表示容器以非正常方式退出，可能是由于应用程序内部错误或异常导致的。通常是容器中 pid 为 1 的进程错误而失败</li>
<li><code>ExitCode 非零</code> ： 任何非零的状态码都表示容器退出时发生了错误。ExitCode 的具体值通常是自定义的，容器内的应用程序可以根据需要返回不同的状态码来表示不同的错误情况。你需要查看容器内应用程序的文档或日志来了解具体的含义。</li>
<li><code>ExitCode 137</code> ： 通常表示容器因为被操作系统终止（例如，<code>OOM-killer</code>）而非正常退出。这可能是由于内存不足等资源问题导致的。</li>
<li><code>ExitCode 139</code> ： 通常表示容器因为接收到了一个信号而非正常退出。这个信号通常是 <code>SIGSEGV</code>（段错误），表示应用程序试图访问无效的内存。</li>
<li><code>ExitCode 143</code> ： 通常表示容器因为接收到了 <code>SIGTERM</code> 信号而正常退出。这是 Kubernetes 在删除 Pod 时发送的信号，容器应该在接收到该信号后做一些清理工作然后退出。</li>
<li><code>ExitCode 130</code> ： 通常表示容器因为接收到了 <code>SIGINT</code> 信号而正常退出。这是当用户在命令行中按下 <code>Ctrl+C</code> 时发送的信号。</li>
<li><code>ExitCode 255</code> ：通常表示未知错误，或者容器无法启动。这个状态码通常是容器运行时的问题，比如容器镜像不存在或者启动命令有问题。</li>
</ul>
<span id="more"></span>
<h2 id="ImagePullBackOff"><a href="#ImagePullBackOff" class="headerlink" title="ImagePullBackOff"></a>ImagePullBackOff</h2><p>Harbor 证书过期后，更新了证书，<a href="https://csms.tech/202302201557/#SEC_ERROR_UNKNOWN_ISSUER">更新证书后相关问题参考</a>，Kubernetes 中更新 Pod 失败，<em><strong>节点上使用了 <code>containerd</code> 做为 CRI</strong></em>，具体报错信息如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pods -n ops</span></span><br><span class="line">NAME                                   READY   STATUS                  RESTARTS      AGE</span><br><span class="line">get-cloud-cdn-statistics-pjfsc-6jnzl   0/1     Init:ImagePullBackOff   0             2m28s</span><br><span class="line">get-cloud-cdn-statistics-r67s2-qs8kj   0/1     Init:ImagePullBackOff   0             81m</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe pod -n ops get-cloud-cdn-statistics-pjfsc-x9mh7</span></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age               From               Message</span><br><span class="line">  ----     ------     ----              ----               -------</span><br><span class="line">  Normal   Scheduled  17s               default-scheduler  Successfully assigned ops/get-cloud-cdn-statistics-pjfsc-x9mh7 to k8s-worker1</span><br><span class="line">  Normal   BackOff    17s               kubelet            Back-off pulling image &quot;harbor1.mydomain.com/ops/all/cloud-server-cdn-statistics-code:master-0.0-20230207143540&quot;</span><br><span class="line">  Warning  Failed     17s               kubelet            Error: ImagePullBackOff</span><br><span class="line">  Normal   Pulling    2s (x2 over 18s)  kubelet            Pulling image &quot;harbor1.mydomain.com/ops/all/cloud-server-cdn-statistics-code:master-0.0-20230207143540&quot;</span><br><span class="line">  Warning  Failed     2s (x2 over 18s)  kubelet            Failed to pull image &quot;harbor1.mydomain.com/ops/all/cloud-server-cdn-statistics-code:master-0.0-20230207143540&quot;: rpc error: code = Unknown desc = failed to pull and unpack image &quot;harbor1.mydomain.com/ops/all/cloud-server-cdn-statistics-code:master-0.0-20230207143540&quot;: failed to resolve reference &quot;harbor1.mydomain.com/ops/all/cloud-server-cdn-statistics-code:master-0.0-20230207143540&quot;: failed to do request: Head &quot;https://harbor1.mydomain.com/v2/ops/all/cloud-server-cdn-statistics-code/manifests/master-0.0-20230207143540&quot;: x509: certificate signed by unknown authority</span><br><span class="line">  Warning  Failed     2s (x2 over 18s)  kubelet            Error: ErrImagePull</span><br></pre></td></tr></table></figure>

<p>在节点 <code>k8s-worker1</code> 上使用 <code>docker</code> 及 <code>curl</code> 测试访问 Harbor 域名均正常。因此判断问题出现在 <code>containerd</code> 未识别到证书导致。</p>
<p>对于 Kubernetes 使用 <code>containerd</code> 作为容器运行时，如果需要配置额外的证书（如信任自签名的 Harbor 仓库证书），可能需要修改或创建 <code>containerd</code> 的配置文件，通常为 <code>/etc/containerd/config.toml</code>。在配置文件中添加证书配置，<em><strong>在这个示例中，<code>ca_file</code> 应该指向 Harbor 证书。确保该路径正确且证书格式为 PEM</strong></em>。</p>
<figure class="highlight shell"><figcaption><span>/etc/containerd/config.toml</span></figcaption><table><tr><td class="code"><pre><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor1.mydomain.com&quot;.tls]</span><br><span class="line">    ca_file = &quot;/etc/docker/certs.d/harbor1.mydomain.com/ca.crt&quot;</span><br></pre></td></tr></table></figure>
<p>重启 <code>containerd</code> 服务，然后重新部署 Pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure>


<h2 id="POD-状态为-InvalidImageName"><a href="#POD-状态为-InvalidImageName" class="headerlink" title="POD 状态为 InvalidImageName"></a>POD 状态为 InvalidImageName</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>InvalidImageName</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n cs</span><br><span class="line">NAME               READY   STATUS              RESTARTS   AGE</span><br><span class="line">54fdc56754-qrlt6   0/2     InvalidImageName    0          14s</span><br><span class="line">8486f49b89-zp25b   0/2     Init:ErrImagePull   0          7s</span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>镜像的 url 地址中，以 <code>http://</code> 或 <code>https://</code> 开头。配置中镜像的 url 地址中无需指定协议（<code>http://</code> 或 <code>https://</code>） </p>
<h2 id="Pod-状态为-Error"><a href="#Pod-状态为-Error" class="headerlink" title="Pod 状态为 Error"></a>Pod 状态为 Error</h2><h3 id="The-node-was-low-on-resource-ephemeral-storage"><a href="#The-node-was-low-on-resource-ephemeral-storage" class="headerlink" title="The node was low on resource: ephemeral-storage"></a>The node was low on resource: ephemeral-storage</h3><p><strong>错误场景</strong>：</p>
<p>查看 Pod 状态，显示 Error</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS                   RESTARTS   AGE</span><br><span class="line">front-7df8ccc4c7-xhp6s    0/1     Error                    0          5h42m</span><br></pre></td></tr></table></figure>
<p>检查 Pod 的具体信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod front-7df8ccc4c7-xhp6s</span></span><br><span class="line">...</span><br><span class="line">Status:       Failed</span><br><span class="line">Reason:       Evicted</span><br><span class="line">Message:      The node was low on resource: ephemeral-storage. Container php was using 394, which exceeds its request of 0. </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>其中包含异常的关键信息：<code>Status:       Failed</code>，<code>Reason:       Evicted</code>，具体原因为 <code>The node was low on resource: ephemeral-storage</code></p>
<p>检查节点上的 Kuberlet 日志，搜索关键字 <code>evicte</code> 或者 <code>disk</code> ，也可以看到系统上文件系统空间使用率超过了阈值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet  | grep -i -e disk -e evict</span></span><br><span class="line"> image_gc_manager.go:310] &quot;Dis usage on image filesystem is over the high threshold, trying to free bytes down to the low threshold&quot; usage=85 highThreshold=85 amountToFree=5122092236 lowThreshold=80</span><br><span class="line"> eviction_manager.go:349] &quot;Eviction manager: must evict pod(s) to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"> eviction_manager.go:338] &quot;Eviction manager: attempting to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>根据以上信息，可知 Pod 异常是因为 <code>The node was low on resource: ephemeral-storage</code>，表示 <strong>临时存储资源</strong> 不足导致节点处于 <code>Tainted</code> ，其上的 Pod 被驱逐(<code>Evicted</code>)</p>
<p><strong><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">本地临时存储说明</a></strong></p>
<p>针对此种情况，如果某 Pod 的临时存储用量超出了你所允许的范围，kubelet 会向其发出逐出（<code>eviction</code>）信号，触发该 Pod 被逐出所在节点。</p>
<p>如果用于可写入容器镜像层、节点层面日志或者 <code>emptyDir</code> 卷的文件系统中可用空间太少， 节点会为自身设置本地存储不足的污点(<code>Tainted</code>)标签。 这一污点会触发对那些无法容忍该污点的 Pod 的逐出操作。</p>
<p><strong>解决方法</strong> ：</p>
<ul>
<li><p>增加磁盘空间</p>
</li>
<li><p>调整 <code>kubelet</code> 的 <code>nodefs.available</code> 的 threshold 值</p>
<p>  修改节点上的 <code>kubelet</code> 的启动配置文件 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，添加以下启动参数，主要为定义环境变量 <code>KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</code>，并将其添加到启动参数中</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;KUBELET_EVICT_NODEFS_THRESHOLD_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</span><br></pre></td></tr></table></figure>
<p>  修改之后重启 <code>kubelet</code> 服务，并通过日志查看 <code>nodefs.available</code> 的新值是否生效</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart kubelet</span><br><span class="line"></span><br><span class="line">$ journalctl -u kubelet | grep -i nodefs</span><br><span class="line">17604 container_manager_linux.go:267] &quot;Creating Container Manager object based on Node Config&quot; nodeConfig=&#123;RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:&#123;KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:&#123;&#125;] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[&#123;Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05&#125; GracePeriod:0s MinReclaim:&lt;nil&gt;&#125;]&#125; QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>日志中看到 <code>Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05</code>，表明更改生效。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ephemeral-storage 问题](https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/)">[2]</span></a></sup></p>
</li>
</ul>
<h2 id="Pod-状态为-Init"><a href="#Pod-状态为-Init" class="headerlink" title="Pod 状态为 Init"></a>Pod 状态为 Init</h2><h3 id="Unable-to-attach-or-mount-volumes"><a href="#Unable-to-attach-or-mount-volumes" class="headerlink" title="Unable to attach or mount volumes"></a>Unable to attach or mount volumes</h3><p>Pod 启动异常，查看 Pod 状态为 <code>Init:0/1</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                          READY   STATUS     RESTARTS   AGE</span><br><span class="line">admin-cbb479556-j9qg2    0/1     Init:0/1   0          3m37s</span><br></pre></td></tr></table></figure>
<p>查看 Pod 的详细描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod admin-cbb479556-j9qg2</span></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason       Age    From               Message</span><br><span class="line">  ----     ------       ----   ----               -------</span><br><span class="line">  Normal   Scheduled    3m41s  default-scheduler  Successfully assigned admin-cbb479556-j9qg2 to k8s-work2</span><br><span class="line">  Warning  FailedMount  99s    kubelet            Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[wwwroot kube-api-access-z8745 logs]: timed out waiting for the condition</span><br><span class="line">  Warning  FailedMount  42s    kubelet            MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</span><br><span class="line">Mounting command: mount</span><br><span class="line">Mounting arguments: -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</span><br><span class="line">Output: mount.nfs: Connection timed out</span><br></pre></td></tr></table></figure>

<p>根据 <code>Events</code> 中输出的信息，<code>MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</code>，显示挂载卷失败，输出中包含了挂载卷时使用的命令和参数（<code>mount -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</code>）及命令失败后的返回结果（<code>mount.nfs: Connection timed out</code>）</p>
<p>根据 <code>Events</code> 中的信息，查看配置，发现此卷为 NFS 类型的 PV，根据报错排查，此例原因为 NFS 的服务器地址填写错误，更新 PV 配置中的 NFS Server 的地址后，Pod 正常启动。</p>
<h2 id="ContainerCreating"><a href="#ContainerCreating" class="headerlink" title="ContainerCreating"></a>ContainerCreating</h2><h3 id="dbus-connection-closed-by-user"><a href="#dbus-connection-closed-by-user" class="headerlink" title="dbus: connection closed by user"></a>dbus: connection closed by user</h3><p>更新 <code>DaemonSet</code> 类型的 <code>node_exporter</code>，其中一个节点上的 Pod 未创建成功，状态一直保持在 <code>ContainerCreating</code>，检查 Pod 的详细描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -n prometheus -o wide</span></span><br><span class="line">NAME                         READY   STATUS              RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">node-exporter-glnk5          1/1     Running             0          28h     172.31.8.197    work2       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-kzs2r          1/1     Running             1          28h     172.31.100.86   work1       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-nxz9v          0/1     ContainerCreating   0          5m30s   172.31.100.38   master      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-vpkwt          1/1     Running             0          31m     172.31.100.69   work4       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-wft7v          1/1     Running             0          14m     172.31.14.7     work3       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prometheus-67ccbbd78-zqw9x   1/1     Running             0          46h     10.244.14.75    work2       &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod -n prometheus node-exporter-nxz9v</span></span><br><span class="line">Name:         node-exporter-nxz9v</span><br><span class="line">Namespace:    prometheus</span><br><span class="line">Priority:     0</span><br><span class="line"></span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Pending</span><br><span class="line">IP:           172.31.100.38</span><br><span class="line">IPs:</span><br><span class="line">  IP:           172.31.100.38</span><br><span class="line">Controlled By:  DaemonSet/node-exporter</span><br><span class="line">Containers:</span><br><span class="line">  node-exporter:</span><br><span class="line">    Container ID:  </span><br><span class="line">    Image:         prom/node-exporter</span><br><span class="line">    Image ID:      </span><br><span class="line">    Port:          9100/TCP</span><br><span class="line">    Host Port:     9100/TCP</span><br><span class="line">    Args:</span><br><span class="line">      --path.procfs</span><br><span class="line">      /host/proc</span><br><span class="line">      --path.sysfs</span><br><span class="line">      /host/sys</span><br><span class="line">      --collector.disable-defaults</span><br><span class="line">      --collector.cpu</span><br><span class="line">      --collector.cpufreq</span><br><span class="line">      --collector.meminfo</span><br><span class="line">      --collector.diskstats</span><br><span class="line">      --collector.filesystem</span><br><span class="line">      --collector.filefd</span><br><span class="line">      --collector.loadavg</span><br><span class="line">      --collector.netdev</span><br><span class="line">      --collector.netstat</span><br><span class="line">      --collector.nfs</span><br><span class="line">      --collector.os</span><br><span class="line">      --collector.stat</span><br><span class="line">      --collector.time</span><br><span class="line">      --collector.udp_queues</span><br><span class="line">      --collector.uname</span><br><span class="line">      --collector.xfs</span><br><span class="line">      --collector.netclass</span><br><span class="line">      --collector.vmstat</span><br><span class="line">      --collector.systemd</span><br><span class="line">      --collector.systemd.unit-include</span><br><span class="line">      (sshd|crond|iptables|systemd-journald|kubelet|containerd).service</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       ContainerCreating</span><br><span class="line">    Ready:          False</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                    Age                  From               Message</span><br><span class="line">  ----     ------                    ----                 ----               -------</span><br><span class="line">  Normal   Scheduled                 5m24s                default-scheduler  Successfully assigned prometheus/node-exporter-nxz9v to master</span><br><span class="line">  Warning  FailedCreatePodContainer  1s (x26 over 5m24s)  kubelet            unable to ensure pod container exists: failed to create container for [kubepods besteffort pode526f19a-57d6-417c-ba5a-fb0f232d31c6] : dbus: connection closed by user</span><br></pre></td></tr></table></figure>
<p>错误信息显示为 <code>unable to ensure pod container exists: failed to create container for [kubepods besteffort pode526f19a-57d6-417c-ba5a-fb0f232d31c6] : dbus: connection closed by user</code></p>
<p>查看 <code>kubelet</code> 日志，显示同样的日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet</span></span><br><span class="line">master kubelet[1160]: E0707 14:40:55.036424    1160 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: I0707 14:40:55.036455    1160 qos_container_manager_linux.go:138] &quot;Failed to reserve QoS requests&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: E0707 14:41:00.263041    1160 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: E0707 14:41:00.263152    1160 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: 0cdaf660-bb6a-40ee-99ae-21dff3b55411 cgroups exist and are correctly applied: failed to create container for [kubepods besteffort pod0cdaf660-bb6a-40ee-99ae-21dff3b55411] : dbus: connection closed by user&quot; pod=&quot;prometheus/node-exporter-rcd8x&quot; podUID=0cdaf660-bb6a-40ee-99ae-21dff3b55411</span><br></pre></td></tr></table></figure>
<p>根据以上日志信息，问题原因为 <code>kubelet</code> 和系统服务 <code>dbus</code> 通信异常，可以 <strong>通过重启 <code>kubelet</code> 服务</strong> 的方法解决此问题。</p>
<h3 id="“cni0”-already-has-an-IP-address-different-from"><a href="#“cni0”-already-has-an-IP-address-different-from" class="headerlink" title="“cni0” already has an IP address different from"></a>“cni0” already has an IP address different from</h3><p>集群中创建的 POD 状态一直处于 <code>ContainerCreating</code>，检查 Pod 详细信息</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># kubectl describe pod -n cattle-system cattle-cluster-agent-7d766b5476-hsq45</span><br><span class="line">...</span><br><span class="line">FailedCreatePodSandBox  82s (x4 over 85s)   kubelet            (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;2d58156e838349a79da91e0a6d8bccdec0e62c5f5c9ca6a1c30af6186d6253b1&quot; network for pod &quot;cattle-cluster-agent-7d766b5476-hsq45&quot;: networkPlugin cni failed to set up pod &quot;cattle-cluster-agent-7d766b5476-hsq45_cattle-system&quot; network: failed to delegate add: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.2.1/24</span><br></pre></td></tr></table></figure>
<p>关键信息 <code>failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.2.1/24</code>。</p>
<p>检查节点上的 IP 信息，发现 <code>flannel.1</code> 网段和 <code>cni0</code> 网段不一致。可能因为 <code>flannel</code> 读取的配置错误，<em><strong>重启节点后恢复</strong></em>。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># ip add</span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8951 qdisc noqueue state UNKNOWN group default </span><br><span class="line">    link/ether b2:b1:12:2d:8c:66 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.2.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::b0b1:12ff:fe2d:8c66/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8951 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether ca:88:b1:51:0f:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.0.1/24 brd 10.244.2.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::c888:b1ff:fe51:f02/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<h2 id="PodInitializing"><a href="#PodInitializing" class="headerlink" title="PodInitializing"></a>PodInitializing</h2><p>新部署的 Pod 状态一直处于 <code>PodInitializing</code></p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># kubectl get pods</span><br><span class="line">ops           ops-admin-5656d7bb64-mpqz5               0/2     PodInitializing   0          2m55s</span><br></pre></td></tr></table></figure>
<p>登陆到 Pod 所在节点，检查 <code>kubelet</code> 服务日志</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># journalctl -f -u kubelet</span><br><span class="line">Sep 11 17:09:19 ops-k8s-admin kubelet[22700]: &#123;&quot;cniVersion&quot;:&quot;0.3.1&quot;,&quot;hairpinMode&quot;:true,&quot;ipMasq&quot;:false,&quot;ipam&quot;:&#123;&quot;ranges&quot;:[[&#123;&quot;subnet&quot;:&quot;10.244.3.0/24&quot;&#125;]],&quot;routes&quot;:[&#123;&quot;dst&quot;:&quot;10.244.0.0/16&quot;&#125;],&quot;type&quot;:&quot;host-local&quot;&#125;,&quot;isDefaultGateway&quot;:true,&quot;isGateway&quot;:true,&quot;mtu&quot;:8951,&quot;name&quot;:&quot;cbr0&quot;,&quot;type&quot;:&quot;bridge&quot;&#125;E0911 17:09:19.245283   22700 kuberuntime_manager.go:864] container &amp;Container&#123;Name:php,Image:54.236.67.117:5000/comm/ops-php:20221205093123-,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort&#123;ContainerPort&#123;Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,&#125;,&#125;,Env:[]EnvVar&#123;&#125;,Resources:ResourceRequirements&#123;Limits:ResourceList&#123;&#125;,Requests:ResourceList&#123;&#125;,&#125;,VolumeMounts:[]VolumeMount&#123;VolumeMount&#123;Name:wwwroot,ReadOnly:false,MountPath:/home/www,SubPath:,MountPropagation:nil,SubPathExpr:,&#125;,VolumeMount&#123;Name:uploads,ReadOnly:false,MountPath:/home/www/public/uploads,SubPath:,MountPropagation:nil,SubPathExpr:,&#125;,VolumeMount&#123;Name:log-code,ReadOnly:false,MountPath:/home/www/storage/logs,SubPath:,MountPropagation:nil,SubPathExpr:,&#125;,VolumeMount&#123;Name:kube-api-access-z4cxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,&#125;,&#125;,LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&amp;Lifecycle&#123;PostStart:&amp;Handler&#123;Exec:&amp;ExecAction&#123;Command:[/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart],&#125;,HTTPGet:nil,TCPSocket:nil,&#125;,PreStop:nil,&#125;,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource&#123;&#125;,TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice&#123;&#125;,StartupProbe:nil,&#125; start failed in pod ops-admin-5656d7bb64-kvvmx_ops(a44af28c-3a39-439b-97c1-7e78b03ccd91): PostStartHookError: command &#x27;/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart&#x27; exited with 137: : Exec lifecycle hook ([/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart]) for Container &quot;php&quot; in Pod &quot;ops-admin-5656d7bb64-kvvmx_ops(a44af28c-3a39-439b-97c1-7e78b03ccd91)&quot; failed - error: command &#x27;/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart&#x27; exited with 137: , message: &quot;队列延迟启动，因为.env配置不完善，rows=7，等待Apollo获取配置或手动完善</span><br></pre></td></tr></table></figure>
<p>从日志中可以看到关键错误日志信息： <code>start failed in pod ops-admin-5656d7bb64-kvvmx_ops</code>，<code>exited with 137: : Exec lifecycle hook ([/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart]) for Container &quot;php&quot; in Pod &quot;ops-admin-5656d7bb64-kvvmx_ops(a44af28c-3a39-439b-97c1-7e78b03ccd91)&quot; failed - error: command &#39;/bin/sh -c php /home/www/artisan command:apollo.sync &gt;&gt; apollo.log;php /home/www/artisan queue:restart&#39; exited with 137</code></p>
<p>由此可以判断 Pod 无法正常启动的原因为 Pod 中的容器中的进程执行错误导致。</p>
<h3 id="Failed-to-update-QoS-cgroup-configuration"><a href="#Failed-to-update-QoS-cgroup-configuration" class="headerlink" title="Failed to update QoS cgroup configuration"></a>Failed to update QoS cgroup configuration</h3><p>集群中某个节点上面的 Pod 状态显示为 <code>Init</code> 或者 <code>PodInitializing</code>，其他节点正常，登陆异常节点，检查 <code>kubelet</code> 服务日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">kubelet[26451]: E1109 13:32:04.385251   26451 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">kubelet[26451]: E1109 13:32:04.385307   26451 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: e31980b5-849b-4a95-b93d-983c1df31034 cgroups exist and are correctly applied: failed to create container for [kubepods besteffort pode31980b5-849b-4a95-b93d-983c1df31034] : dbus: connection closed by user&quot; pod=&quot;6fd86565c6-4wn7k&quot; podUID=e31980b5-849b-4a95-b93d-983c1df31034</span><br><span class="line">kubelet[26451]: E1109 13:32:04.385416   26451 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: f9e342d5-9f69-41bc-bb5e-df46c37b7bcd cgroups exist and are correctly applied: failed to create container for [kubepods besteffort podf9e342d5-9f69-41bc-bb5e-df46c37b7bcd] : dbus: connection closed by user&quot; pod=&quot;5bfffd564f-sn82t&quot; podUID=f9e342d5-9f69-41bc-bb5e-df46c37b7bcd</span><br><span class="line">kubelet[26451]: E1109 13:32:04.385777   26451 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">kubelet[26451]: E1109 13:32:04.385962   26451 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: 541c88a3-cf05-40ce-b0db-80bf07f542b6 cgroups exist and are correctly applied: failed to create container for [kubepods besteffort pod541c88a3-cf05-40ce-b0db-80bf07f542b6] : dbus: connection closed by user&quot; pod=&quot;5994c65989-zkn2w&quot; podUID=541c88a3-cf05-40ce-b0db-80bf07f542b6</span><br><span class="line">kubelet[26451]: E1109 13:32:08.385429   26451 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">kubelet[26451]: E1109 13:32:08.385657   26451 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: 255ce122-804c-4bcc-9f12-0a3abce77db5 cgroups exist and are correctly applied: failed to create container for [kubepods besteffort pod255ce122-804c-4bcc-9f12-0a3abce77db5] : dbus: connection closed by user&quot; pod=&quot;67d89cf47f-x4wp7&quot; podUID=255ce122-804c-4bcc-9f12-0a3abce77db5</span><br></pre></td></tr></table></figure>

<p>关键日志 <code>&quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</code>，根据此信息，可能是因为要与 DBus 服务通信更新容器的 QoS cgroup 配置失败。具体来说，<code>kubelet</code> 在尝试更新容器的 QoS cgroup 配置时遇到了 <code>dbus: connection closed by user</code> 错误，并且无法正确创建容器。</p>
<p><strong>这种情况可能是由于系统上的 DBus 服务异常导致的</strong>。本示例中在先后重启了 <code>dbus</code> 服务和 <code>kubelet</code> 服务后问题恢复。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart dbus</span><br><span class="line"></span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<h2 id="集群中-Pod-状态为-Pending"><a href="#集群中-Pod-状态为-Pending" class="headerlink" title="集群中 Pod 状态为 Pending"></a>集群中 Pod 状态为 Pending</h2><p>Kubernetes 集群节点信息如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME                STATUS   ROLES           AGE    VERSION</span><br><span class="line">test-k8s-master1   Ready    control-plane   366d   v1.24.7</span><br><span class="line">test-k8s-master2   Ready    control-plane   366d   v1.24.7</span><br><span class="line">test-k8s-master3   Ready    control-plane   366d   v1.24.7</span><br><span class="line">test-k8s-worker1   Ready    &lt;none&gt;          366d   v1.24.7</span><br><span class="line">test-k8s-worker2   Ready    &lt;none&gt;          366d   v1.24.7</span><br></pre></td></tr></table></figure>
<p>集群中 coredns 状态处于 Pending。通常，<strong>Pod 处于 Pending 状态意味着 Kubernetes 调度程序未能将 Pod 分配给任何节点</strong>。查询 Pod 状态如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pods -n kube-system -o wide</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS      AGE   IP              NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6d4b75cb6d-7np4c              0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-6d4b75cb6d-ckl6f              0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-master1                      1/1     Running   1 (65d ago)   68m   172.31.26.116   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-master2                      1/1     Running   0             68m   172.31.19.164   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-master3                      1/1     Running   0             68m   172.31.21.3     k8s-master3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-master1            1/1     Running   2 (65d ago)   68m   172.31.26.116   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-master2            1/1     Running   4 (65d ago)   68m   172.31.19.164   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-master3            1/1     Running   4 (65d ago)   68m   172.31.21.3     k8s-master3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-master1   1/1     Running   1 (41h ago)   68m   172.31.26.116   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-master2   1/1     Running   0             68m   172.31.19.164   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-master3   1/1     Running   1 (41h ago)   68m   172.31.21.3     k8s-master3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-84l4v                      0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-pfwd5                      0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-qbzq8                      0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-qfplm                      0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-w4t62                      0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-master1            1/1     Running   0             68m   172.31.26.116   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-master2            1/1     Running   0             68m   172.31.19.164   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-master3            1/1     Running   1 (41h ago)   68m   172.31.21.3     k8s-master3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-state-metrics-6d44cbdb56-kv8bm   0/1     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-6cd9f9f4cf-rqlzf       0/2     Pending   0             68m   &lt;none&gt;          &lt;none&gt;        &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>看到除了 <code>kube-controller-manager</code>、<code>kube-scheduler</code>、<code>etcd</code>、<code>kube-apiserver</code> 外，其他 Pod 状态都为 <code>Pending</code>，并且 <code>Node</code> 列显示为 <code>&lt;none&gt;</code>，说明集群未将新创建的 Pod 调度到某个节点上。以 <code>coredns-6d4b75cb6d-7np4c</code> 为例查看其描述信息，<code>Events</code> 中未包含任何事件信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe pod -n kube-system coredns-6d4b75cb6d-7np4c</span></span><br><span class="line">Name:                 coredns-6d4b75cb6d-7np4c</span><br><span class="line">Namespace:            kube-system</span><br><span class="line">Priority:             2000000000</span><br><span class="line">Priority Class Name:  system-cluster-critical</span><br><span class="line">Node:                 &lt;none&gt;</span><br><span class="line">Labels:               k8s-app=kube-dns</span><br><span class="line">                      pod-template-hash=6d4b75cb6d</span><br><span class="line">Annotations:          &lt;none&gt;</span><br><span class="line">Status:               Pending</span><br><span class="line">IP:                   </span><br><span class="line">IPs:                  &lt;none&gt;</span><br><span class="line">Controlled By:        ReplicaSet/coredns-6d4b75cb6d</span><br><span class="line">Containers:</span><br><span class="line">  coredns:</span><br><span class="line">    Image:       k8s.gcr.io/coredns/coredns:v1.8.6</span><br><span class="line">    Ports:       53/UDP, 53/TCP, 9153/TCP</span><br><span class="line">    Host Ports:  0/UDP, 0/TCP, 0/TCP</span><br><span class="line">    Args:</span><br><span class="line">      -conf</span><br><span class="line">      /etc/coredns/Corefile</span><br><span class="line">    Limits:</span><br><span class="line">      memory:  170Mi</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        100m</span><br><span class="line">      memory:     70Mi</span><br><span class="line">    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5</span><br><span class="line">    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /etc/coredns from config-volume (ro)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4hm48 (ro)</span><br><span class="line">Volumes:</span><br><span class="line">  config-volume:</span><br><span class="line">    Type:      ConfigMap (a volume populated by a ConfigMap)</span><br><span class="line">    Name:      coredns</span><br><span class="line">    Optional:  false</span><br><span class="line">  kube-api-access-4hm48:</span><br><span class="line">    Type:                    Projected (a volume that contains injected data from multiple sources)</span><br><span class="line">    TokenExpirationSeconds:  3607</span><br><span class="line">    ConfigMapName:           kube-root-ca.crt</span><br><span class="line">    ConfigMapOptional:       &lt;nil&gt;</span><br><span class="line">    DownwardAPI:             true</span><br><span class="line">QoS Class:                   Burstable</span><br><span class="line">Node-Selectors:              kubernetes.io/os=linux</span><br><span class="line">Tolerations:                 CriticalAddonsOnly op=Exists</span><br><span class="line">                             node-role.kubernetes.io/control-plane:NoSchedule</span><br><span class="line">                             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s</span><br><span class="line">                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s</span><br><span class="line">Events:                      &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>鉴于以上信息，怀疑这可能是集群级别的问题。Pod 调度主要由 <code>kube-scheduler</code> 进行，因此首先查看 <code>kube-scheduler</code> 组件日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system kube-scheduler-fm-k8s-c1-master1 | <span class="built_in">tail</span> -n 20</span></span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Unauthorized</span><br><span class="line">leaderelection.go:330] error retrieving resource lock kube-system/kube-scheduler: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Unauthorized</span><br><span class="line">leaderelection.go:330] error retrieving resource lock kube-system/kube-scheduler: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Unauthorized</span><br><span class="line">leaderelection.go:330] error retrieving resource lock kube-system/kube-scheduler: Unauthorized</span><br><span class="line">leaderelection.go:330] error retrieving resource lock kube-system/kube-scheduler: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Unauthorized</span><br><span class="line">reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Unauthorized</span><br><span class="line">reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Unauthorized</span><br><span class="line">leaderelection.go:330] error retrieving resource lock kube-system/kube-scheduler: Unauthorized</span><br></pre></td></tr></table></figure>
<p><em><strong>根据日志内容，显示 <code>kube-scheduler</code> 无法获取到集群资源，原因为 <code>Unauthorized</code>。</strong></em></p>
<p>一般 <code>Unauthorized</code> 常见原因可能是因为 RBAC 或者证书。先检查 RBAC，<code>kube-scheduler</code> 默认使用用户 <code>system:kube-scheduler</code>，下面查看用户绑定的 Role 及权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe clusterrole system:kube-scheduler</span></span><br><span class="line">Name:         system:kube-scheduler</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: true</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                                  Non-Resource URLs  Resource Names    Verbs</span><br><span class="line">  ---------                                  -----------------  --------------    -----</span><br><span class="line">  events                                     []                 []                [create patch update]</span><br><span class="line">  events.events.k8s.io                       []                 []                [create patch update]</span><br><span class="line">  bindings                                   []                 []                [create]</span><br><span class="line">  endpoints                                  []                 []                [create]</span><br><span class="line">  pods/binding                               []                 []                [create]</span><br><span class="line">  tokenreviews.authentication.k8s.io         []                 []                [create]</span><br><span class="line">  subjectaccessreviews.authorization.k8s.io  []                 []                [create]</span><br><span class="line">  leases.coordination.k8s.io                 []                 []                [create]</span><br><span class="line">  pods                                       []                 []                [delete get list watch]</span><br><span class="line">  namespaces                                 []                 []                [get list watch]</span><br><span class="line">  nodes                                      []                 []                [get list watch]</span><br><span class="line">  persistentvolumeclaims                     []                 []                [get list watch]</span><br><span class="line">  persistentvolumes                          []                 []                [get list watch]</span><br><span class="line">  replicationcontrollers                     []                 []                [get list watch]</span><br><span class="line">  services                                   []                 []                [get list watch]</span><br><span class="line">  replicasets.apps                           []                 []                [get list watch]</span><br><span class="line">  statefulsets.apps                          []                 []                [get list watch]</span><br><span class="line">  replicasets.extensions                     []                 []                [get list watch]</span><br><span class="line">  poddisruptionbudgets.policy                []                 []                [get list watch]</span><br><span class="line">  csidrivers.storage.k8s.io                  []                 []                [get list watch]</span><br><span class="line">  csinodes.storage.k8s.io                    []                 []                [get list watch]</span><br><span class="line">  csistoragecapacities.storage.k8s.io        []                 []                [get list watch]</span><br><span class="line">  endpoints                                  []                 [kube-scheduler]  [get update]</span><br><span class="line">  leases.coordination.k8s.io                 []                 [kube-scheduler]  [get update]</span><br><span class="line">  pods/status                                []                 []                [patch update]</span><br><span class="line"><span class="meta prompt_">  </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe clusterrolebinding system:kube-scheduler</span></span><br><span class="line">Name:         system:kube-scheduler</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: true</span><br><span class="line">Role:</span><br><span class="line">  Kind:  ClusterRole</span><br><span class="line">  Name:  system:kube-scheduler</span><br><span class="line">Subjects:</span><br><span class="line">  Kind  Name                   Namespace</span><br><span class="line">  ----  ----                   ---------</span><br><span class="line">  User  system:kube-scheduler</span><br></pre></td></tr></table></figure>
<p>查看 RBAC 权限，并无异常。检查集群证书，集群证书已经更新过，显示正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs check-expiration</span></span><br><span class="line">[check-expiration] Reading configuration from the cluster...</span><br><span class="line">[check-expiration] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line"></span><br><span class="line">CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED</span><br><span class="line">admin.conf                 Dec 07, 2024 06:05 UTC   364d            ca                      no      </span><br><span class="line">apiserver                  Dec 07, 2024 07:17 UTC   364d            ca                      no      </span><br><span class="line">apiserver-etcd-client      Dec 07, 2024 07:15 UTC   364d            etcd-ca                 no      </span><br><span class="line">apiserver-kubelet-client   Dec 07, 2024 07:15 UTC   364d            ca                      no      </span><br><span class="line">controller-manager.conf    Dec 07, 2024 06:05 UTC   364d            ca                      no      </span><br><span class="line">etcd-healthcheck-client    Dec 07, 2024 07:15 UTC   364d            etcd-ca                 no      </span><br><span class="line">etcd-peer                  Dec 07, 2024 07:15 UTC   364d            etcd-ca                 no      </span><br><span class="line">etcd-server                Dec 07, 2024 07:15 UTC   364d            etcd-ca                 no      </span><br><span class="line">front-proxy-client         Dec 07, 2024 07:15 UTC   364d            front-proxy-ca          no      </span><br><span class="line">scheduler.conf             Dec 07, 2024 06:05 UTC   364d            ca                      no      </span><br><span class="line"></span><br><span class="line">CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED</span><br><span class="line">ca                      Dec 03, 2032 09:50 UTC   8y              no      </span><br><span class="line">etcd-ca                 Dec 05, 2033 07:15 UTC   9y              no      </span><br><span class="line">front-proxy-ca          Dec 05, 2033 07:15 UTC   9y              no</span><br></pre></td></tr></table></figure>

<p>查看下 <code>kube-apiserver</code> 日志信息，从日志中看到连接 <code>etcd</code> （<code>127.0.0.1:2379</code>）异常，主要为证书问题，并且 <code>kube-apiserver</code> 日志中显示证书并未更新。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: internal error&quot;. Reconnecting...</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-08T08:40:26Z is after 2023-12-06T09:58:58Z, verifying certificate SN=4790061324473323615, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-08T08:40:26Z is after 2023-12-06T09:58:58Z]&quot;</span><br></pre></td></tr></table></figure>

<p>检查 <code>etcd</code> 日志，日志中显示找不到证书：<code>open /etc/kubernetes/pki/etcd/peer.crt: no such file or directory</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-12-08T07:54:23.780Z&quot;,&quot;caller&quot;:&quot;embed/config_logging.go:169&quot;,&quot;msg&quot;:&quot;rejected connection&quot;,&quot;remote-addr&quot;:&quot;172.31.21.3:30426&quot;,&quot;server-name&quot;:&quot;&quot;,&quot;error&quot;:&quot;open /etc/kubernetes/pki/etcd/peer.crt: no such file or directory&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-12-08T07:54:24.195Z&quot;,&quot;caller&quot;:&quot;embed/config_logging.go:169&quot;,&quot;msg&quot;:&quot;rejected connection&quot;,&quot;remote-addr&quot;:&quot;172.31.19.164:28650&quot;,&quot;server-name&quot;:&quot;&quot;,&quot;error&quot;:&quot;open /etc/kubernetes/pki/etcd/peer.crt: no such file or directory&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>堆叠（Stack）高可用模式下 <code>etcd</code> 组件启动时会挂载 Master 节点的 <code>/etc/kubernetes/pki/etcd/</code> 目录作为自己的证书文件，具体配置可以查看静态 Pod 的配置 <code>/etc/kubernetes/manifests/</code></p>
<figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/etcd.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    component: etcd</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: etcd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - etcd</span><br><span class="line">    - --advertise-client-urls=https://172.31.19.164:2379</span><br><span class="line">    - --cert-file=/etc/kubernetes/pki/etcd/server.crt</span><br><span class="line">    - --client-cert-auth=true</span><br><span class="line">    - --data-dir=/var/lib/etcd</span><br><span class="line">    - --experimental-initial-corrupt-check=true</span><br><span class="line">    - --initial-advertise-peer-urls=https://172.31.19.164:2380</span><br><span class="line">    - --initial-cluster=k8s-master1=https://172.31.26.116:2380,k8s-master3=https://172.31.21.3:2380,k8s-master2=https://172.31.19.164:2380</span><br><span class="line">    - --initial-cluster-state=existing</span><br><span class="line">    - --key-file=/etc/kubernetes/pki/etcd/server.key</span><br><span class="line">    - --listen-client-urls=https://127.0.0.1:2379,https://172.31.19.164:2379</span><br><span class="line">    - --listen-metrics-urls=http://0.0.0.0:2381</span><br><span class="line">    - --listen-peer-urls=https://172.31.19.164:2380</span><br><span class="line">    - --name=k8s-master2</span><br><span class="line">    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt</span><br><span class="line">    - --peer-client-cert-auth=true</span><br><span class="line">    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key</span><br><span class="line">    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    - --snapshot-count=10000</span><br><span class="line">    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    image: k8s.gcr.io/etcd:3.5.3-0</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /var/lib/etcd</span><br><span class="line">      name: etcd-data</span><br><span class="line">    - mountPath: /etc/kubernetes/pki/etcd</span><br><span class="line">      name: etcd-certs</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  priorityClassName: system-node-critical</span><br><span class="line">  securityContext:</span><br><span class="line">    seccompProfile:</span><br><span class="line">      type: RuntimeDefault</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/pki/etcd</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etcd-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /var/lib/etcd</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etcd-data</span><br></pre></td></tr></table></figure>

<p>登陆到 <code>etcd</code> 的容器中，检查目录 <code>/etc/kubernetes/pki/etcd</code>，发现下面为空，没有文件。原因未找到，重启系统后挂载正常。</p>
<h1 id="网络问题"><a href="#网络问题" class="headerlink" title="网络问题"></a>网络问题</h1><h2 id="同一个节点上的-Pod-之间网络不通"><a href="#同一个节点上的-Pod-之间网络不通" class="headerlink" title="同一个节点上的 Pod 之间网络不通"></a>同一个节点上的 Pod 之间网络不通</h2><p><strong>问题现象</strong>：</p>
<p>同一个节点上的 <code>Pod</code> 之间网络不通</p>
<p><strong>排查思路</strong>：</p>
<ul>
<li>检查系统内核配置是否开启转发 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sysctl -a | grep net.ipv4.ip_forward</span></span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure></li>
<li>检查 <code>iptables</code> 是否禁止转发，<a href="https://csms.tech/202209121102/#集群中所有计算机之间具有完全的网络连接"><code>iptables</code> 防火墙配置参考</a></li>
<li>为了定位是否为 <code>iptables</code> 影响，开关闭 <code>iptables</code> 再进行测试，如果关闭防火墙后可以通信，可以确定是防火墙规则导致，需要检查防火墙规则。</li>
<li>更深入的排查，可以部署 <a target="_blank" rel="noopener" href="https://hub.docker.com/r/antrea/netshoot/tags"><code>netshoot</code> 容器</a> 进行抓包定位，</li>
</ul>
<h2 id="Pod-无法访问到外部-Internet-网络"><a href="#Pod-无法访问到外部-Internet-网络" class="headerlink" title="Pod 无法访问到外部 Internet 网络"></a>Pod 无法访问到外部 Internet 网络</h2><p>某个节点上，Pod 无法外部主机的服务（端口 6603&#x2F;tcp）。分别在 Pod ，节点 <code>cni0</code> 网卡，节点出口网卡 <code>eth0</code> ，目标服务网卡上抓包。此例中 Pod IP 为 <code>10.244.4.173</code>，目标服务的 IP 地址为 <code>50.18.6.225</code></p>
<p>查看 Pod 抓包结果</p>
<p><img src="https://i.csms.tech/img_102.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看 节点 <code>cni0</code> 网卡 的抓包</p>
<p><img src="https://i.csms.tech/img_103.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看节点出口网卡 <code>eth0</code> 的抓包。</p>
<p><img src="https://i.csms.tech/img_104.png"></p>
<p><strong>此处看到的源 IP 依然是  Pod 的 IP 地址，此处存在问题</strong>。在云主机的场景中，如果数据包以这种结构发送出去，数据包到了 Internet 网关将拒绝它，因为网关 NAT（将 VM 的 IP 转换为公网 IP） 只了解连接到 VM 的 IP 地址。</p>
<p>正常情况下，Pod 的流量到节点的出口网卡之前，是应该经过 <code>iptables</code> 执行源 NAT - <strong>更改数据包源，使数据包看起来来自 VM 而不是 Pod</strong>。有了正确的源 IP，数据包才可以离开 VM 进入 Internet</p>
<p>此种情况下，数据包可以从节点的出口网卡发送出去，但是到了 Internet 网关将会被丢弃，因此目标服务无法接收到请求，查看目标服务器上的抓包，确实未收到来自此 Pod 的请求。</p>
<p>此处的 <strong>源 NAT</strong> 是由 <code>iptables</code>  负责执行，流入节点出口网卡的数据包未被正确的 <strong>源 NAT</strong>，有可能是因为 <code>kube-proxy</code> 维护的网络规则错误，或者因为 <code>iptables</code> 规则配置错误。可以通过重启 <code>kube-proxy</code> （由服务 <code>kubelet</code> 管理）和 <code>iptables</code> 服务尝试恢复。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart iptables</span><br></pre></td></tr></table></figure>
<p>本示例中，重启这 2 个服务后，Pod 恢复正常。</p>
<h2 id="Pod-间歇性无法连接外部数据库"><a href="#Pod-间歇性无法连接外部数据库" class="headerlink" title="Pod 间歇性无法连接外部数据库"></a>Pod 间歇性无法连接外部数据库</h2><p>集群中的 Pod 出现连接集群之外的数据库服务超时，且出现频率较高</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42684642/article/details/105775436">参考文章</a></p>
<h2 id="跨节点-Pod-无法访问"><a href="#跨节点-Pod-无法访问" class="headerlink" title="跨节点 Pod 无法访问"></a>跨节点 Pod 无法访问</h2><h3 id="环境信息-1"><a href="#环境信息-1" class="headerlink" title="环境信息"></a>环境信息</h3><ul>
<li>Centos 7 5.4.242-1</li>
<li>Kubernetes v1.25.4</li>
<li>kubernetes-cni-1.2.0-0</li>
<li>flannel v0.21.4</li>
</ul>
<p>集群中有 1 个 master 节点， 2 个 work 节点，节点状态均正常，master 无法 ping worker1 上面的 Pod，可以 ping 通 worker2 节点上面的 Pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -A -o wide</span></span><br><span class="line">NAME          STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">k8s-master1   Ready    control-plane   23h   v1.25.4   192.168.142.10   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker1   Ready    &lt;none&gt;          23h   v1.25.4   192.168.142.11   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker2   Ready    &lt;none&gt;          22h   v1.25.4   192.168.142.12   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A -o wide</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS        AGE   IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   1 (4m17s ago)   28m   10.244.1.4       k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0               50m   10.244.2.3       k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 10.244.1.4</span></span><br><span class="line">PING 10.244.1.4 (10.244.1.4) 56(84) bytes of data.</span><br><span class="line">^C</span><br><span class="line">--- 10.244.1.4 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 1001ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 10.244.2.3</span></span><br><span class="line">PING 10.244.2.3 (10.244.2.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=1 ttl=63 time=4.27 ms</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=2 ttl=63 time=0.468 ms</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=3 ttl=63 time=0.443 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.244.2.3 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2056ms</span><br><span class="line">rtt min/avg/max/mdev = 0.443/1.729/4.277/1.801 ms</span><br></pre></td></tr></table></figure>
<p>由此可判断问题大概率出现在 worker1 节点，首先检查 worker1 节点上的 <code>flannel</code> 容器是否正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A -o wide</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS      AGE    IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   0             9m8s   10.244.1.4       k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0             31m    10.244.2.3       k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-d42lm                 1/1     Running   0             22h    192.168.142.11   k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-lqp5v                 1/1     Running   0             22h    192.168.142.10   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-w675f                 1/1     Running   0             70m    192.168.142.12   k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>看到 worker1 节点上的 flannel 容器运行正常。在 worker1 节点上检查 <code>flannel</code> 进程及端口信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">netstat -anutp</span></span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      1817/kubelet        </span><br><span class="line">tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      2224/kube-proxy     </span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1039/sshd           </span><br><span class="line">tcp        0      0 192.168.142.11:47468    192.168.142.10:6443     ESTABLISHED 2224/kube-proxy     </span><br><span class="line">tcp        0      0 192.168.142.11:56584    192.168.142.10:6443     ESTABLISHED 1817/kubelet        </span><br><span class="line">tcp        0     44 192.168.142.11:22       192.168.142.1:62099     ESTABLISHED 1108/sshd: root@pts </span><br><span class="line">tcp        0      0 192.168.142.11:40574    10.96.0.1:443           ESTABLISHED 2566/flanneld       </span><br><span class="line">tcp6       0      0 :::34939                :::*                    LISTEN      1433/cri-dockerd    </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      1817/kubelet        </span><br><span class="line">tcp6       0      0 :::10256                :::*                    LISTEN      2224/kube-proxy     </span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1039/sshd</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ps -elf | grep flannel</span></span><br><span class="line">4 S root       2566   2539  0  80   0 - 353654 futex_ 14:44 ?       00:00:02 /opt/bin/flanneld --ip-masq --kube-subnet-mgr</span><br></pre></td></tr></table></figure>
<p>检查发现 <code>flanneld</code> 进程存在，但是端口未启动，检查 <code>flannel</code> 容器日志输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a | grep flannel | grep -v <span class="string">&quot;Exited&quot;</span></span></span><br><span class="line">92fab879c75b   11ae74319a21                 &quot;/opt/bin/flanneld -…&quot;   34 minutes ago      Up 34 minutes                           k8s_kube-flannel_kube-flannel-ds-77bwd_kube-flannel_078dde8c-573b-4db4-939e-d3dd353477f7_1</span><br><span class="line">237a82c1378a   registry.k8s.io/pause:3.6    &quot;/pause&quot;                 34 minutes ago      Up 34 minutes                           k8s_POD_kube-flannel-ds-77bwd_kube-flannel_078dde8c-573b-4db4-939e-d3dd353477f7_1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker logs 92fab879c75b</span></span><br><span class="line">failed to add vxlanRoute</span><br><span class="line"></span><br><span class="line">network is down</span><br></pre></td></tr></table></figure>
<p>关键错误信息 <code>failed to add vxlanRoute</code>, <code>network is down</code>，<a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/issues/844">参考案例</a>，重启服务器。恢复正常。</p>
<h2 id="coredns-无法解析域名"><a href="#coredns-无法解析域名" class="headerlink" title="coredns 无法解析域名"></a>coredns 无法解析域名</h2><p>Pod 中无法解析域名。</p>
<p>集群相关信息如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get svc -A</span></span><br><span class="line">NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">default       kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP                  25h</span><br><span class="line">kube-system   kube-dns     ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   25h</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在容器中测试 dns 相关信息，访问外部 IP 和 Kubernetes API Server 的 Service 地址均正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 8.8.8.8</span></span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=1 ttl=127 time=37.2 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=2 ttl=127 time=36.9 ms</span><br><span class="line">^C</span><br><span class="line">--- 8.8.8.8 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1001ms</span><br><span class="line">rtt min/avg/max/mdev = 36.946/37.085/37.224/0.139 ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -v 10.96.0.1:443</span></span><br><span class="line">* About to connect() to 10.96.0.1 port 443 (#0)</span><br><span class="line">*   Trying 10.96.0.1...</span><br><span class="line">* Connected to 10.96.0.1 (10.96.0.1) port 443 (#0)</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GET / HTTP/1.1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">User-Agent: curl/7.29.0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Host: 10.96.0.1:443</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Accept: */*</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">* HTTP 1.0, assume close after body</span></span><br><span class="line">&lt; HTTP/1.0 400 Bad Request</span><br><span class="line">&lt; </span><br><span class="line">Client sent an HTTP request to an HTTPS server.</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure>
<p>容器中的 dns 配置为 <code>kube-dns</code> 的 Service 的 IP，测试其端口，显示 <code>Connection refused</code>。测试解析集群内部域名，结果无法解析。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/resolv.conf</span> </span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl 10.96.0.10:53</span></span><br><span class="line">curl: (7) Failed connect to 10.96.0.10:53; Connection refused</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping svc.cluster.local</span></span><br><span class="line">ping: svc.cluster.local: Name or service not known</span><br></pre></td></tr></table></figure>
<p>通过以上步骤，大概可以确定，Pod 的网络正常，应该是 <code>kube-dns</code> 出问题，导致 Pod 无法解析域名。</p>
<p>Service 是通过 Endpoint 和后端的具体的 Pod 关联起来向外提供服务，首先检查 <code>kube-dns</code> 的 Service 对应的 Endpoint，看是否正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ep -A</span></span><br><span class="line">NAMESPACE     NAME         ENDPOINTS             AGE</span><br><span class="line">default       kubernetes   192.168.142.10:6443   25h</span><br><span class="line">kube-system   kube-dns                           25h</span><br></pre></td></tr></table></figure>
<p>检查发现，<code>kube-dns</code> 对应的 ENDPOINTS 列表为空。删除 <code>coredns</code> 容器，重新创建。再次检查后，发现 <code>kube-dns</code> 的 Service 对应的 Endpoint 恢复正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete pod -n kube-system coredns-565d847f94-bzr62 coredns-565d847f94-vmddh</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS       AGE</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   1 (115m ago)   139m</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0              162m</span><br><span class="line">kube-flannel   kube-flannel-ds-77bwd                 1/1     Running   1 (115m ago)   129m</span><br><span class="line">kube-flannel   kube-flannel-ds-lqp5v                 1/1     Running   0              25h</span><br><span class="line">kube-flannel   kube-flannel-ds-w675f                 1/1     Running   0              3h21m</span><br><span class="line">kube-system    coredns-565d847f94-8wmg7              1/1     Running   0              9s</span><br><span class="line">kube-system    coredns-565d847f94-csc9f              0/1     Running   0              9s</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ep -A</span></span><br><span class="line">NAMESPACE     NAME         ENDPOINTS                                     AGE</span><br><span class="line">default       kubernetes   192.168.142.10:6443                           25h</span><br><span class="line">kube-system   kube-dns     10.244.1.7:53,10.244.1.7:53,10.244.1.7:9153   25h</span><br></pre></td></tr></table></figure>

<p>在 Pod 中重新测试解析，结果正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -v 10.96.0.10:53</span></span><br><span class="line">* About to connect() to 10.96.0.10 port 53 (#0)</span><br><span class="line">*   Trying 10.96.0.10...</span><br><span class="line">* Connected to 10.96.0.10 (10.96.0.10) port 53 (#0)</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GET / HTTP/1.1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">User-Agent: curl/7.29.0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Host: 10.96.0.10:53</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Accept: */*</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">* Empty reply from server</span></span><br><span class="line">* Connection #0 to host 10.96.0.10 left intact</span><br><span class="line">curl: (52) Empty reply from server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping qq.com</span></span><br><span class="line">PING qq.com (61.129.7.47) 56(84) bytes of data.</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=1 ttl=127 time=308 ms</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=2 ttl=127 time=312 ms</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=3 ttl=127 time=312 ms</span><br><span class="line">^C</span><br><span class="line">--- qq.com ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 308.493/310.873/312.106/1.743 ms</span><br></pre></td></tr></table></figure>

<h3 id="dns-文件定位参考文档"><a href="#dns-文件定位参考文档" class="headerlink" title="dns 文件定位参考文档"></a>dns 文件定位参考文档</h3><p><a target="_blank" rel="noopener" href="https://www.gylinux.cn/4299.html">故障排查：Kubernetes 中 Pod 无法正常解析域名</a></p>
<h1 id="集群状态异常"><a href="#集群状态异常" class="headerlink" title="集群状态异常"></a>集群状态异常</h1><h2 id="节点状态-NotReady"><a href="#节点状态-NotReady" class="headerlink" title="节点状态 NotReady"></a>节点状态 NotReady</h2><h3 id="PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><a href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago" class="headerlink" title="PLEG is not healthy: pleg was last seen active 10m13.755045415s ago"></a>PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION</span><br><span class="line">k8s-master1   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master3   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-work1     NotReady   &lt;none&gt;          14d   v1.24.7</span><br><span class="line">k8s-work2     Ready      &lt;none&gt;          14d   v1.24.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看节点详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-work1</span></span><br><span class="line">...</span><br><span class="line">Conditions:</span><br><span class="line">  Ready                False   Tue, 15 Nov 2022 10:14:49 +0800   Tue, 15 Nov 2022 10:07:39 +0800   KubeletNotReady              PLEG is not healthy: pleg was last seen active 10m13.755045415s ago; threshold is 3m0s</span><br></pre></td></tr></table></figure>

<h4 id="异常原因"><a href="#异常原因" class="headerlink" title="异常原因"></a>异常原因</h4><p>集群因为此原因（<code>PLEG is not healthy: pleg was last seen active ***h**m***s ago;</code>）状态变为 <code>NotReady</code>，通常是因为节点超负载。</p>
<h3 id="container-runtime-is-down-container-runtime-not-ready"><a href="#container-runtime-is-down-container-runtime-not-ready" class="headerlink" title="container runtime is down, container runtime not ready"></a>container runtime is down, container runtime not ready</h3><p><strong>排查过程</strong>：</p>
<p>检查集群中的 Pod 分布情况时，发现某一节点上几乎所有的 Pod 都被调度去了其他节点，当前检查时此节点的状态已经是 <code>Ready</code>，针对此情况进行分析。</p>
<ol>
<li><p>确定问题发生的大概时间段</p>
<p> 根据 Pod 在其他节点上面被启动的时间，可以大概确定节点异常的时间，根据此时间段可以缩小排查的时间范围。此示例中问题发生的时间大概在 <code>Nov 25 04:49:00</code> 前后。</p>
</li>
<li><p>检查 <code>kubelet</code> 日志</p>
<p> 根据已经推断出的时间段，在 <strong>问题节点</strong> 上，检查 <code>kubelet</code> 日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ journalctl -u kubelet --since &quot;2022-11-25 4:40&quot; | grep -v -e &quot;failed to get fsstats&quot; -e &quot;invalid bearer token&quot; | more</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.153132   17604 generic.go:205] &quot;GenericPLEG: Unable to retrieve pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375524   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;PodSandboxFilter&#123;Id:,State:&amp;PodSandboxStateValue&#123;State:SANDBOX_READY,&#125;,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375559   17604 kuberuntime_sandbox.go:292] &quot;Failed to list pod sandboxes&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375578   17604 kubelet_pods.go:1153] &quot;Error listing containers&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375589   17604 kubelet.go:2162] &quot;Failed cleaning pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.476011   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.507861   17604 remote_runtime.go:680] &quot;ExecSync cmd from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; containerID=&quot;5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac&quot; cmd=[wget --no-check-certificate --spider -q http://localhost:8088/health]</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.676271   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.076918   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329558   17604 remote_runtime.go:536] &quot;ListContainers with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;ContainerFilter&#123;Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329585   17604 container_log_manager.go:183] &quot;Failed to rotate container logs&quot; err=&quot;failed to list containers: rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: E1125 04:49:09.485356   17604 remote_runtime.go:168] &quot;Version from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: I1125 04:49:09.485486   17604 setters.go:532] &quot;Node became not ready&quot; node=&quot;k8s-work2&quot; condition=&#123;Type:Ready Status:False LastHeartbeatTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 LastTransitionTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p> 从以上日志中，可以看到关键的日志信息：</p>
<p> <code>&quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</code></p>
<p> <code>setters.go:532] &quot;Node became not ready&quot;</code>，    <code>Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</code></p>
<p> 从以上日志信息可以看出，节点状态变为了 <code>not ready</code>，原因为 <code>container runtime is down, container runtime not ready</code>，本示例中 <code>container runtime</code> 为 <code>docker</code></p>
</li>
<li><p>检查 docker 服务日志</p>
<p> 根据上面的日志时间，检查 docker 服务的日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">journalctl -u docker --since &quot;2022-11-25 04:0&quot; | more</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410342223+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/41e0dfe97b87c2b8ae941653fa8adbf93bf9358d91e967646e4549ab71b2f004/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.414773158+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.416474238+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.422844592+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure>
<p>根据日志可以看到关键日志 <code>write unix /var/run/docker.sock-&gt;@: write: broken pipe</code></p>
</li>
<li><p>检查 messages 日志</p>
<p> 查看对应时间段的系统日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.153089   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;nil&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375614   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>根据 <code>kubelet</code> 服务日志，节点 <code>Not Ready</code> 的原因为 <code>docker down</code>，根据 docker 服务日志，docker 存在异常，但是此时执行 <code>docker</code> 相关命令，未发现异常。此问题多次出现，<code>docker engine</code> 版本为 <code>19.03.15-3</code>，之后尝试将 <code>docker engine</code> 版本升级为最新版本 <code>20.10.9</code>，问题未在出现。<a href="https://csms.tech/202208041317/#docker-ce-19-03-15-升级到-docker-ce-20-10-9"><code>docker engine</code> 升级参考</a> </p>
<h3 id="“Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”"><a href="#“Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”" class="headerlink" title="“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”"></a>“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</h3><h4 id="环境信息-2"><a href="#环境信息-2" class="headerlink" title="环境信息"></a>环境信息</h4><ul>
<li>Kubernetes v1.21.2</li>
</ul>
<p>新增节点后，节点状态为 <code>NotReady</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME            STATUS     ROLES                  AGE     VERSION</span><br><span class="line">work2           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work3           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work4           Ready      &lt;none&gt;                 10d     v1.21.2</span><br><span class="line">work5           NotReady   &lt;none&gt;                 8m36s   v1.21.2</span><br><span class="line">master          Ready      control-plane,master   191d    v1.21.2</span><br></pre></td></tr></table></figure>
<p>Master 上查看节点的描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-api-work5</span> </span><br><span class="line">Name:               work5</span><br><span class="line">Roles:              &lt;none&gt;</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=work5</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line"></span><br><span class="line">Taints:             node.kubernetes.io/not-ready:NoExecute</span><br><span class="line">                    node.kubernetes.io/not-ready:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">Lease:</span><br><span class="line">  HolderIdentity:  work5</span><br><span class="line">  AcquireTime:     &lt;unset&gt;</span><br><span class="line">  RenewTime:       Wed, 05 Apr 2023 13:50:16 +0800</span><br><span class="line">Conditions:</span><br><span class="line">  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----                 ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  NetworkUnavailable   False   Wed, 05 Apr 2023 13:48:19 +0800   Wed, 05 Apr 2023 13:48:19 +0800   FlannelIsUp                  Flannel is running on this node</span><br><span class="line">  MemoryPressure       False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure         False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure          False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready                False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span><br></pre></td></tr></table></figure>
<p>看到异常原因为 <code>container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</code></p>
<p>在 <code>work5</code> 节点上查看 <code>kubelet</code> 日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet -f</span></span><br><span class="line">Apr 05 13:52:03 work5 kubelet[19520]: E0405 13:52:03.952395   19520 kubelet.go:2211] &quot;Container runtime network not ready&quot; networkReady=&quot;NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized&quot;</span><br><span class="line"></span><br><span class="line">Apr 05 13:52:08 work5 kubelet[19520]: I0405 13:52:08.498481   19520 cni.go:204] &quot;Error validating CNI config list&quot; configList=&quot;&#123;\n  \&quot;name\&quot;: \&quot;cbr0\&quot;,\n  \&quot;cniVersion\&quot;: \&quot;0.3.1\&quot;,\n  \&quot;plugins\&quot;: [\n    &#123;\n      \&quot;type\&quot;: \&quot;flannel\&quot;,\n      \&quot;delegate\&quot;: &#123;\n        \&quot;hairpinMode\&quot;: true,\n        \&quot;isDefaultGateway\&quot;: true\n      &#125;\n    &#125;,\n    &#123;\n      \&quot;type\&quot;: \&quot;portmap\&quot;,\n      \&quot;capabilities\&quot;: &#123;\n        \&quot;portMappings\&quot;: true\n      &#125;\n    &#125;\n  ]\n&#125;\n&quot; err=&quot;[failed to find plugin \&quot;flannel\&quot; in path [/opt/cni/bin]]&quot;</span><br><span class="line">Apr 05 13:52:08 work5 kubelet[19520]: I0405 13:52:08.498501   19520 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no valid networks found in /etc/cni/net.d&quot;</span><br></pre></td></tr></table></figure>

<p>在 Master 节点上查看异常节点上的 <code>kube-flannel</code> POD 状态正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide -n kube-system</span></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-558bd4d5db-6wf7m         1/1     Running   0          18d    10.244.4.132    admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-558bd4d5db-zh9mw         1/1     Running   0          18d    10.244.4.144    admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-master                      1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master            1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master   1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-2lg9x            1/1     Running   0          18d    192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-5fpn8            1/1     Running   0          10d    192.168.100.69   work4       	  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-7ln98            1/1     Running   0          30m    192.168.100.59   work5   		  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-kvhhq            1/1     Running   0          17d    192.168.14.7     work3           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-vz4th            1/1     Running   0          17d    192.168.8.197    work2           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-xr84k            1/1     Running   0          18d    192.168.100.86   admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-9b7kt                 1/1     Running   0          30m    192.168.100.59   work5           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-c6ggk                 1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-gtlqt                 1/1     Running   0          17d    192.168.14.7     work3           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-n6s7p                 1/1     Running   0          10d    192.168.100.69   work4           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-p8m9d                 1/1     Running   0          17d    192.168.8.197    work2           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-qvks4                 1/1     Running   2          191d   192.168.100.86   admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master            1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>接着检查新增节点上提供 <code>flannel</code> 组件的安装包，及相关目录中的文件是否存在异常 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kubernetes</span></span><br><span class="line">kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /opt/cni/bin/</span></span><br><span class="line">bandwidth  dhcp   firewall     host-local  loopback  portmap  sbr     tuning  vrf</span><br><span class="line">bridge     dummy  host-device  ipvlan      macvlan   ptp      static  vlan</span><br></pre></td></tr></table></figure>

<p>比对其他已存在的正常节点上的 <code>kubernetes-cni</code> 信息，发现其他节点上的 <code>kubernetes-cni</code> 版本为 <code>kubernetes-cni-0.8.7-0</code>，怀疑为版本问题导致，卸载问题节点上的 <code>kubernetes-cni-1.2.0-0</code>，重新安装 <code>kubernetes-cni-0.8.7-0</code>。卸载 <code>kubernetes-cni</code> 会导致之前安装的 <code>kubeadm</code> 和 <code>kubelet</code> 被卸载，也需要重新安装。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum remove kubernetes-cni-1.2.0-0</span></span><br><span class="line">...</span><br><span class="line">Removed:</span><br><span class="line">  kubernetes-cni.x86_64 0:1.2.0-0                                                                                                      </span><br><span class="line"></span><br><span class="line">Dependency Removed:</span><br><span class="line">  kubeadm.x86_64 0:1.21.2-0                                          kubelet.x86_64 0:1.21.2-0</span><br><span class="line"><span class="meta prompt_">  </span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install -y kubelet-1.21.2 kubeadm-1.21.2 kubectl-1.21.2 kubernetes-cni-0.8.7-0</span></span><br></pre></td></tr></table></figure>

<p>安装 <code>kubernetes-cni-0.8.7-0</code> 版本后，再次查看节点状态，变为 <code>Ready</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME            STATUS     ROLES                  AGE     VERSION</span><br><span class="line">work2           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work3           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work4           Ready      &lt;none&gt;                 10d     v1.21.2</span><br><span class="line">work5           Ready      &lt;none&gt;                 8m36s   v1.21.2</span><br><span class="line">master          Ready      control-plane,master   191d    v1.21.2</span><br></pre></td></tr></table></figure>

<h3 id="Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”"><a href="#Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”" class="headerlink" title="Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”"></a>Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</h3><p>节点状态 NotReady，检查节点上的 kubelet日志，显示 <code>Container runtime network not ready&quot; networkReady=&quot;NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized&quot;</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">k8s-master1   Ready      control-plane   35m   v1.25.4   192.168.142.10   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker1   Ready      &lt;none&gt;          30m   v1.25.4   192.168.142.11   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker2   NotReady   &lt;none&gt;          21m   v1.25.4   192.168.142.12   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br></pre></td></tr></table></figure>

<p>检查发现节点上没有 CNI 配置文件 <code>/etc/cni/net.d/10-flannel.conflist</code>，拷贝正常节点上的配置到异常节点后，状态恢复正常。</p>
<h2 id="api-server-启动失败"><a href="#api-server-启动失败" class="headerlink" title="api-server 启动失败"></a>api-server 启动失败</h2><h3 id="No-such-file-or-directory"><a href="#No-such-file-or-directory" class="headerlink" title="No such file or directory"></a>No such file or directory</h3><p>api server 启动失败，执行 <code>kubectl</code> 命令输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">The connection to the server kube-apiserver:6443 was refused - did you specify the right host or port?</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>检查 Api Server 监听的端口 6443 ，显示端口未启动。</p>
<p>检查 Api Server 对应的容器状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a | grep api</span></span><br><span class="line">81688b9cbe45  1f38c0b6a9d1   &quot;kube-apiserver --ad…&quot;   14 seconds ago      Exited (1) 13 seconds ago                       k8s_kube-apiserver_kube-apiserver-k8s-uat-master1.kube-system_c8a87f4921623c7bff57f5662ea486cc_25</span><br></pre></td></tr></table></figure>

<p>容器状态为 <code>Exited</code>，检查容器日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker logs 81688b9cbe45</span></span><br><span class="line">I1116 07:43:53.775588       1 server.go:558] external host was not specified, using 172.31.30.123</span><br><span class="line">I1116 07:43:53.776035       1 server.go:158] Version: v1.24.7</span><br><span class="line">I1116 07:43:53.776057       1 server.go:160] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot;</span><br><span class="line">E1116 07:43:53.776298       1 run.go:74] &quot;command failed&quot; err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</span><br></pre></td></tr></table></figure>
<p>日志显示 <code>err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</code>，检查文件 <code>/etc/kubernetes/pki/apiserver.crt</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line">ls: cannot access /etc/kubernetes/pki/apiserver.crt: No such file or directory</span><br></pre></td></tr></table></figure>

<p>解决方法参考</p>
<ul>
<li>发现此文件确实不存在。若有备份，从备份中恢复此文件。如果没有备份，<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1801882">参考文档</a> 恢复证书<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /k8s/backup/pki/apiserver.key /etc/kubernetes/pki/</span><br><span class="line">cp /k8s/backup/pki/apiserver.crt /etc/kubernetes/pki/</span><br></pre></td></tr></table></figure>
重启 <code>kubelet</code> 后检查 Api Server，发现服务正常启动<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>如果只是缺少了 <code>apiserver.key</code>，<code>apiserver.crt</code> 证书文件，可通过以下命令重新生成证书文件，<a href="https://csms.tech/202209121102/#集群之外的服务器使用-kubectl-报错">生成原理参考</a><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm init phase certs apiserver \</span></span><br><span class="line"><span class="language-bash">     --apiserver-advertise-address  10.150.0.21 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans  10.96.0.1 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans 34.150.1.1</span></span><br><span class="line"> </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.21 34.150.1.1]</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="context-deadline-exceeded"><a href="#context-deadline-exceeded" class="headerlink" title="context deadline exceeded"></a>context deadline exceeded</h3><p><code>kube-apiserver</code> 无法正常启动，检查 <code>kube-apiserver</code> 相关容器日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">The connection to the server kube-apiserver:6443 was refused - did you specify the right host or port?</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker logs  -f f39205f67e71</span></span><br><span class="line">server.go:558] external host was not specified, using 172.31.29.250</span><br><span class="line">server.go:158] Version: v1.24.7</span><br><span class="line">server.go:160] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot;</span><br><span class="line">shared_informer.go:255] Waiting for caches to sync for node_authorizer</span><br><span class="line">plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.</span><br><span class="line">plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.</span><br><span class="line">plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.</span><br><span class="line">plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.</span><br><span class="line">run.go:74] &quot;command failed&quot; err=&quot;context deadline exceeded&quot;</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: context deadline exceeded&quot;. Reconnecting...</span><br></pre></td></tr></table></figure>
<p>查看到日志中的关键错误信息: <code>&quot;command failed&quot; err=&quot;context deadline exceeded&quot;</code>，<code>[core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: context deadline exceeded&quot;. Reconnecting...</code>，由此可知，主要问题在于连接 <code>etcd</code> 组件异常，因 <code>transport: authentication handshake failed</code> 无法和 <code>etcd</code> 建立连接，<code>kube-apiserver</code> 连接 <code>etcd</code> 的认证依赖于证书，因此去检查集群证书，发现证书过期，相关操作参考以下命令及输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span> /etc/kubernetes/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">openssl x509 -text -<span class="keyword">in</span> apiserver.crt</span> </span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 3 (0x2)</span><br><span class="line">        Serial Number: 2154708302505735210 (0x1de70fc8f570742a)</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: CN=kubernetes</span><br><span class="line">        Validity</span><br><span class="line">            Not Before: Nov  1 01:11:01 2022 GMT</span><br><span class="line">            Not After : Nov  1 01:24:48 2023 GMT</span><br><span class="line">        Subject: CN=kube-apiserver</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs check-expiration</span></span><br><span class="line">[check-expiration] Reading configuration from the cluster...</span><br><span class="line">[check-expiration] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration</span><br><span class="line"></span><br><span class="line">CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED</span><br><span class="line">admin.conf                 Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">apiserver                  Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">apiserver-etcd-client      Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">apiserver-kubelet-client   Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">controller-manager.conf    Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">etcd-healthcheck-client    Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">etcd-peer                  Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">etcd-server                Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">front-proxy-client         Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       front-proxy-ca          no      </span><br><span class="line">scheduler.conf             Nov 01, 2023 01:24 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line"></span><br><span class="line">CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED</span><br><span class="line">ca                      Oct 29, 2032 01:11 UTC   8y              no      </span><br><span class="line">etcd-ca                 Oct 29, 2032 01:11 UTC   8y              no      </span><br><span class="line">front-proxy-ca          Oct 29, 2032 01:11 UTC   8y              no</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参考以下命令，备份及更新集群证书，并重启 <code>kubelet</code> 后，<code>kube-apiserver</code> 恢复正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">tar -cf /etc/kubernetes/kubernetes.20231115.tar /etc/kubernetes/kubernetes</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs renew all</span></span><br><span class="line">[renew] Reading configuration from the cluster...</span><br><span class="line">[renew] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">[renew] Error reading configuration from the Cluster. Falling back to default configuration</span><br><span class="line"></span><br><span class="line">certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed</span><br><span class="line">certificate for serving the Kubernetes API renewed</span><br><span class="line">certificate the apiserver uses to access etcd renewed</span><br><span class="line">certificate for the API server to connect to kubelet renewed</span><br><span class="line">certificate embedded in the kubeconfig file for the controller manager to use renewed</span><br><span class="line">certificate for liveness probes to healthcheck etcd renewed</span><br><span class="line">certificate for etcd nodes to communicate with each other renewed</span><br><span class="line">certificate for serving etcd renewed</span><br><span class="line">certificate for the front proxy client renewed</span><br><span class="line">certificate embedded in the kubeconfig file for the scheduler manager to use renewed</span><br><span class="line"></span><br><span class="line">Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl restart kubelet</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">error: You must be logged in to the server (Unauthorized)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE    VERSION</span><br><span class="line">k8s-master1   Ready      control-plane   379d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   379d   v1.24.7</span><br><span class="line">k8s-master3   Ready      control-plane   379d   v1.24.7</span><br><span class="line">k8s-work1     NotReady   &lt;none&gt;          379d   v1.24.7</span><br><span class="line">k8s-work2     Ready      &lt;none&gt;          379d   v1.24.7</span><br></pre></td></tr></table></figure>

<h2 id="kubelet-启动失败"><a href="#kubelet-启动失败" class="headerlink" title="kubelet 启动失败"></a>kubelet 启动失败</h2><h3 id="failed-to-parse-kubelet-flag"><a href="#failed-to-parse-kubelet-flag" class="headerlink" title="failed to parse kubelet flag"></a>failed to parse kubelet flag</h3><p><code>kubelet</code> 重启失败，查看 <code>kubelet</code> 服务日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">&quot;command failed&quot; err=&quot;failed to parse kubelet flag: unknown flag: --network-plugin&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>问题原因</strong> 为 <strong>版本不匹配</strong>。集群版本为 <code>1.21.2</code>，检查问题节点上的组件版本信息，<code>kubelet</code> 变为了 <code>kubelet-1.27.3</code>，可能是升级了 <code>kubelet</code> 软件包版本导致。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kube</span></span><br><span class="line">kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line">kubectl-1.25.2-0.x86_64</span><br><span class="line">kubelet-1.27.3-0.x86_64</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看正常节点上的组件版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kube</span></span><br><span class="line">kubectl-1.21.2-0.x86_64</span><br><span class="line">kubernetes-cni-0.8.7-0.x86_64</span><br><span class="line">kubelet-1.21.2-0.x86_64</span><br><span class="line">kubeadm-1.21.2-0.x86_64</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>解决方法</strong> 为恢复问题节点上的组件版本和集群版本一致。</p>
<ol>
<li>在正常节点上下载软件安装包并拷贝到问题节点上 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yumdownloader kubernetes-cni-0.8.7-0 kubectl-1.21.2-0 kubeadm-1.21.2-0 kubelet-1.21.2-0</span><br></pre></td></tr></table></figure></li>
<li>在问题节点上卸载软件包并安装和集群一致版本的软件包 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum remove kubelet-1.27.3-0 kubectl-1.25.2-0.x86_64 kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line"></span><br><span class="line">yum localinstall kubectl-1.21.2-0.x86_64.rpm  kubeadm-1.21.2-0.x86_64.rpm kubelet-1.21.2-0.x86_64.rpm kubernetes-cni-0.8.7-0.x86_64.rpm</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>重启服务，恢复正常 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-“"><a href="#misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-“" class="headerlink" title="misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“"></a>misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“</h3><p><code>kubelet</code> 服务启动失败</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl status kubelet</span></span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /usr/lib/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br><span class="line">   Active: activating (auto-restart) (Result: exit-code) since Thu 2023-07-13 14:18:33 CST; 2s ago</span><br><span class="line">     Docs: https://kubernetes.io/docs/</span><br><span class="line">  Process: 28476 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 28476 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Jul 13 14:18:33 k8s-node-5 systemd[1]: Unit kubelet.service entered failed state.</span><br><span class="line">Jul 13 14:18:33 k8s-node-5 systemd[1]: kubelet.service failed.</span><br></pre></td></tr></table></figure>

<p>检查 <code>kubelet</code> 服务日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">Jul 13 14:18:43 k8s-node-5 kubelet[28572]: E0713 14:18:43.328660   28572 server.go:292] &quot;Failed to run kubelet&quot; err=&quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&quot;systemd\&quot; is different from docker cgroup driver: \&quot;cgroupfs\&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>根据日志信息可知，<code>kubelet</code> 服务启动失败是因为 <code>kubelet</code> 使用的 <code>cgroup</code> 驱动是 <code>systemd</code>，而 docker 服务使用的是 <code>cgroupfs</code>。二者不一致导致 <code>kubelet</code> 无法启动。</p>
<p>修改 <code>docker</code> 服务使用的 <code>cgroup</code> 驱动为 <code>systemd</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF | sudo tee /etc/docker/daemon.json</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>重启 <code>docker</code> 服务后，<code>kubelet</code> 服务运行正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<h3 id="misconfiguration"><a href="#misconfiguration" class="headerlink" title="misconfiguration"></a>misconfiguration</h3><p><code>kubelet</code> 服务启动失败，检查日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">kubelet[22771]: E0908 14:10:08.325316   22771 server.go:292] &quot;Failed to run kubelet&quot; err=&quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&quot;cgroupfs\&quot; is different from docker cgroup driver: \&quot;systemd\&quot;&quot;</span><br></pre></td></tr></table></figure>
<p>从日志可知，是因为 <code>kubelet</code> 和 <code>docker</code> 使用的 cgroup 驱动不一致导致。<code>kubelet</code> 使用了 <code>cgroupfs</code>，而 <code>docker</code> 使用了 <code>systemd</code></p>
<p>参考以下步骤修改 <code>kubelet</code> 使用 <code>systemd</code> 驱动</p>
<ol>
<li>修改 <code>kubelet</code> 配置 <code>/var/lib/kubelet/config.yaml</code> 中的 <code>cgroupDriver</code> <figure class="highlight shell"><figcaption><span>/var/lib/kubelet/config.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">cgroupDriver: systemd</span><br></pre></td></tr></table></figure></li>
<li>重启 <code>kubelet</code> 服务</li>
</ol>
<h2 id="其他控制平面故障"><a href="#其他控制平面故障" class="headerlink" title="其他控制平面故障"></a>其他控制平面故障</h2><h3 id="高可用集群中一个-master-节点异常导致整个集群中的所有应用服务不可用"><a href="#高可用集群中一个-master-节点异常导致整个集群中的所有应用服务不可用" class="headerlink" title="高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用"></a>高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用</h3><a href="/202209121102/" title="参考此文档安装的高可用集群">参考此文档安装的高可用集群</a>，在 [创建高可用控制平面集群](https://csms.tech/202209121102/#创建高可用控制平面的集群)中，要 为 `kube-apiserver` 创建负载均衡器，本次问题环境中未创建负载均衡器，而是使用主机 hosts (`/etc/hosts`) 写入了 `kube-apiserver` 的域名及 IP 映射关系，示例如下

<figure class="highlight shell"><figcaption><span>/etc/hosts</span></figcaption><table><tr><td class="code"><pre><span class="line">172.31.26.116 k8s-master1 kube-api-svr-c1.mydomain.com</span><br><span class="line">172.31.19.164 k8s-master2 kube-api-svr-c1.mydomain.com</span><br><span class="line">172.31.21.3 k8s-master3 kube-api-svr-c1.mydomain.com</span><br><span class="line">172.31.16.124 k8s-worker1</span><br><span class="line">172.31.22.159 k8s-worker2</span><br></pre></td></tr></table></figure>
<p>集群中的所有节点都写入了以上内容，实现了节点通过 <code>/etc/hosts</code> 中的配置解析 <code>kube-apiserver</code> 的域名(<code>kube-api-svr-c1.mydomain.com</code>)。</p>
<p>正常情况下，所有节点都会将 <code>kube-apiserver</code> 解析为 <code>172.31.26.116 kube-api-svr-c1.mydomain.com</code>。</p>
<p>本次故障中，<code>k8s-master1</code> 节点因为 CPU 和内存满载，<code>k8s-master1</code> 异常，无法提供 <code>kube-apiserver</code> 服务。同时，集群中所有的应用都响应异常 ：<code>503 Service Temporarily Unavailable</code></p>
<p>理论上，本环境为 <em>高可用</em> 的Kubernetes 集群，一个 master 节点异常，不会影响集群提供正常的功能，但是此次只是因为 <code>k8s-master1</code> 这一个 master 节点异常，就导致了整个集群无法提供正常的功能。</p>
<p>复现问题分析，发现 <code>k8s-master1</code> 异常后，在其他主节点上使用 <code>kubectl</code> 命令，无法使用，原因为 <em>根据节点 <code>/etc/hosts</code> 配置，<code>kube-apiserver</code> 解析到了异常的 <code>k8s-master1</code> 节点</em>，修改 <code>k8s-master2</code> 上面的 <code>/etc/hosts</code> 配置，将 <code>172.31.19.164 k8s-master2 kube-api-svr-c1.mydomain.com</code> 放在第一行，以使 <code>kube-apiserver</code> 域名解析到 <code>k8s-master2</code>，<code>kubectl</code> 命令连接到 <code>k8s-master2</code> 即可正常查看集群状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE    VERSION</span><br><span class="line">k8s-master1   NotReady   control-plane   224d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   224d   v1.24.7</span><br><span class="line">k8s-master3   NotReady   control-plane   224d   v1.24.7</span><br><span class="line">k8s-worker1   NotReady   &lt;none&gt;          224d   v1.24.7</span><br><span class="line">k8s-worker2   NotReady   &lt;none&gt;          224d   v1.24.7</span><br></pre></td></tr></table></figure>
<p>可以看到，集群中除了 <code>k8s-master2</code> ，其他所有节点都异常，原因为 <strong>除了 <code>k8s-master2</code> 其他节点都将 <code>apiserver</code> 解析到了异常的 <code>k8s-master1</code> 节点</strong>。修改所有节点的 <code>/etc/hosts</code> 文件，将 <code>kube-api-svr-c1.mydomain.com</code> 解析到 <code>k8s-master1</code> 之外的 master 节点，集群恢复正常。</p>
<p>本次故障的根本原因为<em><strong>本环境中未实现 Kubernetes 的高可用，虽然已经部署了高可用环境，但是因为 <code>apiserver</code> 的域名未实现高可用(负载均衡)，导致 <code>apiserver</code> 请求全部到了异常节点</strong></em>。要从根本上解决此问题，需要为 <code>apiserver</code> 的请求域名部署负载均衡实现真正的高可用。<em><strong>使用 <code>/etc/hosts</code> 将域名解析到多个 IP 不能实现高可用</strong></em>。</p>
<h3 id="deployment-部署后未创建对应的-Pod"><a href="#deployment-部署后未创建对应的-Pod" class="headerlink" title="deployment 部署后未创建对应的 Pod"></a>deployment 部署后未创建对应的 Pod</h3><p>deployment 部署成功后，检查对应的 Pod，发现未创建对应的 Pod。查看 <code>deployment</code> 描述信息如下，<code>Events</code> 内容为空。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe deployment ops-admin -n ops</span></span><br><span class="line">Name:                   ops-admin</span><br><span class="line">Namespace:              ops</span><br><span class="line">Labels:                 env=prod</span><br><span class="line">                        project=ops-admin</span><br><span class="line">Selector:               env=prod,project=ops-admin</span><br><span class="line">Replicas:               1 desired | 0 updated | 0 total | 0 available | 0 unavailable</span><br><span class="line">StrategyType:           RollingUpdate</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  env=prod</span><br><span class="line">           project=ops-admin</span><br><span class="line">... </span><br><span class="line">OldReplicaSets:    &lt;none&gt;</span><br><span class="line">NewReplicaSet:     &lt;none&gt;</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>检查 <code>replicaset</code> 信息，未发现任何的相关 <code>replicaset</code> 资源</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe replicaset -l <span class="built_in">env</span>=prod,project=ops-admin -n ops</span></span><br><span class="line">No resources found in ops namespace.</span><br></pre></td></tr></table></figure>

<p>检查事件列表，内容为空</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get events -n ops</span></span><br><span class="line">No resources found in ops namespace.</span><br></pre></td></tr></table></figure>

<p>继续检查控制平面组件的状态，看到 <code>controller-manager</code> 和 <code>scheduler</code> 状态异常(此异常可能是因为配置原因，具体见 <a href="#controller-manager-%E5%92%8C-scheduler-%E7%BB%84%E4%BB%B6%E7%8A%B6%E6%80%81-Unhealthy">controller-manager 和 scheduler 组件状态 Unhealthy</a>)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get componentstatuses</span></span><br><span class="line">Warning: v1 ComponentStatus is deprecated in v1.19+</span><br><span class="line">NAME                 STATUS      MESSAGE                                                                                       ERROR</span><br><span class="line">controller-manager   Unhealthy   Get &quot;http://127.0.0.1:10252/healthz&quot;: dial tcp 127.0.0.1:10252: connect: connection refused   </span><br><span class="line">scheduler            Unhealthy   Get &quot;http://127.0.0.1:10251/healthz&quot;: dial tcp 127.0.0.1:10251: connect: connection refused   </span><br><span class="line">etcd-0               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>接着查看 <code>controller-manager</code> 和 <code>scheduler</code> 对应的 Pod 的日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system -l component=kube-controller-manager</span></span><br><span class="line">E0925 04:44:40.190504       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:44:43.861798       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:44:48.017384       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:44:52.260888       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:44:54.825907       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:44:58.502754       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:45:02.539845       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:45:06.623081       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:45:09.166492       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line">E0925 04:45:12.507228       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system -l component=kube-scheduler</span></span><br><span class="line">E0925 04:45:30.669008       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Unauthorized</span><br><span class="line">E0925 04:45:30.986953       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Unauthorized</span><br><span class="line">E0925 04:45:31.297694       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Unauthorized</span><br><span class="line">E0925 04:45:36.275267       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Unauthorized</span><br><span class="line">E0925 04:45:36.839025       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Unauthorized</span><br><span class="line">E0925 04:45:37.365357       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Unauthorized</span><br><span class="line">E0925 04:45:45.008682       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Unauthorized</span><br><span class="line">E0925 04:45:52.514736       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Unauthorized</span><br><span class="line">E0925 04:45:52.698173       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Unauthorized</span><br><span class="line">E0925 04:45:54.558346       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Unauthorized  </span><br></pre></td></tr></table></figure>
<p>日志显示存在鉴权问题（<code>Unauthorized</code>），本示例中此问题主要是由集群证书过期导致。</p>
<h3 id="controller-manager-和-scheduler-组件状态-Unhealthy"><a href="#controller-manager-和-scheduler-组件状态-Unhealthy" class="headerlink" title="controller-manager 和 scheduler 组件状态 Unhealthy"></a>controller-manager 和 scheduler 组件状态 Unhealthy</h3><h4 id="环境信息-3"><a href="#环境信息-3" class="headerlink" title="环境信息"></a>环境信息</h4><ul>
<li>Kubernetes 1.21</li>
</ul>
<p>检查集群组件状态，显示 <code>controller-manager</code> 和 <code>scheduler</code> 组件状态 <code>Unhealthy</code>。集群功能正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get componentstatuses</span></span><br><span class="line">Warning: v1 ComponentStatus is deprecated in v1.19+</span><br><span class="line">NAME                 STATUS      MESSAGE                                                                                       ERROR</span><br><span class="line">controller-manager   Unhealthy   Get &quot;http://127.0.0.1:10252/healthz&quot;: dial tcp 127.0.0.1:10252: connect: connection refused   </span><br><span class="line">scheduler            Unhealthy   Get &quot;http://127.0.0.1:10251/healthz&quot;: dial tcp 127.0.0.1:10251: connect: connection refused   </span><br><span class="line">etcd-0               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>出现此情况是因为 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> 和 <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code> 设置的默认端口是 <code>0</code>，修改此配置文件，注释 <code>- --port=0</code> 即可（修改此配置文件后，系统会自动加载配置，无需重启），等待大概一分钟后重新查看，状态正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get componentstatuses</span></span><br><span class="line">Warning: v1 ComponentStatus is deprecated in v1.19+</span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Kubernetes-集群证书相关问题"><a href="#Kubernetes-集群证书相关问题" class="headerlink" title="Kubernetes 集群证书相关问题"></a>Kubernetes 集群证书相关问题</h1><h2 id="Kubernetes-相关证书"><a href="#Kubernetes-相关证书" class="headerlink" title="Kubernetes 相关证书"></a>Kubernetes 相关证书</h2><p>Kubernetes 集群中使用了多种证书来保证各种组件间的安全通信。以下是 Kubernetes 中使用的主要证书：</p>
<blockquote>
<p><em><strong>证书默认存放路径</strong></em> <code>/etc/kubernetes/pki</code>，以下文件相对路径基于此默认存放路径</p>
</blockquote>
<h3 id="Kubernetes-集群证书"><a href="#Kubernetes-集群证书" class="headerlink" title="Kubernetes 集群证书"></a>Kubernetes 集群证书</h3><ul>
<li><p><em><strong>kubernetes CA 证书</strong></em></p>
<p>Kubernetes CA 证书用于签发集群中的其他证书，例如 <code>kube-apiserver</code> 服务器证书、<code>kubelet</code> 证书、<code>controller-manager</code> 证书等。</p>
<ul>
<li><code>ca.crt</code>  CA 的公钥证书。</li>
<li><code>ca.key</code>  CA 的私钥。</li>
</ul>
<p>在使用 <code>kubeadm</code> 初始化 Kubernetes 集群时，CA 证书通常会自动生成。</p>
</li>
<li><p><em><strong>kube-apiserver 组件证书</strong></em></p>
<p>加密 Kubernetes API 服务器的 HTTPS 接口。</p>
<ul>
<li><code>apiserver.crt</code></li>
<li><code>apiserver.key</code></li>
</ul>
<p>证书的 SANs 中包含了 Kubernetes API 服务器的 IP 地址、DNS 名称等。要更新 SANs 的值，使用命令 <code>kubeadm init phase certs apiserver</code>，<a href="https://csms.tech/202209121102/#集群之外的服务器使用-kubectl-报错">参考示例</a></p>
<p>要单独更新 <code>kube-apiserver</code> 的证书，参考命令 <code>kubeadm certs renew </code></p>
</li>
<li><p>Kube Proxy、Controller Manager 和 Scheduler 默认不使用 TLS 和 API Server 进行身份验证，而是使用 Kubernetes Service Account。</p>
</li>
<li><p><em><strong>Kubelet  组件证书</strong></em></p>
<p>加密 Kubelet 与 API 服务器的通信。kubelet 使用这些证书来安全地与 Kubernetes API 服务器通信。这包括节点状态的报告、Pod 的创建和管理等。当 kubelet 首次加入集群时，它会使用这些证书进行 TLS 认证。API 服务器通过这些证书验证节点的身份，并基于此授予相应的权限。</p>
<p>Kubernetes 支持自动证书轮换，这意味着 kubelet 会在证书接近过期时自动请求新的证书。这个过程是自动的，确保了长期运行的集群保持安全。</p>
<p>通常为节点的 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 和对应的 Key 文件。</p>
</li>
<li><p><em><strong>Service Account Keys</strong></em>  </p>
<p>在初始化 Kubernetes 集群时，这些密钥通常由 <code>kubeadm</code> 或相似的工具自动生成。</p>
<ul>
<li><code>sa.key</code> 用于签发新的 Service Account JWT Tokens。通常由集群管理员或自动化工具（如 <code>kubeadm</code>）安全地生成和管理。</li>
<li><code>sa.pub</code>（<strong>非 TLS 证书，而是密钥对</strong>）。  由 Kubernetes API 服务器用来验证 Token 的合法性。与私钥配对使用。</li>
</ul>
<p>在 Kubernetes 中，Service Account Keys 是一对公钥和私钥，用于验证和签发 Service Account Tokens。这些令牌允许 Pod 以特定的 Service Account 身份与 Kubernetes API 服务器进行认证和授权。</p>
</li>
<li><p><em><strong>apiserver-kubelet-client</strong></em></p>
<p>这对证书和密钥用于 API 服务器请求集群中每个节点上的 <code>kubelet</code> 时的安全通信。API 服务器使用这些证书与 <code>kubelet</code> 通信，进行诸如启动 Pod、获取节点状态等操作。</p>
<ul>
<li><code>apiserver-kubelet-client.crt</code></li>
<li><code>apiserver-kubelet-client.key</code></li>
</ul>
</li>
</ul>
<h3 id="etcd-集群证书"><a href="#etcd-集群证书" class="headerlink" title="etcd 集群证书"></a>etcd 集群证书</h3><ul>
<li><p><em><strong>etcd CA 证书</strong></em></p>
<p>Etcd CA 证书用于签发 etcd 集群相关的证书，如 <code>etcd</code> 服务器证书、<code>etcd</code> 客户端证书、Peer 实体证书等。</p>
<ul>
<li><code>etcd/ca.crt</code></li>
<li><code>etcd/ca.key</code></li>
</ul>
</li>
<li><p><em><strong>apiserver-etcd-client</strong></em></p>
<p> 这对证书和密钥用于 Kubernetes API 服务器请求 <code>etcd</code> 数据库时的加密通信</p>
<ul>
<li><code>apiserver-etcd-client.crt </code></li>
<li><code>apiserver-etcd-client.key</code></li>
</ul>
</li>
</ul>
<h2 id="Found-multiple-CRI-endpoints-on-the-host"><a href="#Found-multiple-CRI-endpoints-on-the-host" class="headerlink" title="Found multiple CRI endpoints on the host"></a>Found multiple CRI endpoints on the host</h2><p>使用 <code>kubeadm</code> 重置集群证书时报错，具体操作如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sudo kubeadm init phase certs all</span></span><br><span class="line">Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the &#x27;criSocket&#x27; field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure>
<p>原因为系统上存在多个可用的 CRI，需要手动配置使用哪一个。这通常是在 <code>kubeadm</code> 配置文件中设置 <code>criSocket</code> 字段来完成。如果集群正在使用 <code>containerd</code> 作为容器运行时，参考以下配置解决</p>
<ol>
<li>创建 <code>kubeadm</code> 配置文件。创建一个 <code>kubeadm</code> 配置文件（如果还没有的话，如 <code>kubeadm-config.yaml</code>），并在其中指定 <code>criSocket</code> 字段。例如，如果使用 <code>containerd</code>，则文件内容如下： <figure class="highlight shell"><figcaption><span>kubeadm-config.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: /var/run/containerd/containerd.sock</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>运行 <code>kubeadm</code> 命令 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sudo kubeadm init phase certs all --config=./kubeadm-config.yaml</span> </span><br><span class="line">W1208 14:19:51.376721   13737 common.go:84] your configuration file uses a deprecated API spec: &quot;kubeadm.k8s.io/v1beta2&quot;. Please use &#x27;kubeadm config migrate --old-config old.yaml --new-config new.yaml&#x27;, which will write the new, similar spec using a newer API version.</span><br><span class="line">W1208 14:19:51.376972   13737 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme &quot;unix&quot; to the &quot;criSocket&quot; with value &quot;/var/run/containerd/containerd.sock&quot;. Please update your configuration!</span><br><span class="line">I1208 14:19:51.517814   13737 version.go:255] remote version is much newer: v1.28.4; falling back to: stable-1.24</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using existing ca certificate authority</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [fm-k8s-c1-master1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.26.116]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [fm-k8s-c1-master1 localhost] and IPs [172.31.26.116 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [fm-k8s-c1-master1 localhost] and IPs [172.31.26.116 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="kubeadm-更新-apiserver-证书报错"><a href="#kubeadm-更新-apiserver-证书报错" class="headerlink" title="kubeadm 更新 apiserver 证书报错"></a>kubeadm 更新 apiserver 证书报错</h2><p>使用以下方式更新 <code>kube-apiserver</code> 的证书时报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs apiserver --apiserver-advertise-address --apiserver-cert-extra-sans kubernetes --apiserver-cert-extra-sans kubernetes.default --apiserver-cert-extra-sans kubernetes.default.svc  --config=/home/username/kubeadm-config.yaml</span></span><br><span class="line">can not mix &#x27;--config&#x27; with arguments [apiserver-advertise-address apiserver-cert-extra-sans]</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure>
<p>根据报错提示，不能同时使用 <code>--config</code> 和 <code>apiserver-advertise-address apiserver-cert-extra-sans</code>。</p>
<p>本示例中使用 <code>--config</code> 是用来解决 <a href="#Found-multiple-CRI-endpoints-on-the-host">Found multiple CRI endpoints on the host</a>。要简单解决，只需要确保主机上只有 <strong>一个可用的 CRI 即可</strong>。</p>
<h2 id="etcd-集群证书异常导致-etcd-和-Kubernetes-集群不可用"><a href="#etcd-集群证书异常导致-etcd-和-Kubernetes-集群不可用" class="headerlink" title="etcd 集群证书异常导致 etcd 和 Kubernetes 集群不可用"></a>etcd 集群证书异常导致 etcd 和 Kubernetes 集群不可用</h2><p><code>etcd</code> 堆叠(Stack)架构的 Kubernetes 集群无法访问，获取不到集群状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">The connection to the server kube-apiserver.uat.148962587001:6443 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure>
<p>通过 CRI（以 <code>docker</code> 为例）查看 <code>kube-apiserver</code> 容器状态，显示为 <code>Exited</code>。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker ps -a</span></span><br><span class="line">CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS                          PORTS     NAMES</span><br><span class="line">cf4bf9f2c4a3   9efa6dff568f                &quot;kube-scheduler --au…&quot;   About a minute ago   Exited (1) About a minute ago             k8s_kube-scheduler_kube-scheduler-k8s-master3_kube-system_a3a06a8f4bb3d9a7a753421061337314_808</span><br><span class="line">27e06d290dbb   9e2bfc195de6                &quot;kube-controller-man…&quot;   2 minutes ago        Exited (1) 2 minutes ago                  k8s_kube-controller-manager_kube-controller-manager-k8s-master3_kube-system_1d62164acfdda6946d09aa8255b4b191_808</span><br><span class="line">b0aa1a2e24ee   c7cbaca6e63b                &quot;kube-apiserver --ad…&quot;   3 minutes ago        Exited (1) 3 minutes ago                  k8s_kube-apiserver_kube-apiserver-k8s-master3_kube-system_fd413ffd28d3bcce4b1330c38307ebe2_791</span><br></pre></td></tr></table></figure>
<p>查看 <code>kube-apiserver</code> 容器日志信息。错误日志中有关键错误信息 <code>failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: internal error&quot;. Reconnecting...</code>，这显示了 <code>kube-apiserver</code> 因为 TLS 证书原因无法连接到 <code>etcd</code>。 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker logs b0aa1a2e24ee</span></span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: internal error&quot;. Reconnecting...</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-08T07:21:21Z is after 2023-12-06T09:58:58Z, verifying certificate SN=1505375741374655454, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-08T07:21:21Z is after 2023-12-06T09:58:58Z]&quot;</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: internal error&quot;. Reconnecting...</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-08T07:21:22Z is after 2023-12-06T09:58:58Z, verifying certificate SN=1505375741374655454, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-08T07:21:22Z is after 2023-12-06T09:58:58Z]&quot;  </span><br></pre></td></tr></table></figure>

<p>根据日志指向，首先来确定集群中 <code>etcd</code> 的状态。<code>etcd</code> 集群暴露了健康状态检查的 Endpoint，可以通过请求此接口检查 <code>etcd</code> 集群的状态。检查结果显示集群状态不正常，原因为未选举出 Leader </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">curl 127.0.0.1:2381/health</span></span><br><span class="line">&#123;&quot;health&quot;:&quot;false&quot;,&quot;reason&quot;:&quot;RAFT NO LEADER&quot;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>根据当前的排查结果，可以确定 <code>etcd</code> 集群异常，<code>etcd</code> 集群是 Kubernetes 集群的配置核心，其异常会导致整个 Kubernetes 集群不可用。为了确定 <code>etcd</code> 集群异常原因，检查 <code>etcd</code> 集群中节点的日志，以查找有用信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker logs etcd</span></span><br><span class="line">&#123;&quot;logger&quot;:&quot;raft&quot;,&quot;caller&quot;:&quot;etcdserver/zap_raft.go:77&quot;,&quot;msg&quot;:&quot;651e0623614c0f76 is starting a new election at term 13&quot;&#125;</span><br><span class="line">&#123;&quot;logger&quot;:&quot;raft&quot;,&quot;caller&quot;:&quot;etcdserver/zap_raft.go:77&quot;,&quot;msg&quot;:&quot;651e0623614c0f76 became pre-candidate at term 13&quot;&#125;</span><br><span class="line">&#123;&quot;logger&quot;:&quot;raft&quot;,&quot;caller&quot;:&quot;etcdserver/zap_raft.go:77&quot;,&quot;msg&quot;:&quot;651e0623614c0f76 received MsgPreVoteResp from 651e0623614c0f76 at term 13&quot;&#125;</span><br><span class="line">&#123;&quot;logger&quot;:&quot;raft&quot;,&quot;caller&quot;:&quot;etcdserver/zap_raft.go:77&quot;,&quot;msg&quot;:&quot;651e0623614c0f76 [logterm: 13, index: 216516485] sent MsgPreVote request to 71e91a3cb0d95be8 at term 13&quot;&#125;</span><br><span class="line">&#123;&quot;logger&quot;:&quot;raft&quot;,&quot;caller&quot;:&quot;etcdserver/zap_raft.go:77&quot;,&quot;msg&quot;:&quot;651e0623614c0f76 [logterm: 13, index: 216516485] sent MsgPreVote request to d0ca64fcbfb25318 at term 13&quot;&#125;</span><br><span class="line">&#123;&quot;caller&quot;:&quot;rafthttp/probing_status.go:68&quot;,&quot;msg&quot;:&quot;prober detected unhealthy status&quot;,&quot;round-tripper-name&quot;:&quot;ROUND_TRIPPER_SNAPSHOT&quot;,&quot;remote-peer-id&quot;:&quot;d0ca64fcbfb25318&quot;,&quot;rtt&quot;:&quot;0s&quot;,&quot;error&quot;:&quot;x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;&#125;</span><br><span class="line">&#123;&quot;caller&quot;:&quot;rafthttp/probing_status.go:68&quot;,&quot;msg&quot;:&quot;prober detected unhealthy status&quot;,&quot;round-tripper-name&quot;:&quot;ROUND_TRIPPER_RAFT_MESSAGE&quot;,&quot;remote-peer-id&quot;:&quot;d0ca64fcbfb25318&quot;,&quot;rtt&quot;:&quot;0s&quot;,&quot;error&quot;:&quot;x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;&#125;</span><br><span class="line">&#123;&quot;caller&quot;:&quot;rafthttp/probing_status.go:68&quot;,&quot;msg&quot;:&quot;prober detected unhealthy status&quot;,&quot;round-tripper-name&quot;:&quot;ROUND_TRIPPER_RAFT_MESSAGE&quot;,&quot;remote-peer-id&quot;:&quot;71e91a3cb0d95be8&quot;,&quot;rtt&quot;:&quot;0s&quot;,&quot;error&quot;:&quot;x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;&#125;</span><br><span class="line">&#123;&quot;caller&quot;:&quot;rafthttp/probing_status.go:68&quot;,&quot;msg&quot;:&quot;prober detected unhealthy status&quot;,&quot;round-tripper-name&quot;:&quot;ROUND_TRIPPER_SNAPSHOT&quot;,&quot;remote-peer-id&quot;:&quot;71e91a3cb0d95be8&quot;,&quot;rtt&quot;:&quot;0s&quot;,&quot;error&quot;:&quot;x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>检查 <code>etcd</code> 节点容器日志，日志中有报错显示证书认证问题：<code>x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;</code>，根据错误提示，<em><strong>可能是 <code>etcd</code> 集群使用的证书存在问题，具体错误说明 <code>etcd</code> 节点之间在尝试相互验证对方证书时遇到了问题。这通常发生在节点使用不同的 CA 证书，或者配置不正确时。</strong></em>。</p>
<p>通常情况下，在启用了 <code>--client-cert-auth=true</code> 和 <code>--peer-client-cert-auth=true</code> 的 <code>etcd</code> 集群中，<code>etcd</code> 对等节点间需要使用 HTTPS 证书进行加密通信，对等实体间需要验证客户端证书，这个过程中需要相同的 CA 证书进行 CA 机构的签名校验。根据日志提示，问题应集中在 <code>etcd</code> 集群中的证书上。</p>
<p>根据以上日志及思路引导，首先检查 <code>etcd</code> 各个节点的 CA 证书是否一致，结果发现 3 个 <code>etcd</code> 节点上面的 CA 证书不同</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# md5sum /etc/kubernetes/pki/etcd/ca.crt </span><br><span class="line">65fec4a08b77132febabfef3ca4eaafa  /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line"></span><br><span class="line">[root@k8s-master2 ~]# md5sum /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">1b6330a0acacd09dabeff2ff0c97451f  /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line"></span><br><span class="line">[root@k8s-master3 ~]# md5sum /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">3f398e445a4844e6c2a3fee6f24203aa  /etc/kubernetes/pki/etcd/ca.crt</span><br></pre></td></tr></table></figure>

<p>在一个正常配置的 <code>etcd</code> 集群中，所有节点应该使用由同一个根 CA 签发的证书，以便它们能够相互验证和信任。这是因为节点之间的通信（包括 Raft 协议的通信和快照传输）使用 TLS 加密，而且节点需要能够验证彼此的证书。如果每个节点上的 CA 证书不同，它们将无法验证其他节点的证书，从而导致通信失败。</p>
<p>为了解决这个 <code>etcd</code> 集群中各个节点使用的 CA 证书不一致的问题，参考以下步骤</p>
<ol>
<li>备份 Kubernets 集群中的证书目录 <code>/etc/kubernetes/pki/</code>。<em><strong>在所有的 Master 节点上操作</strong></em> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -czf /ops/kubernetes_backup/pki.tar /etc/kubernetes/pki/</span><br></pre></td></tr></table></figure></li>
<li>删除 <code>etcd</code> 节点上的所有证书。<em><strong>在所有的 Master 节点上操作</strong></em> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf /etc/kubernetes/pki/etcd/*</span><br></pre></td></tr></table></figure></li>
<li>使用 <code>kubeadm</code> 重新生成 <code>etcd-ca</code> 证书。<em><strong>只需在一个 Master 节点上操作</strong></em> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm init phase certs etcd-ca</span><br></pre></td></tr></table></figure></li>
<li>将重新生成的 <code>etcd-ca</code> 证书对（证书 <code>/etc/kubernetes/pki/etcd/ca.crt</code> 和私钥 <code>/etc/kubernetes/pki/etcd/ca.key</code>） 拷贝到另外 2 台 <code>etcd</code> 节点的相同路径下（<code>/etc/kubernetes/pki/etcd/</code>）</li>
<li>重新生成 <code>etcd</code> 集群使用的证书文件。<em><strong>在所有的 Master 节点上操作</strong></em>。执行以下操作，基于以上步骤中重新生成的 CA 证书，签发 <code>etcd</code> 集群所需的其他证书 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm init phase certs etcd-healthcheck-client</span><br><span class="line">kubeadm init phase certs etcd-peer</span><br><span class="line">kubeadm init phase certs etcd-server</span><br></pre></td></tr></table></figure></li>
<li>重新检查 <code>etcd</code> 集群及 Kubernetes 集群的状态 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">curl 127.0.0.1:2381/health</span></span><br><span class="line">&#123;&quot;health&quot;:&quot;true&quot;,&quot;reason&quot;:&quot;&quot;&#125;</span><br><span class="line">   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS   ROLES           AGE    VERSION</span><br><span class="line">k8s-master1   Ready    control-plane   369d   v1.24.7</span><br><span class="line">k8s-master2   Ready    control-plane   369d   v1.24.7</span><br><span class="line">k8s-master3   Ready    control-plane   369d   v1.24.7</span><br><span class="line">k8s-worker1   Ready    &lt;none&gt;          369d   v1.24.7</span><br><span class="line">k8s-worker2   Ready    &lt;none&gt;          369d   v1.24.7</span><br></pre></td></tr></table></figure></li>
</ol>
<p>安装以上步骤操作后，检查 Kubernetes 集群中管理节点 Pod 状态发现 <code>master3</code> 节点上面的 <code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 处于 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                  READY   STATUS             RESTARTS          AGE</span><br><span class="line">coredns-6d4b75cb6d-57jhk              0/1     Pending            0                 2d23h</span><br><span class="line">coredns-6d4b75cb6d-w6wgg              0/1     Pending            0                 2d23h</span><br><span class="line">etcd-k8s-master1                      1/1     Running            1 (68d ago)       2d22h</span><br><span class="line">etcd-k8s-master2                      1/1     Running            0                 2d23h</span><br><span class="line">etcd-k8s-master3                      1/1     Running            0                 2d23h</span><br><span class="line">kube-apiserver-k8s-master1            1/1     Running            792 (12m ago)     2d23h</span><br><span class="line">kube-apiserver-k8s-master2            1/1     Running            4 (68d ago)       2d23h</span><br><span class="line">kube-apiserver-k8s-master3            0/1     CrashLoopBackOff   807 (18s ago)     2d23h</span><br><span class="line">kube-controller-manager-k8s-master1   1/1     Running            825 (14m ago)     2d23h</span><br><span class="line">kube-controller-manager-k8s-master2   1/1     Running            0                 2d23h</span><br><span class="line">kube-controller-manager-k8s-master3   0/1     CrashLoopBackOff   823 (4m35s ago)   2d23h</span><br><span class="line">kube-scheduler-k8s-master3            0/1     CrashLoopBackOff   823 (3m53s ago)   2d23h</span><br></pre></td></tr></table></figure>
<p>检查 <code>kube-apiserver-k8s-master3</code> 日志发现是因为证书问题无法连接到 <code>etcd</code>。<strong>原因可能是因为 这些组件连接 <code>etcd</code> 的证书导致</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system kube-apiserver-k8s-master3</span></span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...</span><br><span class="line">clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to &#123;127.0.0.1:2379 127.0.0.1 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...</span><br></pre></td></tr></table></figure>
<p><strong>在 <code>master3</code>节点上</strong>执行以下命令更新 <code>kube-apiserver</code> 连接 <code>etcd</code> 时使用的客户端证书 <code>/etc/kubernetes/pki/apiserver-etcd-client.crt</code>，需要先删除 <code>/etc/kubernetes/pki/apiserver-etcd-client.crt</code> 和 <code>/etc/kubernetes/pki/apiserver-etcd-client.key</code>，否则更新时会报错：<code>error execution phase certs/apiserver-etcd-client: [certs] certificate apiserver-etcd-client not signed by CA certificate etcd/ca: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;etcd-ca&quot;)</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs apiserver-etcd-client --config=/root/kubeadm-config.yaml</span> </span><br><span class="line">W1211 15:06:57.911002    1401 common.go:84] your configuration file uses a deprecated API spec: &quot;kubeadm.k8s.io/v1beta2&quot;. Please use &#x27;kubeadm config migrate --old-config old.yaml --new-config new.yaml&#x27;, which will write the new, similar spec using a newer API version.</span><br><span class="line">W1211 15:06:57.911751    1401 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme &quot;unix&quot; to the &quot;criSocket&quot; with value &quot;/var/run/containerd/containerd.sock&quot;. Please update your configuration!</span><br><span class="line">I1211 15:06:58.238799    1401 version.go:255] remote version is much newer: v1.28.4; falling back to: stable-1.24</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>检查 <code>kube-controller-manager-k8s-master3</code> 日志和 <code>kube-scheduler-k8s-master3</code> 日志，显示相关端口已经绑定，kill 掉被占用端口的进程后重试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system kube-controller-manager-k8s-master3</span></span><br><span class="line">I1211 07:04:37.643782       1 serving.go:348] Generated self-signed cert in-memory</span><br><span class="line">failed to create listener: failed to listen on 0.0.0.0:10257: listen tcp 0.0.0.0:10257: bind: address already in use</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl logs -n kube-system kube-scheduler-k8s-master3</span></span><br><span class="line">I1211 07:10:21.304329       1 serving.go:348] Generated self-signed cert in-memory</span><br><span class="line">E1211 07:10:21.304587       1 run.go:74] &quot;command failed&quot; err=&quot;failed to create listener: failed to listen on 0.0.0.0:10259: listen tcp 0.0.0.0:10259: bind: address already in use&quot;</span><br></pre></td></tr></table></figure>

<h3 id="更新证书后，kube-apiserver-报证书过期错误"><a href="#更新证书后，kube-apiserver-报证书过期错误" class="headerlink" title="更新证书后，kube-apiserver 报证书过期错误"></a>更新证书后，kube-apiserver 报证书过期错误</h3><p>在更新集群证书后，<code>kube-apiserver</code> 异常，检查日志，有证书过期的错误</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z, verifying certificate SN=2750116196247444292, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z, verifying certificate SN=2750116196247444292, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z, verifying certificate SN=2750116196247444292, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z, verifying certificate SN=2750116196247444292, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z, verifying certificate SN=2750116196247444292, SKID=, AKID=08:39:2B:D0:14:00:F4:7F:3F:58:26:36:32:BA:F8:0E:0E:B4:D4:83 failed: x509: certificate has expired or is not yet valid: current time 2023-12-11T07:35:22Z is after 2023-12-06T09:50:35Z]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[invalid bearer token, service account token has been invalidated]&quot;</span><br><span class="line">authentication.go:63] &quot;Unable to authenticate the request&quot; err=&quot;[invalid bearer token, service account token has been invalidated]&quot;</span><br></pre></td></tr></table></figure>
<p>这个可能是因为更新证书后，其他节点依旧使用旧的证书在请求 <code>kube-apiserver</code>，此时可以重启所有节点上的 <code>kubelet</code> 服务。</p>
<h1 id="Ingress-接入异常"><a href="#Ingress-接入异常" class="headerlink" title="Ingress 接入异常"></a>Ingress 接入异常</h1><h2 id="503-Service-Temporarily-Unavailable"><a href="#503-Service-Temporarily-Unavailable" class="headerlink" title="503 Service Temporarily Unavailable"></a>503 Service Temporarily Unavailable</h2><p><code>Deployment</code>，<code>Service</code>，<code>Ingress</code> 部署后，通过 <code>Ingress</code> 配置的域名访问，显示 <code>503 Service Temporarily Unavailable</code><br><img src="https://i.csms.tech/img_110.png"></p>
<p><strong>排查步骤</strong></p>
<p>检查 <code>Ingress-Nginx</code> Pod 的日志，检索对应域名日志，显示返回码为 503</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">52.77.198.154 - - [15/Dec/2022:02:10:59 +0000] &quot;GET /graph HTTP/1.1&quot; 503 592 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36&quot; 507 0.000 [prometheus-prometheus-service-8080] [] - - - - 00b07fe234401054153fdbd0ffafb158</span><br></pre></td></tr></table></figure>

<p>查看 Ingress 对应的 <code>Service</code>，从以下输出中可以看到对应的 <code>Service</code> 为 <code>prometheus-service</code>，端口为 8080</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ingress -n prometheus -o wide</span></span><br><span class="line">NAME            CLASS   HOSTS                     ADDRESS                      PORTS   AGE</span><br><span class="line">prometheus-ui   nginx   prometheus.example.com    172.31.23.72,172.31.27.193   80      19h</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe ingress prometheus-ui -n prometheus</span> </span><br><span class="line">Name:             prometheus-ui</span><br><span class="line">Labels:           &lt;none&gt;</span><br><span class="line">Namespace:        prometheus</span><br><span class="line">Address:          172.31.23.72,172.31.27.193</span><br><span class="line">Ingress Class:    nginx</span><br><span class="line">Default backend:  &lt;default&gt;</span><br><span class="line">Rules:</span><br><span class="line">  Host                     Path  Backends</span><br><span class="line">  ----                     ----  --------</span><br><span class="line">  prometheus.example.com  </span><br><span class="line">                           /   prometheus-service:8080 ()</span><br><span class="line">Annotations:               field.cattle.io/publicEndpoints:</span><br><span class="line">                             [&#123;&quot;addresses&quot;:[&quot;172.31.23.72&quot;,&quot;172.31.27.193&quot;],&quot;port&quot;:80,&quot;protocol&quot;:&quot;HTTP&quot;,&quot;serviceName&quot;:&quot;prometheus:prometheus-service&quot;,&quot;ingressName&quot;:&quot;pr...</span><br><span class="line">Events:                    &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看 <code>Service</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -n prometheus -o wide</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR</span><br><span class="line">prometheus-service   ClusterIP   10.99.75.232   &lt;none&gt;        8090/TCP   19h   app=prometheus-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe service -n prometheus prometheus-service</span></span><br><span class="line">Name:              prometheus-service</span><br><span class="line">Namespace:         prometheus</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=prometheus-server</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Family Policy:  SingleStack</span><br><span class="line">IP Families:       IPv4</span><br><span class="line">IP:                10.99.75.232</span><br><span class="line">IPs:               10.99.75.232</span><br><span class="line">Port:              prometheus-port  8090/TCP</span><br><span class="line">TargetPort:        9090/TCP</span><br><span class="line">Endpoints:         10.244.3.95:9090</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>从以上信息可以看到，服务的端口为 <code>Port: prometheus-port  8090/TCP</code>，而 Ingress 中配置的服务端口为 <code>8080</code> ，修改 Ingress 配置，将服务端口修改正确。修改后访问正常。</p>
<h1 id="其他错误"><a href="#其他错误" class="headerlink" title="其他错误"></a>其他错误</h1><h2 id="invalid-Host-header"><a href="#invalid-Host-header" class="headerlink" title="invalid Host header"></a>invalid Host header</h2><p>在 Master 节点上执行以下命令时报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it -n kube-system coredns-6d4b75cb6d-kdlqg -- sh</span></span><br><span class="line">error: Internal error occurred: error executing command in container: http: invalid Host header</span><br></pre></td></tr></table></figure>
<p>此错误主要是因为 Kubernetes 和 <code>cri-docker</code> 版本问题导致 <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GO 1.20.6 breaks cri-dockerd](https://github.com/k3s-io/k3s/issues/8089)
">[3]</span></a></sup></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1456389">相关参考</a></p>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1931089">Back-off restarting failed container 怎么办</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/">ephemeral-storage 问题</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/k3s-io/k3s/issues/8089">GO 1.20.6 breaks cri-dockerd</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="../tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i> Kubernetes</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="../202209241108/" rel="prev" title="kubernetes 对象的 yaml 描述语法说明">
                  <i class="fa fa-chevron-left"></i> kubernetes 对象的 yaml 描述语法说明
                </a>
            </div>
            <div class="post-nav-item">
                <a href="../202209301604/" rel="next" title="ingress-nginx 安装配置">
                  ingress-nginx 安装配置 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COSMOS</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="../js/comments.js"></script><script src="../js/utils.js"></script><script src="../js/motion.js"></script><script src="../js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="../js/third-party/search/local-search.js"></script>




  <script src="../js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fl9999","repo":"fl9999.github.io","client_id":"a11bf6f7860762b725b5","client_secret":"a99046105f8bddc72ec718d54dc3fd7f22070821","admin_user":"fl9999","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d41d8cd98f00b204e9800998ecf8427e"}</script>
<script src="../js/third-party/comments/gitalk.js"></script>

</body>
</html>
