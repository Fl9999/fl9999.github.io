<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16-next.png">
  <link rel="mask-icon" href="../images/logo.svg" color="#222">

<link rel="stylesheet" href="../css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"csms.tech","root":"/","images":"../images","scheme":"Gemini","darkmode":true,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeIn","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"../search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="../js/config.js"></script>

    <meta name="description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 常见错误总结">
<meta property="og:url" content="http://csms.tech/202209281614/index.html">
<meta property="og:site_name" content="L B T">
<meta property="og:description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.csms.tech/img_102.png">
<meta property="og:image" content="https://i.csms.tech/img_103.png">
<meta property="og:image" content="https://i.csms.tech/img_104.png">
<meta property="og:image" content="https://i.csms.tech/img_110.png">
<meta property="article:published_time" content="2022-09-28T08:14:41.000Z">
<meta property="article:modified_time" content="2022-12-15T05:09:36.000Z">
<meta property="article:author" content="COSMOS">
<meta property="article:tag" content="kubernetes">
<meta property="article:tag" content="k8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.csms.tech/img_102.png">


<link rel="canonical" href="http://csms.tech/202209281614/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://csms.tech/202209281614/","path":"202209281614/","title":"kubernetes 常见错误总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kubernetes 常见错误总结 | L B T</title>
  








  <noscript>
    <link rel="stylesheet" href="../css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">L B T</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记 录 过 去 的 经 验</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="../index.html" rel="section"><i class="fa fa-earth-americas fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="../categories/" rel="section"><i class="fa fa-folder-tree fa-fw"></i>目录<span class="badge">28</span></a></li><li class="menu-item menu-item-tags"><a href="../tags/" rel="section"><i class="fa fa-tornado fa-fw"></i>标签<span class="badge">73</span></a></li><li class="menu-item menu-item-archives"><a href="../archives/" rel="section"><i class="fa fa-rectangle-list fa-fw"></i>列表<span class="badge">117</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">1.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">2.</span> <span class="nav-text">POD 状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CrashLoopBackOff"><span class="nav-number">2.1.</span> <span class="nav-text">CrashLoopBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E4%B8%BA-InvalidImageName"><span class="nav-number">2.2.</span> <span class="nav-text">POD 状态为 InvalidImageName</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Error"><span class="nav-number">2.3.</span> <span class="nav-text">Pod 状态为 Error</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-node-was-low-on-resource-ephemeral-storage"><span class="nav-number">2.3.1.</span> <span class="nav-text">The node was low on resource: ephemeral-storage</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Init"><span class="nav-number">2.4.</span> <span class="nav-text">Pod 状态为 Init</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unable-to-attach-or-mount-volumes"><span class="nav-number">2.4.1.</span> <span class="nav-text">Unable to attach or mount volumes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">网络问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84-Pod-%E4%B9%8B%E9%97%B4%E7%BD%91%E7%BB%9C%E4%B8%8D%E9%80%9A"><span class="nav-number">3.1.</span> <span class="nav-text">同一个节点上的 Pod 之间网络不通</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E5%88%B0%E5%A4%96%E9%83%A8-Internet-%E7%BD%91%E7%BB%9C"><span class="nav-number">3.2.</span> <span class="nav-text">Pod 无法访问到外部 Internet 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E9%97%B4%E6%AD%87%E6%80%A7%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">3.3.</span> <span class="nav-text">Pod 间歇性无法连接外部数据库</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">4.</span> <span class="nav-text">集群状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81-NotReady"><span class="nav-number">4.1.</span> <span class="nav-text">节点状态 NotReady</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><span class="nav-number">4.1.1.</span> <span class="nav-text">PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E5%8E%9F%E5%9B%A0"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">异常原因</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#container-runtime-is-down-container-runtime-not-ready"><span class="nav-number">4.1.2.</span> <span class="nav-text">container runtime is down, container runtime not ready</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#api-server-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">4.2.</span> <span class="nav-text">api-server 启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E5%9C%BA%E6%99%AF"><span class="nav-number">4.2.1.</span> <span class="nav-text">错误场景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">解决方法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ingress-%E6%8E%A5%E5%85%A5%E5%BC%82%E5%B8%B8"><span class="nav-number">5.</span> <span class="nav-text">Ingress 接入异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#503-Service-Temporarily-Unavailable"><span class="nav-number">5.1.</span> <span class="nav-text">503 Service Temporarily Unavailable</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%9A%E6%B3%A8"><span class="nav-number">7.</span> <span class="nav-text">脚注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">COSMOS</p>
  <div class="site-description" itemprop="description">得 能 莫 忘</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="../archives/">
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="../categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="../tags/">
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://csms.tech/202209281614/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="../images/avatar.gif">
      <meta itemprop="name" content="COSMOS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L B T">
      <meta itemprop="description" content="得 能 莫 忘">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kubernetes 常见错误总结 | L B T">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kubernetes 常见错误总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-28 16:14:41" itemprop="dateCreated datePublished" datetime="2022-09-28T16:14:41+08:00">2022-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-12-15 13:09:36" itemprop="dateModified" datetime="2022-12-15T13:09:36+08:00">2022-12-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">上层目录</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="../categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><ul>
<li>Centos7 5.4.212-1</li>
<li>Docker 20.10.18</li>
<li>containerd.io-1.6.8</li>
<li>kubectl-1.25.0</li>
<li>kubeadm-1.25.0</li>
<li>kubelet-1.25.0</li>
</ul>
<h1 id="POD-状态异常"><a href="#POD-状态异常" class="headerlink" title="POD 状态异常"></a>POD 状态异常</h1><h2 id="CrashLoopBackOff"><a href="#CrashLoopBackOff" class="headerlink" title="CrashLoopBackOff"></a>CrashLoopBackOff</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                                     READY   STATUS             RESTARTS       AGE</span><br><span class="line">test-centos7-7cc5dc6987-jz486            0/1     CrashLoopBackOff   8 (111s ago)   17m</span><br></pre></td></tr></table></figure>
<p>查看 <code>Pod</code> 详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod test-centos7-7cc5dc6987-jz486</span></span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                  From               Message</span><br><span class="line">  ----     ------     ----                 ----               -------</span><br><span class="line">  Normal   Scheduled  18m                  default-scheduler  Successfully assigned default/test-centos7-7cc5dc6987-jz486 to ops-kubernetes3</span><br><span class="line">  Normal   Pulled     16m (x5 over 18m)    kubelet            Container image &quot;centos:centos7.9.2009&quot; already present on machine</span><br><span class="line">  Normal   Created    16m (x5 over 18m)    kubelet            Created container centos7</span><br><span class="line">  Normal   Started    16m (x5 over 18m)    kubelet            Started container centos7</span><br><span class="line">  Warning  BackOff    3m3s (x71 over 18m)  kubelet            Back-off restarting failed container</span><br></pre></td></tr></table></figure>
<p>结果显示，<code>Reason</code> 为 <code>BackOff</code>，<code>Message</code> 显示 <code>Back-off restarting failed container</code></p>
<p><strong>可能原因</strong> ：</p>
<p><code>Back-off restarting failed container</code> 的原因，通常是因为，容器内 PID 为 1 的进程退出导致（通常用户在构建镜像执行 <code>CMD</code> 时，启动的程序，均是 PID 为1）<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Back-off restarting failed container 怎么办](https://cloud.tencent.com/developer/article/1931089)">[1]</span></a></sup></p>
<p>容器进程退出（命令执行结束或者进程异常结束），则容器生命周期结束。kubernetes 控制器检查到容器退出，会持续重启容器。针对此种情况，需要检查镜像，是否不存在常驻进程，或者常驻进程异常。</p>
<p>针对此种情况，可以单独使用 <code>docker</code> 客户端部署镜像，查看镜像的运行情况，如果部署后，容器中的进程立马结束或退出，则容器也会随之结束。</p>
<span id="more"></span>
<h2 id="POD-状态为-InvalidImageName"><a href="#POD-状态为-InvalidImageName" class="headerlink" title="POD 状态为 InvalidImageName"></a>POD 状态为 InvalidImageName</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>InvalidImageName</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n cs</span><br><span class="line">NAME               READY   STATUS              RESTARTS   AGE</span><br><span class="line">54fdc56754-qrlt6   0/2     InvalidImageName    0          14s</span><br><span class="line">8486f49b89-zp25b   0/2     Init:ErrImagePull   0          7s</span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>镜像的 url 地址中，以 <code>http://</code> 或 <code>https://</code> 开头。配置中镜像的 url 地址中无需指定协议（<code>http://</code> 或 <code>https://</code>） </p>
<h2 id="Pod-状态为-Error"><a href="#Pod-状态为-Error" class="headerlink" title="Pod 状态为 Error"></a>Pod 状态为 Error</h2><h3 id="The-node-was-low-on-resource-ephemeral-storage"><a href="#The-node-was-low-on-resource-ephemeral-storage" class="headerlink" title="The node was low on resource: ephemeral-storage"></a>The node was low on resource: ephemeral-storage</h3><p><strong>错误场景</strong>：</p>
<p>查看 Pod 状态，显示 Error</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS                   RESTARTS   AGE</span><br><span class="line">front-7df8ccc4c7-xhp6s    0/1     Error                    0          5h42m</span><br></pre></td></tr></table></figure>
<p>检查 Pod 的具体信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod front-7df8ccc4c7-xhp6s</span></span><br><span class="line">...</span><br><span class="line">Status:       Failed</span><br><span class="line">Reason:       Evicted</span><br><span class="line">Message:      The node was low on resource: ephemeral-storage. Container php was using 394, which exceeds its request of 0. </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>其中包含异常的关键信息：<code>Status:       Failed</code>，<code>Reason:       Evicted</code>，具体原因为 <code>The node was low on resource: ephemeral-storage</code></p>
<p>检查节点上的 Kuberlet 日志，搜索关键字 <code>evicte</code> 或者 <code>disk</code> ，也可以看到系统上文件系统空间使用率超过了阈值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet  | grep -i -e disk -e evict</span></span><br><span class="line"> image_gc_manager.go:310] &quot;Dis usage on image filesystem is over the high threshold, trying to free bytes down to the low threshold&quot; usage=85 highThreshold=85 amountToFree=5122092236 lowThreshold=80</span><br><span class="line"> eviction_manager.go:349] &quot;Eviction manager: must evict pod(s) to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"> eviction_manager.go:338] &quot;Eviction manager: attempting to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>根据以上信息，可知 Pod 异常是因为 <code>The node was low on resource: ephemeral-storage</code>，表示 <strong>临时存储资源</strong> 不足导致节点处于 <code>Tainted</code> ，其上的 Pod 被驱逐(<code>Evicted</code>)</p>
<p><strong><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">本地临时存储说明</a></strong></p>
<p>针对此种情况，如果某 Pod 的临时存储用量超出了你所允许的范围，kubelet 会向其发出逐出（<code>eviction</code>）信号，触发该 Pod 被逐出所在节点。</p>
<p>如果用于可写入容器镜像层、节点层面日志或者 <code>emptyDir</code> 卷的文件系统中可用空间太少， 节点会为自身设置本地存储不足的污点(<code>Tainted</code>)标签。 这一污点会触发对那些无法容忍该污点的 Pod 的逐出操作。</p>
<p><strong>解决方法</strong> ：</p>
<ul>
<li><p>增加磁盘空间</p>
</li>
<li><p>调整 <code>kubelet</code> 的 <code>nodefs.available</code> 的 threshold 值</p>
<p>  修改节点上的 <code>kubelet</code> 的启动配置文件 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，添加以下启动参数，主要为定义环境变量 <code>KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</code>，并将其添加到启动参数中</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;KUBELET_EVICT_NODEFS_THRESHOLD_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</span><br></pre></td></tr></table></figure>
<p>  修改之后重启 <code>kubelet</code> 服务，并通过日志查看 <code>nodefs.available</code> 的新值是否生效</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart kubelet</span><br><span class="line"></span><br><span class="line">$ journalctl -u kubelet | grep -i nodefs</span><br><span class="line">17604 container_manager_linux.go:267] &quot;Creating Container Manager object based on Node Config&quot; nodeConfig=&#123;RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:&#123;KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:&#123;&#125;] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[&#123;Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05&#125; GracePeriod:0s MinReclaim:&lt;nil&gt;&#125;]&#125; QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>日志中看到 <code>Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05</code>，表明更改生效。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ephemeral-storage 问题](https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/)">[2]</span></a></sup></p>
</li>
</ul>
<h2 id="Pod-状态为-Init"><a href="#Pod-状态为-Init" class="headerlink" title="Pod 状态为 Init"></a>Pod 状态为 Init</h2><h3 id="Unable-to-attach-or-mount-volumes"><a href="#Unable-to-attach-or-mount-volumes" class="headerlink" title="Unable to attach or mount volumes"></a>Unable to attach or mount volumes</h3><p>Pod 启动异常，查看 Pod 状态为 <code>Init:0/1</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -n agmjys</span></span><br><span class="line">NAME                          READY   STATUS     RESTARTS   AGE</span><br><span class="line">admin-cbb479556-j9qg2    0/1     Init:0/1   0          3m37s</span><br></pre></td></tr></table></figure>
<p>查看 Pod 的详细描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod admin-cbb479556-j9qg2</span></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason       Age    From               Message</span><br><span class="line">  ----     ------       ----   ----               -------</span><br><span class="line">  Normal   Scheduled    3m41s  default-scheduler  Successfully assigned admin-cbb479556-j9qg2 to k8s-work2</span><br><span class="line">  Warning  FailedMount  99s    kubelet            Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[wwwroot kube-api-access-z8745 logs]: timed out waiting for the condition</span><br><span class="line">  Warning  FailedMount  42s    kubelet            MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</span><br><span class="line">Mounting command: mount</span><br><span class="line">Mounting arguments: -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</span><br><span class="line">Output: mount.nfs: Connection timed out</span><br></pre></td></tr></table></figure>

<p>根据 <code>Events</code> 中输出的信息，<code>MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</code>，显示挂载卷失败，输出中包含了挂载卷时使用的命令和参数（<code>mount -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</code>）及命令失败后的返回结果（<code>mount.nfs: Connection timed out</code>）</p>
<p>根据 <code>Events</code> 中的信息，查看配置，发现此卷为 NFS 类型的 PV，根据报错排查，此例原因为 NFS 的服务器地址填写错误，更新 PV 配置中的 NFS Server 的地址后，Pod 正常启动。</p>
<h1 id="网络问题"><a href="#网络问题" class="headerlink" title="网络问题"></a>网络问题</h1><h2 id="同一个节点上的-Pod-之间网络不通"><a href="#同一个节点上的-Pod-之间网络不通" class="headerlink" title="同一个节点上的 Pod 之间网络不通"></a>同一个节点上的 Pod 之间网络不通</h2><p><strong>问题现象</strong>：</p>
<p>同一个节点上的 <code>Pod</code> 之间网络不通</p>
<p><strong>排查思路</strong>：</p>
<ul>
<li>检查系统内核配置是否开启转发 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sysctl -a | grep net.ipv4.ip_forward</span></span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure></li>
<li>检查 <code>iptables</code> 是否禁止转发，<a href="https://csms.tech/202209121102/#集群中所有计算机之间具有完全的网络连接"><code>iptables</code> 防火墙配置参考</a>)</li>
<li>为了定位是否为 <code>iptables</code> 影响，开关闭 <code>iptables</code> 再进行测试，如果关闭防火墙后可以通信，可以确定是防火墙规则导致，需要检查防火墙规则。</li>
<li>更深入的排查，可以部署 <a target="_blank" rel="noopener" href="https://hub.docker.com/r/antrea/netshoot/tags"><code>netshoot</code> 容器</a> 进行抓包定位，</li>
</ul>
<h2 id="Pod-无法访问到外部-Internet-网络"><a href="#Pod-无法访问到外部-Internet-网络" class="headerlink" title="Pod 无法访问到外部 Internet 网络"></a>Pod 无法访问到外部 Internet 网络</h2><p>某个节点上，Pod 无法外部主机的服务（端口 6603&#x2F;tcp）。分别在 Pod ，节点 <code>cni0</code> 网卡，节点出口网卡 <code>eth0</code> ，目标服务网卡上抓包。此例中 Pod IP 为 <code>10.244.4.173</code>，目标服务的 IP 地址为 <code>50.18.6.225</code></p>
<p>查看 Pod 抓包结果</p>
<p><img src="https://i.csms.tech/img_102.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看 节点 <code>cni0</code> 网卡 的抓包</p>
<p><img src="https://i.csms.tech/img_103.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看节点出口网卡 <code>eth0</code> 的抓包。</p>
<p><img src="https://i.csms.tech/img_104.png"></p>
<p><strong>此处看到的源 IP 依然是  Pod 的 IP 地址，此处存在问题</strong>。在云主机的场景中，如果数据包以这种结构发送出去，数据包到了 Internet 网关将拒绝它，因为网关 NAT（将 VM 的 IP 转换为公网 IP） 只了解连接到 VM 的 IP 地址。</p>
<p>正常情况下，Pod 的流量到节点的出口网卡之前，是应该经过 <code>iptables</code> 执行源 NAT - <strong>更改数据包源，使数据包看起来来自 VM 而不是 Pod</strong>。有了正确的源 IP，数据包才可以离开 VM 进入 Internet</p>
<p>此种情况下，数据包可以从节点的出口网卡发送出去，但是到了 Internet 网关将会被丢弃，因此目标服务无法接收到请求，查看目标服务器上的抓包，确实未收到来自此 Pod 的请求。</p>
<p>此处的 <strong>源 NAT</strong> 是由 <code>iptables</code>  负责执行，流入节点出口网卡的数据包未被正确的 <strong>源 NAT</strong>，有可能是因为 <code>kube-proxy</code> 维护的网络规则错误，或者因为 <code>iptables</code> 规则配置错误。可以通过重启 <code>kube-proxy</code> （由服务 <code>kubelet</code> 管理）和 <code>iptables</code> 服务尝试恢复。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart iptables</span><br></pre></td></tr></table></figure>
<p>本示例中，重启这 2 个服务后，Pod 恢复正常。</p>
<h2 id="Pod-间歇性无法连接外部数据库"><a href="#Pod-间歇性无法连接外部数据库" class="headerlink" title="Pod 间歇性无法连接外部数据库"></a>Pod 间歇性无法连接外部数据库</h2><p>集群中的 Pod 出现连接集群之外的数据库服务超时，且出现频率较高</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42684642/article/details/105775436">参考文章</a></p>
<h1 id="集群状态异常"><a href="#集群状态异常" class="headerlink" title="集群状态异常"></a>集群状态异常</h1><h2 id="节点状态-NotReady"><a href="#节点状态-NotReady" class="headerlink" title="节点状态 NotReady"></a>节点状态 NotReady</h2><h3 id="PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><a href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago" class="headerlink" title="PLEG is not healthy: pleg was last seen active 10m13.755045415s ago"></a>PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION</span><br><span class="line">k8s-master1   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master3   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-work1     NotReady   &lt;none&gt;          14d   v1.24.7</span><br><span class="line">k8s-work2     Ready      &lt;none&gt;          14d   v1.24.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看节点详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-work1</span></span><br><span class="line">...</span><br><span class="line">Conditions:</span><br><span class="line">  Ready                False   Tue, 15 Nov 2022 10:14:49 +0800   Tue, 15 Nov 2022 10:07:39 +0800   KubeletNotReady              PLEG is not healthy: pleg was last seen active 10m13.755045415s ago; threshold is 3m0s</span><br></pre></td></tr></table></figure>

<h4 id="异常原因"><a href="#异常原因" class="headerlink" title="异常原因"></a>异常原因</h4><p>集群因为此原因（<code>PLEG is not healthy: pleg was last seen active ***h**m***s ago;</code>）状态变为 <code>NotReady</code>，通常是因为节点超负载。</p>
<h3 id="container-runtime-is-down-container-runtime-not-ready"><a href="#container-runtime-is-down-container-runtime-not-ready" class="headerlink" title="container runtime is down, container runtime not ready"></a>container runtime is down, container runtime not ready</h3><p><strong>排查过程</strong>：</p>
<p>检查集群中的 Pod 分布情况时，发现某一节点上几乎所有的 Pod 都被调度去了其他节点，当前检查时此节点的状态已经是 <code>Ready</code>，针对此情况进行分析。</p>
<ol>
<li><p>确定问题发生的大概时间段</p>
<p> 根据 Pod 在其他节点上面被启动的时间，可以大概确定节点异常的时间，根据此时间段可以缩小排查的时间范围。此示例中问题发生的时间大概在 <code>Nov 25 04:49:00</code> 前后。</p>
</li>
<li><p>检查 <code>kubelet</code> 日志</p>
<p> 根据已经推断出的时间段，在 <strong>问题节点</strong> 上，检查 <code>kubelet</code> 日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ journalctl -u kubelet --since &quot;2022-11-25 4:40&quot; | grep -v -e &quot;failed to get fsstats&quot; -e &quot;invalid bearer token&quot; | more</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.153132   17604 generic.go:205] &quot;GenericPLEG: Unable to retrieve pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375524   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;PodSandboxFilter&#123;Id:,State:&amp;PodSandboxStateValue&#123;State:SANDBOX_READY,&#125;,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375559   17604 kuberuntime_sandbox.go:292] &quot;Failed to list pod sandboxes&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375578   17604 kubelet_pods.go:1153] &quot;Error listing containers&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375589   17604 kubelet.go:2162] &quot;Failed cleaning pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.476011   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.507861   17604 remote_runtime.go:680] &quot;ExecSync cmd from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; containerID=&quot;5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac&quot; cmd=[wget --no-check-certificate --spider -q http://localhost:8088/health]</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.676271   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.076918   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329558   17604 remote_runtime.go:536] &quot;ListContainers with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;ContainerFilter&#123;Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329585   17604 container_log_manager.go:183] &quot;Failed to rotate container logs&quot; err=&quot;failed to list containers: rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: E1125 04:49:09.485356   17604 remote_runtime.go:168] &quot;Version from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: I1125 04:49:09.485486   17604 setters.go:532] &quot;Node became not ready&quot; node=&quot;k8s-work2&quot; condition=&#123;Type:Ready Status:False LastHeartbeatTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 LastTransitionTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p> 从以上日志中，可以看到关键的日志信息：</p>
<p> <code>&quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</code></p>
<p> <code>setters.go:532] &quot;Node became not ready&quot;</code>，    <code>Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</code></p>
<p> 从以上日志信息可以看出，节点状态变为了 <code>not ready</code>，原因为 <code>container runtime is down, container runtime not ready</code>，本示例中 <code>container runtime</code> 为 <code>docker</code></p>
</li>
<li><p>检查 docker 服务日志</p>
<p> 根据上面的日志时间，检查 docker 服务的日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">journalctl -u docker --since &quot;2022-11-25 04:0&quot; | more</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410342223+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/41e0dfe97b87c2b8ae941653fa8adbf93bf9358d91e967646e4549ab71b2f004/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.414773158+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.416474238+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.422844592+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure>
<p>根据日志可以看到关键日志 <code>write unix /var/run/docker.sock-&gt;@: write: broken pipe</code></p>
</li>
<li><p>检查 messages 日志</p>
<p> 查看对应时间段的系统日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.153089   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;nil&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375614   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>根据 <code>kubelet</code> 服务日志，节点 <code>Not Ready</code> 的原因为 <code>docker down</code>，根据 docker 服务日志，docker 存在异常，但是此时执行 <code>docker</code> 相关命令，未发现异常。此问题多次出现，<code>docker engine</code> 版本为 <code>19.03.15-3</code>，之后尝试将 <code>docker engine</code> 版本升级为最新版本 <code>20.10.9</code>，问题未在出现。<a href="https://csms.tech/202208041317/#docker-ce-19-03-15-升级到-docker-ce-20-10-9"><code>docker engine</code> 升级参考</a> </p>
<h2 id="api-server-启动失败"><a href="#api-server-启动失败" class="headerlink" title="api-server 启动失败"></a>api-server 启动失败</h2><h3 id="错误场景"><a href="#错误场景" class="headerlink" title="错误场景"></a>错误场景</h3><p>api server 启动失败，执行 <code>kubectl</code> 命令输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">The connection to the server kube-apiserver.uat.148962587001:6443 was refused - did you specify the right host or port?</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>检查 Api Server 监听的端口 6443 ，显示端口未启动。</p>
<p>检查 Api Server 对应的容器状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a | grep api</span></span><br><span class="line">81688b9cbe45  1f38c0b6a9d1   &quot;kube-apiserver --ad…&quot;   14 seconds ago      Exited (1) 13 seconds ago                       k8s_kube-apiserver_kube-apiserver-k8s-uat-master1.148962587001_kube-system_c8a87f4921623c7bff57f5662ea486cc_25</span><br></pre></td></tr></table></figure>

<p>容器状态为 <code>Exited</code>，检查容器日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker logs 81688b9cbe45</span></span><br><span class="line">I1116 07:43:53.775588       1 server.go:558] external host was not specified, using 172.31.30.123</span><br><span class="line">I1116 07:43:53.776035       1 server.go:158] Version: v1.24.7</span><br><span class="line">I1116 07:43:53.776057       1 server.go:160] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot;</span><br><span class="line">E1116 07:43:53.776298       1 run.go:74] &quot;command failed&quot; err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</span><br></pre></td></tr></table></figure>
<p>日志显示 <code>err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</code>，检查文件 <code>/etc/kubernetes/pki/apiserver.crt</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line">ls: cannot access /etc/kubernetes/pki/apiserver.crt: No such file or directory</span><br></pre></td></tr></table></figure>

<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ul>
<li>发现此文件确实不存在。若有备份，从备份中恢复此文件。如果没有备份，<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1801882">参考文档</a> 恢复证书<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /k8s/backup/pki/apiserver.key /etc/kubernetes/pki/</span><br><span class="line">cp /k8s/backup/pki/apiserver.crt /etc/kubernetes/pki/</span><br></pre></td></tr></table></figure>
重启 <code>kubelet</code> 后检查 Api Server，发现服务正常启动<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>如果只是缺少了 <code>apiserver.key</code>，<code>apiserver.crt</code> 证书文件，可通过以下命令重新生成证书文件，<a href="https://csms.tech/202209121102/#集群之外的服务器使用-kubectl-报错">生成原理参考</a><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm init phase certs apiserver \</span></span><br><span class="line"><span class="language-bash">     --apiserver-advertise-address  10.150.0.21 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans  10.96.0.1 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans 34.150.1.1</span></span><br><span class="line"> </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.21 34.150.1.1]</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Ingress-接入异常"><a href="#Ingress-接入异常" class="headerlink" title="Ingress 接入异常"></a>Ingress 接入异常</h1><h2 id="503-Service-Temporarily-Unavailable"><a href="#503-Service-Temporarily-Unavailable" class="headerlink" title="503 Service Temporarily Unavailable"></a>503 Service Temporarily Unavailable</h2><p><code>Deployment</code>，<code>Service</code>，<code>Ingress</code> 部署后，通过 <code>Ingress</code> 配置的域名访问，显示 <code>503 Service Temporarily Unavailable</code><br><img src="https://i.csms.tech/img_110.png"></p>
<p><strong>排查步骤</strong></p>
<p>检查 <code>Ingress-Nginx</code> Pod 的日志，检索对应域名日志，显示返回码为 503</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">52.77.198.154 - - [15/Dec/2022:02:10:59 +0000] &quot;GET /graph HTTP/1.1&quot; 503 592 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36&quot; 507 0.000 [prometheus-prometheus-service-8080] [] - - - - 00b07fe234401054153fdbd0ffafb158</span><br></pre></td></tr></table></figure>

<p>查看 Ingress 对应的 <code>Service</code>，从以下输出中可以看到对应的 <code>Service</code> 为 <code>prometheus-service</code>，端口为 8080</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ingress -n prometheus -o wide</span></span><br><span class="line">NAME            CLASS   HOSTS                     ADDRESS                      PORTS   AGE</span><br><span class="line">prometheus-ui   nginx   prometheus.example.com    172.31.23.72,172.31.27.193   80      19h</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe ingress prometheus-ui -n prometheus</span> </span><br><span class="line">Name:             prometheus-ui</span><br><span class="line">Labels:           &lt;none&gt;</span><br><span class="line">Namespace:        prometheus</span><br><span class="line">Address:          172.31.23.72,172.31.27.193</span><br><span class="line">Ingress Class:    nginx</span><br><span class="line">Default backend:  &lt;default&gt;</span><br><span class="line">Rules:</span><br><span class="line">  Host                     Path  Backends</span><br><span class="line">  ----                     ----  --------</span><br><span class="line">  prometheus.example.com  </span><br><span class="line">                           /   prometheus-service:8080 ()</span><br><span class="line">Annotations:               field.cattle.io/publicEndpoints:</span><br><span class="line">                             [&#123;&quot;addresses&quot;:[&quot;172.31.23.72&quot;,&quot;172.31.27.193&quot;],&quot;port&quot;:80,&quot;protocol&quot;:&quot;HTTP&quot;,&quot;serviceName&quot;:&quot;prometheus:prometheus-service&quot;,&quot;ingressName&quot;:&quot;pr...</span><br><span class="line">Events:                    &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看 <code>Service</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -n prometheus -o wide</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR</span><br><span class="line">prometheus-service   ClusterIP   10.99.75.232   &lt;none&gt;        8090/TCP   19h   app=prometheus-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe service -n prometheus prometheus-service</span></span><br><span class="line">Name:              prometheus-service</span><br><span class="line">Namespace:         prometheus</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=prometheus-server</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Family Policy:  SingleStack</span><br><span class="line">IP Families:       IPv4</span><br><span class="line">IP:                10.99.75.232</span><br><span class="line">IPs:               10.99.75.232</span><br><span class="line">Port:              prometheus-port  8090/TCP</span><br><span class="line">TargetPort:        9090/TCP</span><br><span class="line">Endpoints:         10.244.3.95:9090</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>从以上信息可以看到，**服务的端口为 <code>Port:              prometheus-port  8090/TCP</code>，而 Ingress 中配置的服务端口为 <code>8080</code>**，修改 Ingress 配置，将服务端口修改正确。修改后访问正常。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1456389">相关参考</a></p>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1931089">Back-off restarting failed container 怎么办</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/">ephemeral-storage 问题</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="../tags/kubernetes/" rel="tag"><i class="fa fa-tag"></i> kubernetes</a>
              <a href="../tags/k8s/" rel="tag"><i class="fa fa-tag"></i> k8s</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="../202209241108/" rel="prev" title="kubernetes 对象的 yaml 描述语法说明">
                  <i class="fa fa-chevron-left"></i> kubernetes 对象的 yaml 描述语法说明
                </a>
            </div>
            <div class="post-nav-item">
                <a href="../202209301604/" rel="next" title="ingress-nginx 安装配置">
                  ingress-nginx 安装配置 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COSMOS</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="../js/comments.js"></script><script src="../js/utils.js"></script><script src="../js/motion.js"></script><script src="../js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="../js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fl9999","repo":"fl9999.github.io","client_id":"a11bf6f7860762b725b5","client_secret":"a99046105f8bddc72ec718d54dc3fd7f22070821","admin_user":"fl9999","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d41d8cd98f00b204e9800998ecf8427e"}</script>
<script src="../js/third-party/comments/gitalk.js"></script>

</body>
</html>
