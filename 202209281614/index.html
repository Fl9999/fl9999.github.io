<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16-next.png">
  <link rel="mask-icon" href="../images/logo.svg" color="#222">

<link rel="stylesheet" href="../css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"csms.tech","root":"/","images":"../images","scheme":"Gemini","darkmode":true,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeIn","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"../search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="../js/config.js"></script>

    <meta name="description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 常见错误总结">
<meta property="og:url" content="http://csms.tech/202209281614/index.html">
<meta property="og:site_name" content="L B T">
<meta property="og:description" content="环境信息 Centos7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.25.0 kubeadm-1.25.0 kubelet-1.25.0  POD 状态异常CrashLoopBackOff错误场景 ：  Pod 状态显示 CrashLoopBackOff $ kubectl get podsNAME">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.csms.tech/img_102.png">
<meta property="og:image" content="https://i.csms.tech/img_103.png">
<meta property="og:image" content="https://i.csms.tech/img_104.png">
<meta property="og:image" content="https://i.csms.tech/img_110.png">
<meta property="article:published_time" content="2022-09-28T08:14:41.000Z">
<meta property="article:modified_time" content="2023-08-23T01:53:38.000Z">
<meta property="article:author" content="COSMOS">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.csms.tech/img_102.png">


<link rel="canonical" href="http://csms.tech/202209281614/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://csms.tech/202209281614/","path":"202209281614/","title":"kubernetes 常见错误总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kubernetes 常见错误总结 | L B T</title>
  








  <noscript>
    <link rel="stylesheet" href="../css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">L B T</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记 录 过 去 的 经 验</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="../index.html" rel="section"><i class="fa fa-earth-americas fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="../categories/" rel="section"><i class="fa fa-folder-tree fa-fw"></i>总目录<span class="badge">41</span></a></li><li class="menu-item menu-item-linux"><a href="../categories/Linux" rel="section"><i class="fa fa-brands fa-linux fa-fw"></i>Linux</a></li><li class="menu-item menu-item-python"><a href="../categories/Python" rel="section"><i class="fa fa-brands fa-python fa-fw"></i>Python</a></li><li class="menu-item menu-item-docker"><a href="../categories/Docker" rel="section"><i class="fa fa-brands fa-docker fa-fw"></i>Docker</a></li><li class="menu-item menu-item-kubernetes"><a href="../categories/Kubernetes" rel="section"><i class="fa fa-dharmachakra fa-fw"></i>Kubernetes</a></li><li class="menu-item menu-item-tags"><a href="../tags/" rel="section"><i class="fa fa-tornado fa-fw"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="../archives/" rel="section"><i class="fa fa-rectangle-list fa-fw"></i>列表<span class="badge">190</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章总目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">1.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">2.</span> <span class="nav-text">POD 状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CrashLoopBackOff"><span class="nav-number">2.1.</span> <span class="nav-text">CrashLoopBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POD-%E7%8A%B6%E6%80%81%E4%B8%BA-InvalidImageName"><span class="nav-number">2.2.</span> <span class="nav-text">POD 状态为 InvalidImageName</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Error"><span class="nav-number">2.3.</span> <span class="nav-text">Pod 状态为 Error</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-node-was-low-on-resource-ephemeral-storage"><span class="nav-number">2.3.1.</span> <span class="nav-text">The node was low on resource: ephemeral-storage</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E7%8A%B6%E6%80%81%E4%B8%BA-Init"><span class="nav-number">2.4.</span> <span class="nav-text">Pod 状态为 Init</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unable-to-attach-or-mount-volumes"><span class="nav-number">2.4.1.</span> <span class="nav-text">Unable to attach or mount volumes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ContainerCreating"><span class="nav-number">2.5.</span> <span class="nav-text">ContainerCreating</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dbus-connection-closed-by-user"><span class="nav-number">2.5.1.</span> <span class="nav-text">dbus: connection closed by user</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%9Ccni0%E2%80%9D-already-has-an-IP-address-different-from"><span class="nav-number">2.5.2.</span> <span class="nav-text">“cni0” already has an IP address different from</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">网络问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84-Pod-%E4%B9%8B%E9%97%B4%E7%BD%91%E7%BB%9C%E4%B8%8D%E9%80%9A"><span class="nav-number">3.1.</span> <span class="nav-text">同一个节点上的 Pod 之间网络不通</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E5%88%B0%E5%A4%96%E9%83%A8-Internet-%E7%BD%91%E7%BB%9C"><span class="nav-number">3.2.</span> <span class="nav-text">Pod 无法访问到外部 Internet 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E9%97%B4%E6%AD%87%E6%80%A7%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">3.3.</span> <span class="nav-text">Pod 间歇性无法连接外部数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B7%A8%E8%8A%82%E7%82%B9-Pod-%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE"><span class="nav-number">3.4.</span> <span class="nav-text">跨节点 Pod 无法访问</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF-1"><span class="nav-number">3.4.1.</span> <span class="nav-text">环境信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coredns-%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D"><span class="nav-number">3.5.</span> <span class="nav-text">coredns 无法解析域名</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dns-%E6%96%87%E4%BB%B6%E5%AE%9A%E4%BD%8D%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"><span class="nav-number">3.5.1.</span> <span class="nav-text">dns 文件定位参考文档</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">4.</span> <span class="nav-text">集群状态异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81-NotReady"><span class="nav-number">4.1.</span> <span class="nav-text">节点状态 NotReady</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><span class="nav-number">4.1.1.</span> <span class="nav-text">PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E5%8E%9F%E5%9B%A0"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">异常原因</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#container-runtime-is-down-container-runtime-not-ready"><span class="nav-number">4.1.2.</span> <span class="nav-text">container runtime is down, container runtime not ready</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%9CContainer-runtime-network-not-ready%E2%80%9D-networkReady-x3D-%E2%80%9DNetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized%E2%80%9D"><span class="nav-number">4.1.3.</span> <span class="nav-text">“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF-2"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">环境信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Container-runtime-network-not-ready%E2%80%9D-networkReady-x3D-%E2%80%9DNetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized%E2%80%9D"><span class="nav-number">4.1.4.</span> <span class="nav-text">Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#api-server-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">4.2.</span> <span class="nav-text">api-server 启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E5%9C%BA%E6%99%AF"><span class="nav-number">4.2.1.</span> <span class="nav-text">错误场景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">解决方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">4.3.</span> <span class="nav-text">kubelet 启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#failed-to-parse-kubelet-flag"><span class="nav-number">4.3.1.</span> <span class="nav-text">failed to parse kubelet flag</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-%E2%80%9C"><span class="nav-number">4.3.2.</span> <span class="nav-text">misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E4%B8%8D%E5%8F%AF%E7%94%A8"><span class="nav-number">4.4.</span> <span class="nav-text">服务不可用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E4%B8%AD%E4%B8%80%E4%B8%AA-master-%E8%8A%82%E7%82%B9%E5%BC%82%E5%B8%B8%E5%AF%BC%E8%87%B4%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E4%B8%8D%E5%8F%AF%E7%94%A8"><span class="nav-number">4.5.</span> <span class="nav-text">高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ingress-%E6%8E%A5%E5%85%A5%E5%BC%82%E5%B8%B8"><span class="nav-number">5.</span> <span class="nav-text">Ingress 接入异常</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#503-Service-Temporarily-Unavailable"><span class="nav-number">5.1.</span> <span class="nav-text">503 Service Temporarily Unavailable</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%9A%E6%B3%A8"><span class="nav-number">7.</span> <span class="nav-text">脚注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">COSMOS</p>
  <div class="site-description" itemprop="description">得 能 莫 忘</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="../archives/">
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="../categories/">
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="../tags/">
        <span class="site-state-item-count">74</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://csms.tech/202209281614/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="../images/avatar.gif">
      <meta itemprop="name" content="COSMOS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L B T">
      <meta itemprop="description" content="得 能 莫 忘">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kubernetes 常见错误总结 | L B T">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kubernetes 常见错误总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-28 16:14:41" itemprop="dateCreated datePublished" datetime="2022-09-28T16:14:41+08:00">2022-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-08-23 09:53:38" itemprop="dateModified" datetime="2023-08-23T09:53:38+08:00">2023-08-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">上层目录</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="../categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><ul>
<li>Centos7 5.4.212-1</li>
<li>Docker 20.10.18</li>
<li>containerd.io-1.6.8</li>
<li>kubectl-1.25.0</li>
<li>kubeadm-1.25.0</li>
<li>kubelet-1.25.0</li>
</ul>
<h1 id="POD-状态异常"><a href="#POD-状态异常" class="headerlink" title="POD 状态异常"></a>POD 状态异常</h1><h2 id="CrashLoopBackOff"><a href="#CrashLoopBackOff" class="headerlink" title="CrashLoopBackOff"></a>CrashLoopBackOff</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                                     READY   STATUS             RESTARTS       AGE</span><br><span class="line">test-centos7-7cc5dc6987-jz486            0/1     CrashLoopBackOff   8 (111s ago)   17m</span><br></pre></td></tr></table></figure>
<p>查看 <code>Pod</code> 详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod test-centos7-7cc5dc6987-jz486</span></span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                  From               Message</span><br><span class="line">  ----     ------     ----                 ----               -------</span><br><span class="line">  Normal   Scheduled  18m                  default-scheduler  Successfully assigned default/test-centos7-7cc5dc6987-jz486 to ops-kubernetes3</span><br><span class="line">  Normal   Pulled     16m (x5 over 18m)    kubelet            Container image &quot;centos:centos7.9.2009&quot; already present on machine</span><br><span class="line">  Normal   Created    16m (x5 over 18m)    kubelet            Created container centos7</span><br><span class="line">  Normal   Started    16m (x5 over 18m)    kubelet            Started container centos7</span><br><span class="line">  Warning  BackOff    3m3s (x71 over 18m)  kubelet            Back-off restarting failed container</span><br></pre></td></tr></table></figure>
<p>结果显示，<code>Reason</code> 为 <code>BackOff</code>，<code>Message</code> 显示 <code>Back-off restarting failed container</code></p>
<p><strong>可能原因</strong> ：</p>
<p><code>Back-off restarting failed container</code> 的原因，通常是因为，容器内 PID 为 1 的进程退出导致（通常用户在构建镜像执行 <code>CMD</code> 时，启动的程序，均是 PID 为1）<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Back-off restarting failed container 怎么办](https://cloud.tencent.com/developer/article/1931089)">[1]</span></a></sup></p>
<p>容器进程退出（命令执行结束或者进程异常结束），则容器生命周期结束。kubernetes 控制器检查到容器退出，会持续重启容器。针对此种情况，需要检查镜像，是否不存在常驻进程，或者常驻进程异常。</p>
<p>针对此种情况，可以单独使用 <code>docker</code> 客户端部署镜像，查看镜像的运行情况，如果部署后，容器中的进程立马结束或退出，则容器也会随之结束。</p>
<span id="more"></span>
<h2 id="POD-状态为-InvalidImageName"><a href="#POD-状态为-InvalidImageName" class="headerlink" title="POD 状态为 InvalidImageName"></a>POD 状态为 InvalidImageName</h2><p><strong>错误场景</strong> ： </p>
<p><code>Pod</code> 状态显示 <code>InvalidImageName</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n cs</span><br><span class="line">NAME               READY   STATUS              RESTARTS   AGE</span><br><span class="line">54fdc56754-qrlt6   0/2     InvalidImageName    0          14s</span><br><span class="line">8486f49b89-zp25b   0/2     Init:ErrImagePull   0          7s</span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>镜像的 url 地址中，以 <code>http://</code> 或 <code>https://</code> 开头。配置中镜像的 url 地址中无需指定协议（<code>http://</code> 或 <code>https://</code>） </p>
<h2 id="Pod-状态为-Error"><a href="#Pod-状态为-Error" class="headerlink" title="Pod 状态为 Error"></a>Pod 状态为 Error</h2><h3 id="The-node-was-low-on-resource-ephemeral-storage"><a href="#The-node-was-low-on-resource-ephemeral-storage" class="headerlink" title="The node was low on resource: ephemeral-storage"></a>The node was low on resource: ephemeral-storage</h3><p><strong>错误场景</strong>：</p>
<p>查看 Pod 状态，显示 Error</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS                   RESTARTS   AGE</span><br><span class="line">front-7df8ccc4c7-xhp6s    0/1     Error                    0          5h42m</span><br></pre></td></tr></table></figure>
<p>检查 Pod 的具体信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod front-7df8ccc4c7-xhp6s</span></span><br><span class="line">...</span><br><span class="line">Status:       Failed</span><br><span class="line">Reason:       Evicted</span><br><span class="line">Message:      The node was low on resource: ephemeral-storage. Container php was using 394, which exceeds its request of 0. </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>其中包含异常的关键信息：<code>Status:       Failed</code>，<code>Reason:       Evicted</code>，具体原因为 <code>The node was low on resource: ephemeral-storage</code></p>
<p>检查节点上的 Kuberlet 日志，搜索关键字 <code>evicte</code> 或者 <code>disk</code> ，也可以看到系统上文件系统空间使用率超过了阈值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet  | grep -i -e disk -e evict</span></span><br><span class="line"> image_gc_manager.go:310] &quot;Dis usage on image filesystem is over the high threshold, trying to free bytes down to the low threshold&quot; usage=85 highThreshold=85 amountToFree=5122092236 lowThreshold=80</span><br><span class="line"> eviction_manager.go:349] &quot;Eviction manager: must evict pod(s) to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"> eviction_manager.go:338] &quot;Eviction manager: attempting to reclaim&quot; resourceName=&quot;ephemeral-storage&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>可能原因</strong> ：</p>
<p>根据以上信息，可知 Pod 异常是因为 <code>The node was low on resource: ephemeral-storage</code>，表示 <strong>临时存储资源</strong> 不足导致节点处于 <code>Tainted</code> ，其上的 Pod 被驱逐(<code>Evicted</code>)</p>
<p><strong><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">本地临时存储说明</a></strong></p>
<p>针对此种情况，如果某 Pod 的临时存储用量超出了你所允许的范围，kubelet 会向其发出逐出（<code>eviction</code>）信号，触发该 Pod 被逐出所在节点。</p>
<p>如果用于可写入容器镜像层、节点层面日志或者 <code>emptyDir</code> 卷的文件系统中可用空间太少， 节点会为自身设置本地存储不足的污点(<code>Tainted</code>)标签。 这一污点会触发对那些无法容忍该污点的 Pod 的逐出操作。</p>
<p><strong>解决方法</strong> ：</p>
<ul>
<li><p>增加磁盘空间</p>
</li>
<li><p>调整 <code>kubelet</code> 的 <code>nodefs.available</code> 的 threshold 值</p>
<p>  修改节点上的 <code>kubelet</code> 的启动配置文件 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，添加以下启动参数，主要为定义环境变量 <code>KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</code>，并将其添加到启动参数中</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;KUBELET_EVICT_NODEFS_THRESHOLD_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICT_NODEFS_THRESHOLD_ARGS</span><br></pre></td></tr></table></figure>
<p>  修改之后重启 <code>kubelet</code> 服务，并通过日志查看 <code>nodefs.available</code> 的新值是否生效</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart kubelet</span><br><span class="line"></span><br><span class="line">$ journalctl -u kubelet | grep -i nodefs</span><br><span class="line">17604 container_manager_linux.go:267] &quot;Creating Container Manager object based on Node Config&quot; nodeConfig=&#123;RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:&#123;KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:&#123;&#125;] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[&#123;Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05&#125; GracePeriod:0s MinReclaim:&lt;nil&gt;&#125;]&#125; QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>日志中看到 <code>Signal:nodefs.available Operator:LessThan Value:&#123;Quantity:&lt;nil&gt; Percentage:0.05</code>，表明更改生效。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ephemeral-storage 问题](https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/)">[2]</span></a></sup></p>
</li>
</ul>
<h2 id="Pod-状态为-Init"><a href="#Pod-状态为-Init" class="headerlink" title="Pod 状态为 Init"></a>Pod 状态为 Init</h2><h3 id="Unable-to-attach-or-mount-volumes"><a href="#Unable-to-attach-or-mount-volumes" class="headerlink" title="Unable to attach or mount volumes"></a>Unable to attach or mount volumes</h3><p>Pod 启动异常，查看 Pod 状态为 <code>Init:0/1</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                          READY   STATUS     RESTARTS   AGE</span><br><span class="line">admin-cbb479556-j9qg2    0/1     Init:0/1   0          3m37s</span><br></pre></td></tr></table></figure>
<p>查看 Pod 的详细描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod admin-cbb479556-j9qg2</span></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason       Age    From               Message</span><br><span class="line">  ----     ------       ----   ----               -------</span><br><span class="line">  Normal   Scheduled    3m41s  default-scheduler  Successfully assigned admin-cbb479556-j9qg2 to k8s-work2</span><br><span class="line">  Warning  FailedMount  99s    kubelet            Unable to attach or mount volumes: unmounted volumes=[logs], unattached volumes=[wwwroot kube-api-access-z8745 logs]: timed out waiting for the condition</span><br><span class="line">  Warning  FailedMount  42s    kubelet            MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</span><br><span class="line">Mounting command: mount</span><br><span class="line">Mounting arguments: -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</span><br><span class="line">Output: mount.nfs: Connection timed out</span><br></pre></td></tr></table></figure>

<p>根据 <code>Events</code> 中输出的信息，<code>MountVolume.SetUp failed for volume &quot;uat-nfs-pv&quot; : mount failed: exit status 32</code>，显示挂载卷失败，输出中包含了挂载卷时使用的命令和参数（<code>mount -t nfs 34.230.1.1:/data/NFSDataHome /var/lib/kubelet/pods/9d9a4807-706c-4369-b8be-b5727ee6aa8f/volumes/kubernetes.io~nfs/uat-nfs-pv</code>）及命令失败后的返回结果（<code>mount.nfs: Connection timed out</code>）</p>
<p>根据 <code>Events</code> 中的信息，查看配置，发现此卷为 NFS 类型的 PV，根据报错排查，此例原因为 NFS 的服务器地址填写错误，更新 PV 配置中的 NFS Server 的地址后，Pod 正常启动。</p>
<h2 id="ContainerCreating"><a href="#ContainerCreating" class="headerlink" title="ContainerCreating"></a>ContainerCreating</h2><h3 id="dbus-connection-closed-by-user"><a href="#dbus-connection-closed-by-user" class="headerlink" title="dbus: connection closed by user"></a>dbus: connection closed by user</h3><p>更新 <code>DaemonSet</code> 类型的 <code>node_exporter</code>，其中一个节点上的 Pod 未创建成功，状态一直保持在 <code>ContainerCreating</code>，检查 Pod 的详细描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -n prometheus -o wide</span></span><br><span class="line">NAME                         READY   STATUS              RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">node-exporter-glnk5          1/1     Running             0          28h     172.31.8.197    work2       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-kzs2r          1/1     Running             1          28h     172.31.100.86   work1       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-nxz9v          0/1     ContainerCreating   0          5m30s   172.31.100.38   master      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-vpkwt          1/1     Running             0          31m     172.31.100.69   work4       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-wft7v          1/1     Running             0          14m     172.31.14.7     work3       &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prometheus-67ccbbd78-zqw9x   1/1     Running             0          46h     10.244.14.75    work2       &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe pod -n prometheus node-exporter-nxz9v</span></span><br><span class="line">Name:         node-exporter-nxz9v</span><br><span class="line">Namespace:    prometheus</span><br><span class="line">Priority:     0</span><br><span class="line"></span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Pending</span><br><span class="line">IP:           172.31.100.38</span><br><span class="line">IPs:</span><br><span class="line">  IP:           172.31.100.38</span><br><span class="line">Controlled By:  DaemonSet/node-exporter</span><br><span class="line">Containers:</span><br><span class="line">  node-exporter:</span><br><span class="line">    Container ID:  </span><br><span class="line">    Image:         prom/node-exporter</span><br><span class="line">    Image ID:      </span><br><span class="line">    Port:          9100/TCP</span><br><span class="line">    Host Port:     9100/TCP</span><br><span class="line">    Args:</span><br><span class="line">      --path.procfs</span><br><span class="line">      /host/proc</span><br><span class="line">      --path.sysfs</span><br><span class="line">      /host/sys</span><br><span class="line">      --collector.disable-defaults</span><br><span class="line">      --collector.cpu</span><br><span class="line">      --collector.cpufreq</span><br><span class="line">      --collector.meminfo</span><br><span class="line">      --collector.diskstats</span><br><span class="line">      --collector.filesystem</span><br><span class="line">      --collector.filefd</span><br><span class="line">      --collector.loadavg</span><br><span class="line">      --collector.netdev</span><br><span class="line">      --collector.netstat</span><br><span class="line">      --collector.nfs</span><br><span class="line">      --collector.os</span><br><span class="line">      --collector.stat</span><br><span class="line">      --collector.time</span><br><span class="line">      --collector.udp_queues</span><br><span class="line">      --collector.uname</span><br><span class="line">      --collector.xfs</span><br><span class="line">      --collector.netclass</span><br><span class="line">      --collector.vmstat</span><br><span class="line">      --collector.systemd</span><br><span class="line">      --collector.systemd.unit-include</span><br><span class="line">      (sshd|crond|iptables|systemd-journald|kubelet|containerd).service</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       ContainerCreating</span><br><span class="line">    Ready:          False</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                    Age                  From               Message</span><br><span class="line">  ----     ------                    ----                 ----               -------</span><br><span class="line">  Normal   Scheduled                 5m24s                default-scheduler  Successfully assigned prometheus/node-exporter-nxz9v to master</span><br><span class="line">  Warning  FailedCreatePodContainer  1s (x26 over 5m24s)  kubelet            unable to ensure pod container exists: failed to create container for [kubepods besteffort pode526f19a-57d6-417c-ba5a-fb0f232d31c6] : dbus: connection closed by user</span><br></pre></td></tr></table></figure>
<p>错误信息显示为 <code>unable to ensure pod container exists: failed to create container for [kubepods besteffort pode526f19a-57d6-417c-ba5a-fb0f232d31c6] : dbus: connection closed by user</code></p>
<p>查看 <code>kubelet</code> 日志，显示同样的日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet</span></span><br><span class="line">master kubelet[1160]: E0707 14:40:55.036424    1160 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: I0707 14:40:55.036455    1160 qos_container_manager_linux.go:138] &quot;Failed to reserve QoS requests&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: E0707 14:41:00.263041    1160 qos_container_manager_linux.go:328] &quot;Failed to update QoS cgroup configuration&quot; err=&quot;dbus: connection closed by user&quot;</span><br><span class="line">master kubelet[1160]: E0707 14:41:00.263152    1160 pod_workers.go:190] &quot;Error syncing pod, skipping&quot; err=&quot;failed to ensure that the pod: 0cdaf660-bb6a-40ee-99ae-21dff3b55411 cgroups exist and are correctly applied: failed to create container for [kubepods besteffort pod0cdaf660-bb6a-40ee-99ae-21dff3b55411] : dbus: connection closed by user&quot; pod=&quot;prometheus/node-exporter-rcd8x&quot; podUID=0cdaf660-bb6a-40ee-99ae-21dff3b55411</span><br></pre></td></tr></table></figure>
<p>根据以上日志信息，问题原因为 <code>kubelet</code> 和系统服务 <code>dbus</code> 通信异常，可以 <strong>通过重启 <code>kubelet</code> 服务</strong> 的方法解决此问题。</p>
<h3 id="“cni0”-already-has-an-IP-address-different-from"><a href="#“cni0”-already-has-an-IP-address-different-from" class="headerlink" title="“cni0” already has an IP address different from"></a>“cni0” already has an IP address different from</h3><p>集群中创建的 POD 状态一直处于 <code>ContainerCreating</code>，检查 Pod 详细信息</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># kubectl describe pod -n cattle-system cattle-cluster-agent-7d766b5476-hsq45</span><br><span class="line">...</span><br><span class="line">FailedCreatePodSandBox  82s (x4 over 85s)   kubelet            (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;2d58156e838349a79da91e0a6d8bccdec0e62c5f5c9ca6a1c30af6186d6253b1&quot; network for pod &quot;cattle-cluster-agent-7d766b5476-hsq45&quot;: networkPlugin cni failed to set up pod &quot;cattle-cluster-agent-7d766b5476-hsq45_cattle-system&quot; network: failed to delegate add: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.2.1/24</span><br></pre></td></tr></table></figure>
<p>关键信息 <code>failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.2.1/24</code>。</p>
<p>检查节点上的 IP 信息，发现 <code>flannel.1</code> 网段和 <code>cni0</code> 网段不一致。可能因为 <code>flannel</code> 读取的配置错误，<em><strong>重启节点后恢复</strong></em>。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># ip add</span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8951 qdisc noqueue state UNKNOWN group default </span><br><span class="line">    link/ether b2:b1:12:2d:8c:66 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.2.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::b0b1:12ff:fe2d:8c66/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8951 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether ca:88:b1:51:0f:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.0.1/24 brd 10.244.2.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::c888:b1ff:fe51:f02/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<h1 id="网络问题"><a href="#网络问题" class="headerlink" title="网络问题"></a>网络问题</h1><h2 id="同一个节点上的-Pod-之间网络不通"><a href="#同一个节点上的-Pod-之间网络不通" class="headerlink" title="同一个节点上的 Pod 之间网络不通"></a>同一个节点上的 Pod 之间网络不通</h2><p><strong>问题现象</strong>：</p>
<p>同一个节点上的 <code>Pod</code> 之间网络不通</p>
<p><strong>排查思路</strong>：</p>
<ul>
<li>检查系统内核配置是否开启转发 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sysctl -a | grep net.ipv4.ip_forward</span></span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure></li>
<li>检查 <code>iptables</code> 是否禁止转发，<a href="https://csms.tech/202209121102/#集群中所有计算机之间具有完全的网络连接"><code>iptables</code> 防火墙配置参考</a></li>
<li>为了定位是否为 <code>iptables</code> 影响，开关闭 <code>iptables</code> 再进行测试，如果关闭防火墙后可以通信，可以确定是防火墙规则导致，需要检查防火墙规则。</li>
<li>更深入的排查，可以部署 <a target="_blank" rel="noopener" href="https://hub.docker.com/r/antrea/netshoot/tags"><code>netshoot</code> 容器</a> 进行抓包定位，</li>
</ul>
<h2 id="Pod-无法访问到外部-Internet-网络"><a href="#Pod-无法访问到外部-Internet-网络" class="headerlink" title="Pod 无法访问到外部 Internet 网络"></a>Pod 无法访问到外部 Internet 网络</h2><p>某个节点上，Pod 无法外部主机的服务（端口 6603&#x2F;tcp）。分别在 Pod ，节点 <code>cni0</code> 网卡，节点出口网卡 <code>eth0</code> ，目标服务网卡上抓包。此例中 Pod IP 为 <code>10.244.4.173</code>，目标服务的 IP 地址为 <code>50.18.6.225</code></p>
<p>查看 Pod 抓包结果</p>
<p><img src="https://i.csms.tech/img_102.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看 节点 <code>cni0</code> 网卡 的抓包</p>
<p><img src="https://i.csms.tech/img_103.png"></p>
<p>可以看到源 IP 为 Pod 地址，目标为服务 IP 的 <code>6603/tcp</code> 的请求发送后，未收到 TCP 连接建立的响应。查看节点出口网卡 <code>eth0</code> 的抓包。</p>
<p><img src="https://i.csms.tech/img_104.png"></p>
<p><strong>此处看到的源 IP 依然是  Pod 的 IP 地址，此处存在问题</strong>。在云主机的场景中，如果数据包以这种结构发送出去，数据包到了 Internet 网关将拒绝它，因为网关 NAT（将 VM 的 IP 转换为公网 IP） 只了解连接到 VM 的 IP 地址。</p>
<p>正常情况下，Pod 的流量到节点的出口网卡之前，是应该经过 <code>iptables</code> 执行源 NAT - <strong>更改数据包源，使数据包看起来来自 VM 而不是 Pod</strong>。有了正确的源 IP，数据包才可以离开 VM 进入 Internet</p>
<p>此种情况下，数据包可以从节点的出口网卡发送出去，但是到了 Internet 网关将会被丢弃，因此目标服务无法接收到请求，查看目标服务器上的抓包，确实未收到来自此 Pod 的请求。</p>
<p>此处的 <strong>源 NAT</strong> 是由 <code>iptables</code>  负责执行，流入节点出口网卡的数据包未被正确的 <strong>源 NAT</strong>，有可能是因为 <code>kube-proxy</code> 维护的网络规则错误，或者因为 <code>iptables</code> 规则配置错误。可以通过重启 <code>kube-proxy</code> （由服务 <code>kubelet</code> 管理）和 <code>iptables</code> 服务尝试恢复。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart iptables</span><br></pre></td></tr></table></figure>
<p>本示例中，重启这 2 个服务后，Pod 恢复正常。</p>
<h2 id="Pod-间歇性无法连接外部数据库"><a href="#Pod-间歇性无法连接外部数据库" class="headerlink" title="Pod 间歇性无法连接外部数据库"></a>Pod 间歇性无法连接外部数据库</h2><p>集群中的 Pod 出现连接集群之外的数据库服务超时，且出现频率较高</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42684642/article/details/105775436">参考文章</a></p>
<h2 id="跨节点-Pod-无法访问"><a href="#跨节点-Pod-无法访问" class="headerlink" title="跨节点 Pod 无法访问"></a>跨节点 Pod 无法访问</h2><h3 id="环境信息-1"><a href="#环境信息-1" class="headerlink" title="环境信息"></a>环境信息</h3><ul>
<li>Centos 7 5.4.242-1</li>
<li>Kubernetes v1.25.4</li>
<li>kubernetes-cni-1.2.0-0</li>
<li>flannel v0.21.4</li>
</ul>
<p>集群中有 1 个 master 节点， 2 个 work 节点，节点状态均正常，master 无法 ping worker1 上面的 Pod，可以 ping 通 worker2 节点上面的 Pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -A -o wide</span></span><br><span class="line">NAME          STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">k8s-master1   Ready    control-plane   23h   v1.25.4   192.168.142.10   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker1   Ready    &lt;none&gt;          23h   v1.25.4   192.168.142.11   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker2   Ready    &lt;none&gt;          22h   v1.25.4   192.168.142.12   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A -o wide</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS        AGE   IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   1 (4m17s ago)   28m   10.244.1.4       k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0               50m   10.244.2.3       k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 10.244.1.4</span></span><br><span class="line">PING 10.244.1.4 (10.244.1.4) 56(84) bytes of data.</span><br><span class="line">^C</span><br><span class="line">--- 10.244.1.4 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 1001ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 10.244.2.3</span></span><br><span class="line">PING 10.244.2.3 (10.244.2.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=1 ttl=63 time=4.27 ms</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=2 ttl=63 time=0.468 ms</span><br><span class="line">64 bytes from 10.244.2.3: icmp_seq=3 ttl=63 time=0.443 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.244.2.3 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2056ms</span><br><span class="line">rtt min/avg/max/mdev = 0.443/1.729/4.277/1.801 ms</span><br></pre></td></tr></table></figure>
<p>由此可判断问题大概率出现在 worker1 节点，首先检查 worker1 节点上的 <code>flannel</code> 容器是否正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A -o wide</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS      AGE    IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   0             9m8s   10.244.1.4       k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0             31m    10.244.2.3       k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-d42lm                 1/1     Running   0             22h    192.168.142.11   k8s-worker1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-lqp5v                 1/1     Running   0             22h    192.168.142.10   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel   kube-flannel-ds-w675f                 1/1     Running   0             70m    192.168.142.12   k8s-worker2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>看到 worker1 节点上的 flannel 容器运行正常。在 worker1 节点上检查 <code>flannel</code> 进程及端口信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">netstat -anutp</span></span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      1817/kubelet        </span><br><span class="line">tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      2224/kube-proxy     </span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1039/sshd           </span><br><span class="line">tcp        0      0 192.168.142.11:47468    192.168.142.10:6443     ESTABLISHED 2224/kube-proxy     </span><br><span class="line">tcp        0      0 192.168.142.11:56584    192.168.142.10:6443     ESTABLISHED 1817/kubelet        </span><br><span class="line">tcp        0     44 192.168.142.11:22       192.168.142.1:62099     ESTABLISHED 1108/sshd: root@pts </span><br><span class="line">tcp        0      0 192.168.142.11:40574    10.96.0.1:443           ESTABLISHED 2566/flanneld       </span><br><span class="line">tcp6       0      0 :::34939                :::*                    LISTEN      1433/cri-dockerd    </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      1817/kubelet        </span><br><span class="line">tcp6       0      0 :::10256                :::*                    LISTEN      2224/kube-proxy     </span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1039/sshd</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ps -elf | grep flannel</span></span><br><span class="line">4 S root       2566   2539  0  80   0 - 353654 futex_ 14:44 ?       00:00:02 /opt/bin/flanneld --ip-masq --kube-subnet-mgr</span><br></pre></td></tr></table></figure>
<p>检查发现 <code>flanneld</code> 进程存在，但是端口未启动，检查 <code>flannel</code> 容器日志输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a | grep flannel | grep -v <span class="string">&quot;Exited&quot;</span></span></span><br><span class="line">92fab879c75b   11ae74319a21                 &quot;/opt/bin/flanneld -…&quot;   34 minutes ago      Up 34 minutes                           k8s_kube-flannel_kube-flannel-ds-77bwd_kube-flannel_078dde8c-573b-4db4-939e-d3dd353477f7_1</span><br><span class="line">237a82c1378a   registry.k8s.io/pause:3.6    &quot;/pause&quot;                 34 minutes ago      Up 34 minutes                           k8s_POD_kube-flannel-ds-77bwd_kube-flannel_078dde8c-573b-4db4-939e-d3dd353477f7_1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker logs 92fab879c75b</span></span><br><span class="line">failed to add vxlanRoute</span><br><span class="line"></span><br><span class="line">network is down</span><br></pre></td></tr></table></figure>
<p>关键错误信息 <code>failed to add vxlanRoute</code>, <code>network is down</code>，<a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/issues/844">参考案例</a>，重启服务器。恢复正常。</p>
<h2 id="coredns-无法解析域名"><a href="#coredns-无法解析域名" class="headerlink" title="coredns 无法解析域名"></a>coredns 无法解析域名</h2><p>Pod 中无法解析域名。</p>
<p>集群相关信息如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get svc -A</span></span><br><span class="line">NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">default       kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP                  25h</span><br><span class="line">kube-system   kube-dns     ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   25h</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在容器中测试 dns 相关信息，访问外部 IP 和 Kubernetes API Server 的 Service 地址均正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping 8.8.8.8</span></span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=1 ttl=127 time=37.2 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=2 ttl=127 time=36.9 ms</span><br><span class="line">^C</span><br><span class="line">--- 8.8.8.8 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1001ms</span><br><span class="line">rtt min/avg/max/mdev = 36.946/37.085/37.224/0.139 ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -v 10.96.0.1:443</span></span><br><span class="line">* About to connect() to 10.96.0.1 port 443 (#0)</span><br><span class="line">*   Trying 10.96.0.1...</span><br><span class="line">* Connected to 10.96.0.1 (10.96.0.1) port 443 (#0)</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GET / HTTP/1.1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">User-Agent: curl/7.29.0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Host: 10.96.0.1:443</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Accept: */*</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">* HTTP 1.0, assume close after body</span></span><br><span class="line">&lt; HTTP/1.0 400 Bad Request</span><br><span class="line">&lt; </span><br><span class="line">Client sent an HTTP request to an HTTPS server.</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure>
<p>容器中的 dns 配置为 <code>kube-dns</code> 的 Service 的 IP，测试其端口，显示 <code>Connection refused</code>。测试解析集群内部域名，结果无法解析。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/resolv.conf</span> </span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl 10.96.0.10:53</span></span><br><span class="line">curl: (7) Failed connect to 10.96.0.10:53; Connection refused</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping svc.cluster.local</span></span><br><span class="line">ping: svc.cluster.local: Name or service not known</span><br></pre></td></tr></table></figure>
<p>通过以上步骤，大概可以确定，Pod 的网络正常，应该是 <code>kube-dns</code> 出问题，导致 Pod 无法解析域名。</p>
<p>Service 是通过 Endpoint 和后端的具体的 Pod 关联起来向外提供服务，首先检查 <code>kube-dns</code> 的 Service 对应的 Endpoint，看是否正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ep -A</span></span><br><span class="line">NAMESPACE     NAME         ENDPOINTS             AGE</span><br><span class="line">default       kubernetes   192.168.142.10:6443   25h</span><br><span class="line">kube-system   kube-dns                           25h</span><br></pre></td></tr></table></figure>
<p>检查发现，<code>kube-dns</code> 对应的 ENDPOINTS 列表为空。删除 <code>coredns</code> 容器，重新创建。再次检查后，发现 <code>kube-dns</code> 的 Service 对应的 Endpoint 恢复正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete pod -n kube-system coredns-565d847f94-bzr62 coredns-565d847f94-vmddh</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                  READY   STATUS    RESTARTS       AGE</span><br><span class="line">default        tet-deployment-fbc96cc5d-hlqkg        1/1     Running   1 (115m ago)   139m</span><br><span class="line">default        tet-deployment-fbc96cc5d-mcjzg        1/1     Running   0              162m</span><br><span class="line">kube-flannel   kube-flannel-ds-77bwd                 1/1     Running   1 (115m ago)   129m</span><br><span class="line">kube-flannel   kube-flannel-ds-lqp5v                 1/1     Running   0              25h</span><br><span class="line">kube-flannel   kube-flannel-ds-w675f                 1/1     Running   0              3h21m</span><br><span class="line">kube-system    coredns-565d847f94-8wmg7              1/1     Running   0              9s</span><br><span class="line">kube-system    coredns-565d847f94-csc9f              0/1     Running   0              9s</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ep -A</span></span><br><span class="line">NAMESPACE     NAME         ENDPOINTS                                     AGE</span><br><span class="line">default       kubernetes   192.168.142.10:6443                           25h</span><br><span class="line">kube-system   kube-dns     10.244.1.7:53,10.244.1.7:53,10.244.1.7:9153   25h</span><br></pre></td></tr></table></figure>

<p>在 Pod 中重新测试解析，结果正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -v 10.96.0.10:53</span></span><br><span class="line">* About to connect() to 10.96.0.10 port 53 (#0)</span><br><span class="line">*   Trying 10.96.0.10...</span><br><span class="line">* Connected to 10.96.0.10 (10.96.0.10) port 53 (#0)</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">GET / HTTP/1.1</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">User-Agent: curl/7.29.0</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Host: 10.96.0.10:53</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Accept: */*</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">* Empty reply from server</span></span><br><span class="line">* Connection #0 to host 10.96.0.10 left intact</span><br><span class="line">curl: (52) Empty reply from server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping qq.com</span></span><br><span class="line">PING qq.com (61.129.7.47) 56(84) bytes of data.</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=1 ttl=127 time=308 ms</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=2 ttl=127 time=312 ms</span><br><span class="line">64 bytes from 61.129.7.47 (61.129.7.47): icmp_seq=3 ttl=127 time=312 ms</span><br><span class="line">^C</span><br><span class="line">--- qq.com ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 308.493/310.873/312.106/1.743 ms</span><br></pre></td></tr></table></figure>

<h3 id="dns-文件定位参考文档"><a href="#dns-文件定位参考文档" class="headerlink" title="dns 文件定位参考文档"></a>dns 文件定位参考文档</h3><p><a target="_blank" rel="noopener" href="https://www.gylinux.cn/4299.html">故障排查：Kubernetes 中 Pod 无法正常解析域名</a></p>
<h1 id="集群状态异常"><a href="#集群状态异常" class="headerlink" title="集群状态异常"></a>集群状态异常</h1><h2 id="节点状态-NotReady"><a href="#节点状态-NotReady" class="headerlink" title="节点状态 NotReady"></a>节点状态 NotReady</h2><h3 id="PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago"><a href="#PLEG-is-not-healthy-pleg-was-last-seen-active-10m13-755045415s-ago" class="headerlink" title="PLEG is not healthy: pleg was last seen active 10m13.755045415s ago"></a>PLEG is not healthy: pleg was last seen active 10m13.755045415s ago</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION</span><br><span class="line">k8s-master1   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-master3   Ready      control-plane   14d   v1.24.7</span><br><span class="line">k8s-work1     NotReady   &lt;none&gt;          14d   v1.24.7</span><br><span class="line">k8s-work2     Ready      &lt;none&gt;          14d   v1.24.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看节点详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-work1</span></span><br><span class="line">...</span><br><span class="line">Conditions:</span><br><span class="line">  Ready                False   Tue, 15 Nov 2022 10:14:49 +0800   Tue, 15 Nov 2022 10:07:39 +0800   KubeletNotReady              PLEG is not healthy: pleg was last seen active 10m13.755045415s ago; threshold is 3m0s</span><br></pre></td></tr></table></figure>

<h4 id="异常原因"><a href="#异常原因" class="headerlink" title="异常原因"></a>异常原因</h4><p>集群因为此原因（<code>PLEG is not healthy: pleg was last seen active ***h**m***s ago;</code>）状态变为 <code>NotReady</code>，通常是因为节点超负载。</p>
<h3 id="container-runtime-is-down-container-runtime-not-ready"><a href="#container-runtime-is-down-container-runtime-not-ready" class="headerlink" title="container runtime is down, container runtime not ready"></a>container runtime is down, container runtime not ready</h3><p><strong>排查过程</strong>：</p>
<p>检查集群中的 Pod 分布情况时，发现某一节点上几乎所有的 Pod 都被调度去了其他节点，当前检查时此节点的状态已经是 <code>Ready</code>，针对此情况进行分析。</p>
<ol>
<li><p>确定问题发生的大概时间段</p>
<p> 根据 Pod 在其他节点上面被启动的时间，可以大概确定节点异常的时间，根据此时间段可以缩小排查的时间范围。此示例中问题发生的时间大概在 <code>Nov 25 04:49:00</code> 前后。</p>
</li>
<li><p>检查 <code>kubelet</code> 日志</p>
<p> 根据已经推断出的时间段，在 <strong>问题节点</strong> 上，检查 <code>kubelet</code> 日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ journalctl -u kubelet --since &quot;2022-11-25 4:40&quot; | grep -v -e &quot;failed to get fsstats&quot; -e &quot;invalid bearer token&quot; | more</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.153132   17604 generic.go:205] &quot;GenericPLEG: Unable to retrieve pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375524   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;PodSandboxFilter&#123;Id:,State:&amp;PodSandboxStateValue&#123;State:SANDBOX_READY,&#125;,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375559   17604 kuberuntime_sandbox.go:292] &quot;Failed to list pod sandboxes&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375578   17604 kubelet_pods.go:1153] &quot;Error listing containers&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375589   17604 kubelet.go:2162] &quot;Failed cleaning pods&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.476011   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.507861   17604 remote_runtime.go:680] &quot;ExecSync cmd from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; containerID=&quot;5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac&quot; cmd=[wget --no-check-certificate --spider -q http://localhost:8088/health]</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet[17604]: E1125 04:49:00.676271   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.076918   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet[17604]: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329558   17604 remote_runtime.go:536] &quot;ListContainers with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;&amp;ContainerFilter&#123;Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string&#123;&#125;,&#125;&quot;</span><br><span class="line">Nov 25 04:49:03 k8s-work2 kubelet[17604]: E1125 04:49:03.329585   17604 container_log_manager.go:183] &quot;Failed to rotate container logs&quot; err=&quot;failed to list containers: rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot;</span><br><span class="line"></span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: E1125 04:49:09.485356   17604 remote_runtime.go:168] &quot;Version from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:09 k8s-work2 kubelet[17604]: I1125 04:49:09.485486   17604 setters.go:532] &quot;Node became not ready&quot; node=&quot;k8s-work2&quot; condition=&#123;Type:Ready Status:False LastHeartbeatTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 LastTransitionTime:2022-11-25 04:49:09.485445614 +0800 CST m=+227600.229789769 Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p> 从以上日志中，可以看到关键的日志信息：</p>
<p> <code>&quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</code></p>
<p> <code>setters.go:532] &quot;Node became not ready&quot;</code>，    <code>Reason:KubeletNotReady Message:[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&#125;</code></p>
<p> 从以上日志信息可以看出，节点状态变为了 <code>not ready</code>，原因为 <code>container runtime is down, container runtime not ready</code>，本示例中 <code>container runtime</code> 为 <code>docker</code></p>
</li>
<li><p>检查 docker 服务日志</p>
<p> 根据上面的日志时间，检查 docker 服务的日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">journalctl -u docker --since &quot;2022-11-25 04:0&quot; | more</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.410342223+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/41e0dfe97b87c2b8ae941653fa8adbf93bf9358d91e967646e4549ab71b2f004/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.414773158+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.416474238+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: http: superfluous response.WriteHeader call from github.com/docker/docker/api/server/httputils.WriteJSON (httputils_write_json.go:11)</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd[15611]: time=&quot;2022-11-25T04:49:06.422844592+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure>
<p>根据日志可以看到关键日志 <code>write unix /var/run/docker.sock-&gt;@: write: broken pipe</code></p>
</li>
<li><p>检查 messages 日志</p>
<p> 查看对应时间段的系统日志</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.153089   17604 remote_runtime.go:356] &quot;ListPodSandbox with filter from runtime service failed&quot; err=&quot;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&quot; filter=&quot;nil&quot;</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375603   17604 kubelet.go:2166] &quot;Housekeeping took longer than 15s&quot; err=&quot;housekeeping took too long&quot; seconds=119.005290203</span><br><span class="line">Nov 25 04:49:00 k8s-work2 kubelet: E1125 04:49:00.375614   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;container runtime is down&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.178942   17604 kubelet.go:2359] &quot;Container runtime not ready&quot; runtimeReady=&quot;RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded&quot;</span><br><span class="line">Nov 25 04:49:01 k8s-work2 kubelet: E1125 04:49:01.878007   17604 kubelet.go:2010] &quot;Skipping pod synchronization&quot; err=&quot;[container runtime is down, container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: operation timeout: context deadline exceeded]&quot;</span><br><span class="line">Nov 25 04:49:06 k8s-work2 dockerd: time=&quot;2022-11-25T04:49:06.410127201+08:00&quot; level=error msg=&quot;Handler for GET /v1.40/containers/5cd867ce2a52311e79a20a113c7cedd2a233b3a52b556065b479f2dd11a14eac/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>根据 <code>kubelet</code> 服务日志，节点 <code>Not Ready</code> 的原因为 <code>docker down</code>，根据 docker 服务日志，docker 存在异常，但是此时执行 <code>docker</code> 相关命令，未发现异常。此问题多次出现，<code>docker engine</code> 版本为 <code>19.03.15-3</code>，之后尝试将 <code>docker engine</code> 版本升级为最新版本 <code>20.10.9</code>，问题未在出现。<a href="https://csms.tech/202208041317/#docker-ce-19-03-15-升级到-docker-ce-20-10-9"><code>docker engine</code> 升级参考</a> </p>
<h3 id="“Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”"><a href="#“Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”" class="headerlink" title="“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”"></a>“Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</h3><h4 id="环境信息-2"><a href="#环境信息-2" class="headerlink" title="环境信息"></a>环境信息</h4><ul>
<li>Kubernetes v1.21.2</li>
</ul>
<p>新增节点后，节点状态为 <code>NotReady</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME            STATUS     ROLES                  AGE     VERSION</span><br><span class="line">work2           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work3           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work4           Ready      &lt;none&gt;                 10d     v1.21.2</span><br><span class="line">work5           NotReady   &lt;none&gt;                 8m36s   v1.21.2</span><br><span class="line">master          Ready      control-plane,master   191d    v1.21.2</span><br></pre></td></tr></table></figure>
<p>Master 上查看节点的描述信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-api-work5</span> </span><br><span class="line">Name:               work5</span><br><span class="line">Roles:              &lt;none&gt;</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=work5</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line"></span><br><span class="line">Taints:             node.kubernetes.io/not-ready:NoExecute</span><br><span class="line">                    node.kubernetes.io/not-ready:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">Lease:</span><br><span class="line">  HolderIdentity:  work5</span><br><span class="line">  AcquireTime:     &lt;unset&gt;</span><br><span class="line">  RenewTime:       Wed, 05 Apr 2023 13:50:16 +0800</span><br><span class="line">Conditions:</span><br><span class="line">  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----                 ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  NetworkUnavailable   False   Wed, 05 Apr 2023 13:48:19 +0800   Wed, 05 Apr 2023 13:48:19 +0800   FlannelIsUp                  Flannel is running on this node</span><br><span class="line">  MemoryPressure       False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure         False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure          False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready                False   Wed, 05 Apr 2023 13:48:33 +0800   Wed, 05 Apr 2023 13:48:03 +0800   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span><br></pre></td></tr></table></figure>
<p>看到异常原因为 <code>container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</code></p>
<p>在 <code>work5</code> 节点上查看 <code>kubelet</code> 日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet -f</span></span><br><span class="line">Apr 05 13:52:03 work5 kubelet[19520]: E0405 13:52:03.952395   19520 kubelet.go:2211] &quot;Container runtime network not ready&quot; networkReady=&quot;NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized&quot;</span><br><span class="line"></span><br><span class="line">Apr 05 13:52:08 work5 kubelet[19520]: I0405 13:52:08.498481   19520 cni.go:204] &quot;Error validating CNI config list&quot; configList=&quot;&#123;\n  \&quot;name\&quot;: \&quot;cbr0\&quot;,\n  \&quot;cniVersion\&quot;: \&quot;0.3.1\&quot;,\n  \&quot;plugins\&quot;: [\n    &#123;\n      \&quot;type\&quot;: \&quot;flannel\&quot;,\n      \&quot;delegate\&quot;: &#123;\n        \&quot;hairpinMode\&quot;: true,\n        \&quot;isDefaultGateway\&quot;: true\n      &#125;\n    &#125;,\n    &#123;\n      \&quot;type\&quot;: \&quot;portmap\&quot;,\n      \&quot;capabilities\&quot;: &#123;\n        \&quot;portMappings\&quot;: true\n      &#125;\n    &#125;\n  ]\n&#125;\n&quot; err=&quot;[failed to find plugin \&quot;flannel\&quot; in path [/opt/cni/bin]]&quot;</span><br><span class="line">Apr 05 13:52:08 work5 kubelet[19520]: I0405 13:52:08.498501   19520 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no valid networks found in /etc/cni/net.d&quot;</span><br></pre></td></tr></table></figure>

<p>在 Master 节点上查看异常节点上的 <code>kube-flannel</code> POD 状态正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide -n kube-system</span></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-558bd4d5db-6wf7m         1/1     Running   0          18d    10.244.4.132    admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-558bd4d5db-zh9mw         1/1     Running   0          18d    10.244.4.144    admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-master                      1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master            1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master   1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-2lg9x            1/1     Running   0          18d    192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-5fpn8            1/1     Running   0          10d    192.168.100.69   work4       	  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-7ln98            1/1     Running   0          30m    192.168.100.59   work5   		  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-kvhhq            1/1     Running   0          17d    192.168.14.7     work3           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-vz4th            1/1     Running   0          17d    192.168.8.197    work2           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-xr84k            1/1     Running   0          18d    192.168.100.86   admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-9b7kt                 1/1     Running   0          30m    192.168.100.59   work5           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-c6ggk                 1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-gtlqt                 1/1     Running   0          17d    192.168.14.7     work3           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-n6s7p                 1/1     Running   0          10d    192.168.100.69   work4           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-p8m9d                 1/1     Running   0          17d    192.168.8.197    work2           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-qvks4                 1/1     Running   2          191d   192.168.100.86   admin           &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master            1/1     Running   1          191d   192.168.100.38   master          &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>接着检查新增节点上提供 <code>flannel</code> 组件的安装包，及相关目录中的文件是否存在异常 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kubernetes</span></span><br><span class="line">kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /opt/cni/bin/</span></span><br><span class="line">bandwidth  dhcp   firewall     host-local  loopback  portmap  sbr     tuning  vrf</span><br><span class="line">bridge     dummy  host-device  ipvlan      macvlan   ptp      static  vlan</span><br></pre></td></tr></table></figure>

<p>比对其他已存在的正常节点上的 <code>kubernetes-cni</code> 信息，发现其他节点上的 <code>kubernetes-cni</code> 版本为 <code>kubernetes-cni-0.8.7-0</code>，怀疑为版本问题导致，卸载问题节点上的 <code>kubernetes-cni-1.2.0-0</code>，重新安装 <code>kubernetes-cni-0.8.7-0</code>。卸载 <code>kubernetes-cni</code> 会导致之前安装的 <code>kubeadm</code> 和 <code>kubelet</code> 被卸载，也需要重新安装。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum remove kubernetes-cni-1.2.0-0</span></span><br><span class="line">...</span><br><span class="line">Removed:</span><br><span class="line">  kubernetes-cni.x86_64 0:1.2.0-0                                                                                                      </span><br><span class="line"></span><br><span class="line">Dependency Removed:</span><br><span class="line">  kubeadm.x86_64 0:1.21.2-0                                          kubelet.x86_64 0:1.21.2-0</span><br><span class="line"><span class="meta prompt_">  </span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install -y kubelet-1.21.2 kubeadm-1.21.2 kubectl-1.21.2 kubernetes-cni-0.8.7-0</span></span><br></pre></td></tr></table></figure>

<p>安装 <code>kubernetes-cni-0.8.7-0</code> 版本后，再次查看节点状态，变为 <code>Ready</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME            STATUS     ROLES                  AGE     VERSION</span><br><span class="line">work2           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work3           Ready      &lt;none&gt;                 17d     v1.21.2</span><br><span class="line">work4           Ready      &lt;none&gt;                 10d     v1.21.2</span><br><span class="line">work5           Ready      &lt;none&gt;                 8m36s   v1.21.2</span><br><span class="line">master          Ready      control-plane,master   191d    v1.21.2</span><br></pre></td></tr></table></figure>

<h3 id="Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”"><a href="#Container-runtime-network-not-ready”-networkReady-x3D-”NetworkReady-x3D-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized”" class="headerlink" title="Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”"></a>Container runtime network not ready” networkReady&#x3D;”NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized”</h3><p>节点状态 NotReady，检查节点上的 kubelet日志，显示 <code>Container runtime network not ready&quot; networkReady=&quot;NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized&quot;</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">k8s-master1   Ready      control-plane   35m   v1.25.4   192.168.142.10   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker1   Ready      &lt;none&gt;          30m   v1.25.4   192.168.142.11   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br><span class="line">k8s-worker2   NotReady   &lt;none&gt;          21m   v1.25.4   192.168.142.12   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.242-1.el7.elrepo.x86_64   docker://20.10.9</span><br></pre></td></tr></table></figure>

<p>检查发现节点上没有 CNI 配置文件 <code>/etc/cni/net.d/10-flannel.conflist</code>，拷贝正常节点上的配置到异常节点后，状态恢复正常。</p>
<h2 id="api-server-启动失败"><a href="#api-server-启动失败" class="headerlink" title="api-server 启动失败"></a>api-server 启动失败</h2><h3 id="错误场景"><a href="#错误场景" class="headerlink" title="错误场景"></a>错误场景</h3><p>api server 启动失败，执行 <code>kubectl</code> 命令输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">The connection to the server kube-apiserver.uat.148962587001:6443 was refused - did you specify the right host or port?</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>检查 Api Server 监听的端口 6443 ，显示端口未启动。</p>
<p>检查 Api Server 对应的容器状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a | grep api</span></span><br><span class="line">81688b9cbe45  1f38c0b6a9d1   &quot;kube-apiserver --ad…&quot;   14 seconds ago      Exited (1) 13 seconds ago                       k8s_kube-apiserver_kube-apiserver-k8s-uat-master1.148962587001_kube-system_c8a87f4921623c7bff57f5662ea486cc_25</span><br></pre></td></tr></table></figure>

<p>容器状态为 <code>Exited</code>，检查容器日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker logs 81688b9cbe45</span></span><br><span class="line">I1116 07:43:53.775588       1 server.go:558] external host was not specified, using 172.31.30.123</span><br><span class="line">I1116 07:43:53.776035       1 server.go:158] Version: v1.24.7</span><br><span class="line">I1116 07:43:53.776057       1 server.go:160] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot;</span><br><span class="line">E1116 07:43:53.776298       1 run.go:74] &quot;command failed&quot; err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</span><br></pre></td></tr></table></figure>
<p>日志显示 <code>err=&quot;open /etc/kubernetes/pki/apiserver.crt: no such file or directory&quot;</code>，检查文件 <code>/etc/kubernetes/pki/apiserver.crt</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line">ls: cannot access /etc/kubernetes/pki/apiserver.crt: No such file or directory</span><br></pre></td></tr></table></figure>

<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ul>
<li>发现此文件确实不存在。若有备份，从备份中恢复此文件。如果没有备份，<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1801882">参考文档</a> 恢复证书<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /k8s/backup/pki/apiserver.key /etc/kubernetes/pki/</span><br><span class="line">cp /k8s/backup/pki/apiserver.crt /etc/kubernetes/pki/</span><br></pre></td></tr></table></figure>
重启 <code>kubelet</code> 后检查 Api Server，发现服务正常启动<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>如果只是缺少了 <code>apiserver.key</code>，<code>apiserver.crt</code> 证书文件，可通过以下命令重新生成证书文件，<a href="https://csms.tech/202209121102/#集群之外的服务器使用-kubectl-报错">生成原理参考</a><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm init phase certs apiserver \</span></span><br><span class="line"><span class="language-bash">     --apiserver-advertise-address  10.150.0.21 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans  10.96.0.1 \</span></span><br><span class="line"><span class="language-bash">     --apiserver-cert-extra-sans 34.150.1.1</span></span><br><span class="line"> </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.21 34.150.1.1]</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="kubelet-启动失败"><a href="#kubelet-启动失败" class="headerlink" title="kubelet 启动失败"></a>kubelet 启动失败</h2><h3 id="failed-to-parse-kubelet-flag"><a href="#failed-to-parse-kubelet-flag" class="headerlink" title="failed to parse kubelet flag"></a>failed to parse kubelet flag</h3><p><code>kubelet</code> 重启失败，查看 <code>kubelet</code> 服务日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">&quot;command failed&quot; err=&quot;failed to parse kubelet flag: unknown flag: --network-plugin&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>问题原因</strong> 为 <strong>版本不匹配</strong>。集群版本为 <code>1.21.2</code>，检查问题节点上的组件版本信息，<code>kubelet</code> 变为了 <code>kubelet-1.27.3</code>，可能是升级了 <code>kubelet</code> 软件包版本导致。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kube</span></span><br><span class="line">kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line">kubectl-1.25.2-0.x86_64</span><br><span class="line">kubelet-1.27.3-0.x86_64</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看正常节点上的组件版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -qa | grep kube</span></span><br><span class="line">kubectl-1.21.2-0.x86_64</span><br><span class="line">kubernetes-cni-0.8.7-0.x86_64</span><br><span class="line">kubelet-1.21.2-0.x86_64</span><br><span class="line">kubeadm-1.21.2-0.x86_64</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>解决方法</strong> 为恢复问题节点上的组件版本和集群版本一致。</p>
<ol>
<li>在正常节点上下载软件安装包并拷贝到问题节点上 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yumdownloader kubernetes-cni-0.8.7-0 kubectl-1.21.2-0 kubeadm-1.21.2-0 kubelet-1.21.2-0</span><br></pre></td></tr></table></figure></li>
<li>在问题节点上卸载软件包并安装和集群一致版本的软件包 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum remove kubelet-1.27.3-0 kubectl-1.25.2-0.x86_64 kubernetes-cni-1.2.0-0.x86_64</span><br><span class="line"></span><br><span class="line">yum localinstall kubectl-1.21.2-0.x86_64.rpm  kubeadm-1.21.2-0.x86_64.rpm kubelet-1.21.2-0.x86_64.rpm kubernetes-cni-0.8.7-0.x86_64.rpm</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>重启服务，恢复正常 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-“"><a href="#misconfiguration-kubelet-cgroup-driver-quot-systemd-quot-is-different-from-docker-cgroup-driver-quot-cgroupfs-quot-“" class="headerlink" title="misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“"></a>misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;“</h3><p><code>kubelet</code> 服务启动失败</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl status kubelet</span></span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /usr/lib/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br><span class="line">   Active: activating (auto-restart) (Result: exit-code) since Thu 2023-07-13 14:18:33 CST; 2s ago</span><br><span class="line">     Docs: https://kubernetes.io/docs/</span><br><span class="line">  Process: 28476 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 28476 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Jul 13 14:18:33 k8s-node-5 systemd[1]: Unit kubelet.service entered failed state.</span><br><span class="line">Jul 13 14:18:33 k8s-node-5 systemd[1]: kubelet.service failed.</span><br></pre></td></tr></table></figure>

<p>检查 <code>kubelet</code> 服务日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line">Jul 13 14:18:43 k8s-node-5 kubelet[28572]: E0713 14:18:43.328660   28572 server.go:292] &quot;Failed to run kubelet&quot; err=&quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \&quot;systemd\&quot; is different from docker cgroup driver: \&quot;cgroupfs\&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>根据日志信息可知，<code>kubelet</code> 服务启动失败是因为 <code>kubelet</code> 使用的 <code>cgroup</code> 驱动是 <code>systemd</code>，而 docker 服务使用的是 <code>cgroupfs</code>。二者不一致导致 <code>kubelet</code> 无法启动。</p>
<p>修改 <code>docker</code> 服务使用的 <code>cgroup</code> 驱动为 <code>systemd</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF | sudo tee /etc/docker/daemon.json</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>重启 <code>docker</code> 服务后，<code>kubelet</code> 服务运行正常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<h2 id="服务不可用"><a href="#服务不可用" class="headerlink" title="服务不可用"></a>服务不可用</h2><h2 id="高可用集群中一个-master-节点异常导致整个集群中的所有应用服务不可用"><a href="#高可用集群中一个-master-节点异常导致整个集群中的所有应用服务不可用" class="headerlink" title="高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用"></a>高可用集群中一个 master 节点异常导致整个集群中的所有应用服务不可用</h2><a href="/202209121102/" title="参考此文档安装的高可用集群">参考此文档安装的高可用集群</a>，在 [创建高可用控制平面集群](https://csms.tech/202209121102/#创建高可用控制平面的集群) 中，要为 *`kube-apiserver` 创建负载均衡器*，本次问题环境中未创建负载均衡器，而是使用主机 `hosts`(`/etc/hosts`) 写入了 `kube-apiserver` 的域名及 IP 映射关系，示例如下
<figure class="highlight shell"><figcaption><span>/etc/hosts</span></figcaption><table><tr><td class="code"><pre><span class="line">172.31.26.116 k8s-master1 kube-api-svr-c1.f9s6u9xh.com</span><br><span class="line">172.31.19.164 k8s-master2 kube-api-svr-c1.f9s6u9xh.com</span><br><span class="line">172.31.21.3 k8s-master3 kube-api-svr-c1.f9s6u9xh.com</span><br><span class="line">172.31.16.124 k8s-worker1</span><br><span class="line">172.31.22.159 k8s-worker2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>集群中的所有节点都写入了以上内容，实现了节点通过 <code>/etc/hosts</code> 中的配置解析 <code>kube-apiserver</code> 的域名(<code>kube-api-svr-c1.f9s6u9xh.com</code>)。</p>
<p>正常情况下，所有节点都会将 <code>kube-apiserver</code> 解析为 <code>172.31.26.116 kube-api-svr-c1.f9s6u9xh.com</code>。</p>
<p>本次故障中，<code>k8s-master1</code> 节点因为 CPU 和内存满载，<code>k8s-master1</code> 异常，无法提供 <code>kube-apiserver</code> 服务。同时，集群中所有的应用都响应异常 ：<code>503 Service Temporarily Unavailable</code></p>
<p>理论上，本环境为 <em>高可用</em> 的Kubernetes 集群，一个 master 节点异常，不会影响集群提供正常的功能，但是此次只是因为 <code>k8s-master1</code> 这一个 master 节点异常，就导致了整个集群无法提供正常的功能。</p>
<p>复现问题分析，发现 <code>k8s-master1</code> 异常后，在其他主节点上使用 <code>kubectl</code> 命令，无法使用，原因为*根据节点 <code>/etc/hosts</code> 配置，<code>kube-apiserver</code> 解析到了异常的 <code>k8s-master1</code>*，修改 <code>k8s-master2</code> 上面的 <code>/etc/hosts</code> 配置，将 <code>172.31.19.164 k8s-master2 kube-api-svr-c1.f9s6u9xh.com</code> 放在第一行，以使 <code>kube-apiserver</code> 域名解析到 <code>k8s-master2</code>，<code>kubectl</code> 命令连接到 <code>k8s-master2</code> 即可正常查看集群状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE    VERSION</span><br><span class="line">k8s-master1   NotReady   control-plane   224d   v1.24.7</span><br><span class="line">k8s-master2   Ready      control-plane   224d   v1.24.7</span><br><span class="line">k8s-master3   NotReady   control-plane   224d   v1.24.7</span><br><span class="line">k8s-worker1   NotReady   &lt;none&gt;          224d   v1.24.7</span><br><span class="line">k8s-worker2   NotReady   &lt;none&gt;          224d   v1.24.7</span><br></pre></td></tr></table></figure>
<p>可以看到，集群中除了 <code>k8s-master2</code> ，其他所有节点都异常，原因为**除了 <code>k8s-master2</code> 其他节点都将 <code>apiserver</code> 解析到了异常的 <code>k8s-master1</code>**。修改所有节点的 <code>/etc/hosts</code> 文件，将 <code>kube-api-svr-c1.f9s6u9xh.com</code> 解析到 <code>k8s-master1</code> 之外的 master 节点，集群恢复正常。</p>
<p>本次故障的根本原因为<em><strong>本环境中未实现 Kubernetes 的高可用，虽然已经部署了高可用环境，但是因为 <code>apiserver</code> 的域名未实现高可用(负载均衡)，导致 <code>apiserver</code> 请求全部到了异常节点</strong></em>。要从根本上解决此问题，需要为 <code>apiserver</code> 的请求域名部署负载均衡实现真正的高可用。<em><strong>使用 <code>/etc/hosts</code> 将域名解析到多个 IP 不能实现高可用</strong></em>。</p>
<h1 id="Ingress-接入异常"><a href="#Ingress-接入异常" class="headerlink" title="Ingress 接入异常"></a>Ingress 接入异常</h1><h2 id="503-Service-Temporarily-Unavailable"><a href="#503-Service-Temporarily-Unavailable" class="headerlink" title="503 Service Temporarily Unavailable"></a>503 Service Temporarily Unavailable</h2><p><code>Deployment</code>，<code>Service</code>，<code>Ingress</code> 部署后，通过 <code>Ingress</code> 配置的域名访问，显示 <code>503 Service Temporarily Unavailable</code><br><img src="https://i.csms.tech/img_110.png"></p>
<p><strong>排查步骤</strong></p>
<p>检查 <code>Ingress-Nginx</code> Pod 的日志，检索对应域名日志，显示返回码为 503</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">52.77.198.154 - - [15/Dec/2022:02:10:59 +0000] &quot;GET /graph HTTP/1.1&quot; 503 592 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36&quot; 507 0.000 [prometheus-prometheus-service-8080] [] - - - - 00b07fe234401054153fdbd0ffafb158</span><br></pre></td></tr></table></figure>

<p>查看 Ingress 对应的 <code>Service</code>，从以下输出中可以看到对应的 <code>Service</code> 为 <code>prometheus-service</code>，端口为 8080</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ingress -n prometheus -o wide</span></span><br><span class="line">NAME            CLASS   HOSTS                     ADDRESS                      PORTS   AGE</span><br><span class="line">prometheus-ui   nginx   prometheus.example.com    172.31.23.72,172.31.27.193   80      19h</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe ingress prometheus-ui -n prometheus</span> </span><br><span class="line">Name:             prometheus-ui</span><br><span class="line">Labels:           &lt;none&gt;</span><br><span class="line">Namespace:        prometheus</span><br><span class="line">Address:          172.31.23.72,172.31.27.193</span><br><span class="line">Ingress Class:    nginx</span><br><span class="line">Default backend:  &lt;default&gt;</span><br><span class="line">Rules:</span><br><span class="line">  Host                     Path  Backends</span><br><span class="line">  ----                     ----  --------</span><br><span class="line">  prometheus.example.com  </span><br><span class="line">                           /   prometheus-service:8080 ()</span><br><span class="line">Annotations:               field.cattle.io/publicEndpoints:</span><br><span class="line">                             [&#123;&quot;addresses&quot;:[&quot;172.31.23.72&quot;,&quot;172.31.27.193&quot;],&quot;port&quot;:80,&quot;protocol&quot;:&quot;HTTP&quot;,&quot;serviceName&quot;:&quot;prometheus:prometheus-service&quot;,&quot;ingressName&quot;:&quot;pr...</span><br><span class="line">Events:                    &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看 <code>Service</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -n prometheus -o wide</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR</span><br><span class="line">prometheus-service   ClusterIP   10.99.75.232   &lt;none&gt;        8090/TCP   19h   app=prometheus-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe service -n prometheus prometheus-service</span></span><br><span class="line">Name:              prometheus-service</span><br><span class="line">Namespace:         prometheus</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=prometheus-server</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Family Policy:  SingleStack</span><br><span class="line">IP Families:       IPv4</span><br><span class="line">IP:                10.99.75.232</span><br><span class="line">IPs:               10.99.75.232</span><br><span class="line">Port:              prometheus-port  8090/TCP</span><br><span class="line">TargetPort:        9090/TCP</span><br><span class="line">Endpoints:         10.244.3.95:9090</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>从以上信息可以看到，服务的端口为 <code>Port: prometheus-port  8090/TCP</code>，而 Ingress 中配置的服务端口为 <code>8080</code> ，修改 Ingress 配置，将服务端口修改正确。修改后访问正常。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1456389">相关参考</a></p>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1931089">Back-off restarting failed container 怎么办</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://tonybai.com/2017/10/16/out-of-node-resource-handling-in-kubernetes-cluster/">ephemeral-storage 问题</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="../tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i> Kubernetes</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="../202209241108/" rel="prev" title="kubernetes 对象的 yaml 描述语法说明">
                  <i class="fa fa-chevron-left"></i> kubernetes 对象的 yaml 描述语法说明
                </a>
            </div>
            <div class="post-nav-item">
                <a href="../202209301604/" rel="next" title="ingress-nginx 安装配置">
                  ingress-nginx 安装配置 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COSMOS</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="../js/comments.js"></script><script src="../js/utils.js"></script><script src="../js/motion.js"></script><script src="../js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="../js/third-party/search/local-search.js"></script>




  <script src="../js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fl9999","repo":"fl9999.github.io","client_id":"a11bf6f7860762b725b5","client_secret":"a99046105f8bddc72ec718d54dc3fd7f22070821","admin_user":"fl9999","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d41d8cd98f00b204e9800998ecf8427e"}</script>
<script src="../js/third-party/comments/gitalk.js"></script>

</body>
</html>
