<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16-next.png">
  <link rel="mask-icon" href="../images/logo.svg" color="#222">

<link rel="stylesheet" href="../css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"csms.tech","root":"/","images":"../images","scheme":"Gemini","darkmode":true,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeIn","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"../search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="../js/config.js"></script>

    <meta name="description" content="Kubernetes 官网文档 环境信息 Centos 7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.24.7 kubeadm-1.24.7 kubelet-1.24.7  kubernetes 环境安装前配置升级内核版本Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 安装配置">
<meta property="og:url" content="http://csms.tech/202209121102/index.html">
<meta property="og:site_name" content="L B T">
<meta property="og:description" content="Kubernetes 官网文档 环境信息 Centos 7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.24.7 kubeadm-1.24.7 kubelet-1.24.7  kubernetes 环境安装前配置升级内核版本Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.csms.tech/img_77.png">
<meta property="og:image" content="https://i.csms.tech/img_56.png">
<meta property="og:image" content="https://i.csms.tech/img_57.png">
<meta property="og:image" content="https://i.csms.tech/img_58.png">
<meta property="og:image" content="https://i.csms.tech/img_59.png">
<meta property="og:image" content="https://i.csms.tech/img_60.png">
<meta property="og:image" content="https://i.csms.tech/img_66.png">
<meta property="article:published_time" content="2022-09-12T03:03:09.000Z">
<meta property="article:modified_time" content="2025-03-19T05:03:00.000Z">
<meta property="article:author" content="COSMOS">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.csms.tech/img_77.png">


<link rel="canonical" href="http://csms.tech/202209121102/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://csms.tech/202209121102/","path":"202209121102/","title":"kubernetes 安装配置"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kubernetes 安装配置 | L B T</title>
  








  <noscript>
    <link rel="stylesheet" href="../css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">L B T</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记 录 过 去 的 经 验</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="../index.html" rel="section"><i class="fa fa-earth-americas fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="../categories/" rel="section"><i class="fa fa-folder-tree fa-fw"></i>总目录<span class="badge">53</span></a></li><li class="menu-item menu-item-linux"><a href="../categories/Linux" rel="section"><i class="fa fa-brands fa-linux fa-fw"></i>Linux</a></li><li class="menu-item menu-item-python"><a href="../categories/Python" rel="section"><i class="fa fa-brands fa-python fa-fw"></i>Python</a></li><li class="menu-item menu-item-docker"><a href="../categories/Docker" rel="section"><i class="fa fa-brands fa-docker fa-fw"></i>Docker</a></li><li class="menu-item menu-item-kubernetes"><a href="../categories/Kubernetes" rel="section"><i class="fa fa-dharmachakra fa-fw"></i>Kubernetes</a></li><li class="menu-item menu-item-tags"><a href="../tags/" rel="section"><i class="fa fa-tornado fa-fw"></i>标签<span class="badge">78</span></a></li><li class="menu-item menu-item-archives"><a href="../archives/" rel="section"><i class="fa fa-rectangle-list fa-fw"></i>列表<span class="badge">258</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章总目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">1.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubernetes-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%89%8D%E9%85%8D%E7%BD%AE"><span class="nav-number">2.</span> <span class="nav-text">kubernetes 环境安装前配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC"><span class="nav-number">2.1.</span> <span class="nav-text">升级内核版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E9%97%AD-SELinux"><span class="nav-number">2.2.</span> <span class="nav-text">关闭 SELinux</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%89%80%E6%9C%89%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8B%E9%97%B4%E5%85%B7%E6%9C%89%E5%AE%8C%E5%85%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5"><span class="nav-number">2.3.</span> <span class="nav-text">集群中所有计算机之间具有完全的网络连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A6%81%E6%AD%A2swap%E5%88%86%E5%8C%BA"><span class="nav-number">2.4.</span> <span class="nav-text">禁止swap分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D"><span class="nav-number">2.5.</span> <span class="nav-text">配置主机名</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA-kube-proxy-%E5%BC%80%E5%90%AF-ipvs"><span class="nav-number">2.6.</span> <span class="nav-text">为 kube-proxy 开启 ipvs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E5%8F%91-IPv4-%E5%B9%B6%E8%AE%A9-iptables-%E7%9C%8B%E5%88%B0%E6%A1%A5%E6%8E%A5%E6%B5%81%E9%87%8F"><span class="nav-number">2.7.</span> <span class="nav-text">转发 IPv4 并让 iptables 看到桥接流量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Docker-Engine"><span class="nav-number">2.8.</span> <span class="nav-text">安装 Docker Engine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-cri-dockerd"><span class="nav-number">2.9.</span> <span class="nav-text">安装 cri-dockerd</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE"><span class="nav-number">3.</span> <span class="nav-text">Kubernetes 安装配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-kubeadm%E3%80%81kubelet-%E5%92%8C-kubectl"><span class="nav-number">3.1.</span> <span class="nav-text">安装 kubeadm、kubelet 和 kubectl</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%8A%82%E7%82%B9"><span class="nav-number">3.2.</span> <span class="nav-text">初始化控制平面节点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%8D%95%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E9%9B%86%E7%BE%A4"><span class="nav-number">3.2.1.</span> <span class="nav-text">创建单控制平面集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E7%9A%84%E9%9B%86%E7%BE%A4"><span class="nav-number">3.2.2.</span> <span class="nav-text">创建高可用控制平面的集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-kube-flannel"><span class="nav-number">3.2.3.</span> <span class="nav-text">安装 kube-flannel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-dashboard"><span class="nav-number">4.</span> <span class="nav-text">安装 dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kubernetes-Dashboard-v1-32-%E9%83%A8%E7%BD%B2"><span class="nav-number">4.1.</span> <span class="nav-text">Kubernetes Dashboard v1.32 部署</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Kubernetes-Metrics-Server"><span class="nav-number">5.</span> <span class="nav-text">安装 Kubernetes Metrics Server</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF"><span class="nav-number">6.</span> <span class="nav-text">常见错误</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Found-multiple-CRI-endpoints-on-the-host"><span class="nav-number">6.1.</span> <span class="nav-text">Found multiple CRI endpoints on the host</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-flannel-%E7%8A%B6%E6%80%81%E4%B8%BA-CrashLoopBackOff"><span class="nav-number">6.2.</span> <span class="nav-text">kube-flannel 状态为 CrashLoopBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master-%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81%E4%B8%BA-NotReady"><span class="nav-number">6.3.</span> <span class="nav-text">master 节点状态为 NotReady</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B9%8B%E5%A4%96%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8-kubectl-%E6%8A%A5%E9%94%99"><span class="nav-number">6.4.</span> <span class="nav-text">集群之外的服务器使用 kubectl 报错</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.5.</span> <span class="nav-text">kubelet 启动失败</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E6%9C%AA%E5%90%8C%E6%AD%A5%E5%AF%BC%E8%87%B4%E9%9B%86%E7%BE%A4%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.6.</span> <span class="nav-text">集群证书未同步导致集群添加节点失败</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E5%90%8D%E7%A7%B0%E4%B8%8D%E5%9C%A8-kube-apiserver-%E8%AF%81%E4%B9%A6%E4%B8%AD%E5%AF%BC%E8%87%B4%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.7.</span> <span class="nav-text">节点名称不在 kube-apiserver 证书中导致添加节点失败</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-%E4%BD%BF%E7%94%A8%E4%BA%86-http-%E5%8D%8F%E8%AE%AE%E5%AF%BC%E8%87%B4%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.8.</span> <span class="nav-text">etcd 使用了 http 协议导致添加节点失败</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-proxy-%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.9.</span> <span class="nav-text">kube-proxy 失败</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd-%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5-etcd-%E9%9B%86%E7%BE%A4%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.10.</span> <span class="nav-text">etcd 节点加入 etcd 集群失败</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">7.</span> <span class="nav-text">其他常用配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">新增节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%86-CRI-%E7%94%B1-containerd-%E5%8F%98%E6%9B%B4%E4%B8%BA-Docker"><span class="nav-number">7.2.</span> <span class="nav-text">将 CRI 由 containerd 变更为 Docker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9-kubelet-%E4%BD%BF%E7%94%A8%E7%9A%84-CRI-%E4%B8%BA-containerd"><span class="nav-number">7.3.</span> <span class="nav-text">修改 kubelet 使用的 CRI 为 containerd</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9-Service-%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%9A%84-nodePort-%E7%AB%AF%E5%8F%A3%E8%8C%83%E5%9B%B4"><span class="nav-number">7.4.</span> <span class="nav-text">修改 Service 可使用的 nodePort 端口范围</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%80%E5%90%AF-corndns-%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95"><span class="nav-number">7.5.</span> <span class="nav-text">开启 corndns 日志记录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7-cri-dockerd"><span class="nav-number">7.6.</span> <span class="nav-text">升级 cri-dockerd</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0-Harbor-%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E7%9A%84%E8%AE%A4%E8%AF%81%E4%BF%A1%E6%81%AF"><span class="nav-number">7.7.</span> <span class="nav-text">添加 Harbor 私有镜像仓库的认证信息</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Pod-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E7%9A%84%E8%AE%A4%E8%AF%81%E4%BF%A1%E6%81%AF"><span class="nav-number">7.7.1.</span> <span class="nav-text">配置 Pod 拉取镜像的认证信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E6%B7%BB%E5%8A%A0-hosts"><span class="nav-number">7.8.</span> <span class="nav-text">Pod 添加 hosts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Pod-%E4%B8%AD%E7%9A%84%E6%97%B6%E5%8C%BA%E5%92%8C%E6%97%B6%E9%97%B4"><span class="nav-number">7.9.</span> <span class="nav-text">配置 Pod 中的时区和时间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E8%8A%82%E7%82%B9%E5%85%81%E8%AE%B8%E5%90%AF%E5%8A%A8%E7%9A%84%E6%9C%80%E5%A4%A7-Pod-%E6%95%B0"><span class="nav-number">7.10.</span> <span class="nav-text">配置节点允许启动的最大 Pod 数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4"><span class="nav-number">7.11.</span> <span class="nav-text">重置集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2-kube-state-metrics-%E7%BB%84%E4%BB%B6"><span class="nav-number">7.12.</span> <span class="nav-text">部署 kube-state-metrics 组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%90%8E%E7%9A%84%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4"><span class="nav-number">7.13.</span> <span class="nav-text">集群证书过期后的处理步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%85%AC%E7%BD%91%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">7.14.</span> <span class="nav-text">基于公网的 Kubernetes 集群部署注意事项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#etcd-%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E5%85%AC%E7%BD%91-IP-%E9%80%9A%E4%BF%A1"><span class="nav-number">7.14.1.</span> <span class="nav-text">etcd 集群使用公网 IP 通信</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flanneld-Tunnel-IP-%E9%85%8D%E7%BD%AE"><span class="nav-number">7.15.</span> <span class="nav-text">flanneld Tunnel IP 配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E9%85%8D%E7%BD%AE"><span class="nav-number">7.16.</span> <span class="nav-text">kubelet 配置</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">8.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%9A%E6%B3%A8"><span class="nav-number">9.</span> <span class="nav-text">脚注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">COSMOS</p>
  <div class="site-description" itemprop="description">得 能 莫 忘</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="../archives/">
          <span class="site-state-item-count">258</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="../categories/">
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="../tags/">
        <span class="site-state-item-count">78</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://csms.tech/202209121102/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="../images/avatar.gif">
      <meta itemprop="name" content="COSMOS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L B T">
      <meta itemprop="description" content="得 能 莫 忘">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kubernetes 安装配置 | L B T">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kubernetes 安装配置
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-12 11:03:09" itemprop="dateCreated datePublished" datetime="2022-09-12T11:03:09+08:00">2022-09-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-19 13:03:00" itemprop="dateModified" datetime="2025-03-19T13:03:00+08:00">2025-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">上层目录</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="../categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/">Kubernetes 官网文档</a></p>
<h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><ul>
<li>Centos 7 5.4.212-1</li>
<li>Docker 20.10.18</li>
<li>containerd.io-1.6.8</li>
<li>kubectl-1.24.7</li>
<li>kubeadm-1.24.7</li>
<li>kubelet-1.24.7</li>
</ul>
<h1 id="kubernetes-环境安装前配置"><a href="#kubernetes-环境安装前配置" class="headerlink" title="kubernetes 环境安装前配置"></a>kubernetes 环境安装前配置</h1><h2 id="升级内核版本"><a href="#升级内核版本" class="headerlink" title="升级内核版本"></a>升级内核版本</h2><p>Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到新版本</p>
<a href="/202209140931/" title="Linux 升级内核">Linux 升级内核</a>

<blockquote>
<ul>
<li><p>Centos 7 默认的内核版本 3.10 使用的 cgroup 版本为 v1，Kubernetes 的部分功能必须使用 <code>cgroup v2</code> 来进行增强的资源管理和隔离 <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[About cgroup v2](https://kubernetes.io/docs/concepts/architecture/cgroups/)">[13]</span></a></sup></p>
<p>使用以下命令检查系统使用的 <code>cgroup</code> 版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stat -fc %T /sys/fs/cgroup/</span><br></pre></td></tr></table></figure>
<p>如果输出是 <code>cgroup2fs</code>， <strong>表示使用 cgroup v2</strong></p>
<p>如果输出是 <code>tmpfs</code>， <strong>表示使用 cgroup v1</strong></p>
</li>
<li><p><code>User Namespaces</code> 功能需要 Linux 6.3 以上版本，<code>tmpfs</code> 才能支持 <code>idmap</code> 挂载。并且其他功能（如 ServiceAccount 的挂载）也需要此功能的支持 <sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[User Namespaces](https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/)">[14]</span></a></sup></p>
</li>
<li><p><strong>Kubernetes v1.32 需要 Linux Kernel &gt;&#x3D; 4.19, 建议 5.8+ 以更好的支持 cgroups v2</strong></p>
</li>
<li><p><strong>Rocky Linux 8 或者 Centos 8 默认使用 <code>cgroup v1</code>，需要升级到 <code>cgroup v2</code>，执行命令 <code>grubby --update-kernel=ALL --args=&quot;systemd.unified_cgroup_hierarchy=1&quot;</code> 后重启即可升级到 <code>cgroup v2</code> 。</strong> 使用以下命令检查：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">stat</span> -<span class="built_in">fc</span> %T /sys/fs/cgroup/</span></span><br><span class="line">cgroup2fs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>若 CRI 使用 Containerd，需要 <a href="https://csms.tech/202212011355/#Containerd-常用配置">配置启用 CRI 以及配置其使用 <code>cgroup v2</code></a>。</strong></p>
</li>
</ul>
</blockquote>
<h2 id="关闭-SELinux"><a href="#关闭-SELinux" class="headerlink" title="关闭 SELinux"></a>关闭 SELinux</h2><p>kubernetes 目前未实现对 SELinux 的支持，因此必须要关闭 SELinux</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo setenforce 0</span><br><span class="line">sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=disabled/&#x27; /etc/selinux/config</span><br></pre></td></tr></table></figure>

<h2 id="集群中所有计算机之间具有完全的网络连接"><a href="#集群中所有计算机之间具有完全的网络连接" class="headerlink" title="集群中所有计算机之间具有完全的网络连接"></a>集群中所有计算机之间具有完全的网络连接</h2><p>配置集群所有节点的防火墙，确保所有集群节点之间具有完全的网络连接。</p>
<ul>
<li>放通节点之间的通信</li>
<li>确保防火墙允许 <code>FORWARD</code> 链的流量<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [4:368]</span><br><span class="line"></span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes nodes</span></span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.5.58 -j ACCEPT</span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.5.68 -j ACCEPT</span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.0.230 -j ACCEPT</span><br><span class="line"></span><br><span class="line">-A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT -m comment --comment &quot;k8s ingress http,https&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">-A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT</span><br><span class="line">-A INPUT -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure></li>
</ul>
<span id="more"></span>

<p>集群通信 ( iptables ) 矩阵说明： <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[端口和协议](https://v1-25.docs.kubernetes.io/zh-cn/docs/reference/networking/ports-and-protocols/)">[6]</span></a></sup></p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>iptables table</th>
<th>ipables chain</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used by</th>
</tr>
</thead>
<tbody><tr>
<td>UDP</td>
<td>filter</td>
<td>INPUT</td>
<td>8472</td>
<td>flannel</td>
<td>network</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>6443</td>
<td>Kubernetes API server</td>
<td>ALL node</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver,etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10250</td>
<td>kubelet API</td>
<td>Control plane, Self ,kubectl exec</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>self</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>self</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>30000-32767</td>
<td>NodePortService</td>
<td>All</td>
</tr>
</tbody></table>
<ul>
<li>不同节点之间的 <code>Pod</code> 通信需要经过 <code>flannel</code> 的 <code>8472/udp</code> </li>
<li><code>nodePort</code> 类型的 <code>service</code> ，默认可用的 <code>nodePort</code> 端口范围为 <code>30000-32767</code>，根据实际情况配置</li>
</ul>
<p>若对网络安全要求较为严格，可在 master 节点使用以下防火墙规则，本示例中 CNI 对接的网络插件为 flannel，若使用其他网络插件，则根据插件要求放通对应端口。</p>
<p>本示例中 <code>192.168.142.8 - 10</code> 为 master 节点，<code>192.168.142.11 - 12</code> 为 worker 节点</p>
<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes master</span></span><br><span class="line">-A INPUT -s 192.168.142.8 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;for kubernetes master from kubernetes master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.9 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;for kubernetes master from kubernetes master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.10 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;for kubernetes master from kubernetes master&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.11 -p tcp -m multiport --dports 6443,2379:2380,10250 -j ACCEPT -m comment --comment &quot;for kubernetes master from k8s worker node&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p tcp -m multiport --dports 6443,2379:2380,10250 -j ACCEPT -m comment --comment &quot;for kubernetes master from k8s worker node&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.8 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.11 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes worker node&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes worker node&quot;</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure>
<p>在 node 节点使用以下防火墙规则</p>
<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes node</span></span><br><span class="line">-A INPUT -s 192.168.142.8 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;for kubernetes node from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;for kubernetes node from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;for kubernetes node from kubernetes master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.8 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.11 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;for flannel from kubernetes worker nodes&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Ingress Nginx Controller</span></span><br><span class="line">-A INPUT -p tcp -m multiport --dports 80,443,8443 -j ACCEPT -m comment --comment &quot;for Ingress Nginx Controller&quot;</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure>

<h2 id="禁止swap分区"><a href="#禁止swap分区" class="headerlink" title="禁止swap分区"></a>禁止swap分区</h2><p>以下命令临时关闭 swap，要永久关闭 swap，修改配置文件 <code>/etc/fstab</code>，删除或注释其中 <code>swap</code> 相关的行。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>

<h2 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h2><p>节点之中不可以有重复的主机名、MAC 地址或 product_uuid</p>
<p>配置集群中的 3 台主机名分别为 <code>kubernetes1</code>，<code>kubernetes2</code>，<code>kubernetes3</code>，本示例中 <code>kubernetes1</code> 作为 master </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname kubernetes1</span><br></pre></td></tr></table></figure>

<p>添加主机名和 ip 解析到 <code>/etc/hosts</code> 文件</p>
<figure class="highlight shell"><figcaption><span>/etc/hosts</span></figcaption><table><tr><td class="code"><pre><span class="line">172.31.10.19 kubernetes1</span><br><span class="line">172.31.9.241 kubernetes2</span><br><span class="line">172.31.14.115 kubernetes3</span><br></pre></td></tr></table></figure>

<h2 id="为-kube-proxy-开启-ipvs"><a href="#为-kube-proxy-开启-ipvs" class="headerlink" title="为 kube-proxy 开启 ipvs"></a>为 kube-proxy 开启 ipvs</h2><p><a target="_blank" rel="noopener" href="https://blog.fleeto.us/post/iptables-or-ipvs/">kube-proxy 模式对比：iptables 还是 IPVS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1890887">kube-proxy中使用ipvs与iptables的比较</a></p>
<p>此配置为<strong>可选操作</strong>，在不启用 ipvs 模式的情况下，kube-proxy 会使用 iptables 模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules </span><br></pre></td></tr></table></figure>

<h2 id="转发-IPv4-并让-iptables-看到桥接流量"><a href="#转发-IPv4-并让-iptables-看到桥接流量" class="headerlink" title="转发 IPv4 并让 iptables 看到桥接流量"></a>转发 IPv4 并让 iptables 看到桥接流量</h2><p>以下操作需要在 kubernetes 集群中的所有节点操作  </p>
<p>通过运行 <code>lsmod | grep br_netfilter</code> 来验证 <code>br_netfilter</code> 模块是否已加载。Kubernetes 通过 <code>bridge-netfilter</code> 配置使 iptables 规则可以应用在 Linux Bridge 上，对 Linux 内核进行宿主机和容器之间的数据包的地址转换是必须的，否则 Pod 进行外部服务网络请求时会出现目标主机不可达或者连接拒绝等错误（host unreachable 或者 connection refused）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">lsmod | grep br_netfilter</span></span><br><span class="line">br_netfilter           22256  0 </span><br><span class="line">bridge                151336  1 br_netfilter</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>若要显式加载此模块，请运行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo modprobe overlay</span><br><span class="line">sudo modprobe br_netfilter</span><br></pre></td></tr></table></figure>


<p>为了让 Linux 节点的 iptables 能够正确查看桥接流量，请确认 <code>sysctl</code> 配置中的 <code>net.bridge.bridge-nf-call-iptables</code> 设置为 1</p>
<p>为配置永久生效，可以添加以下配置，<code>/etc/modules-load.d/k8s.conf</code> 中追加要加载的模块</p>
<figure class="highlight shell"><figcaption><span>/etc/modules-load.d/k8s.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">overlay</span><br><span class="line">br_netfilter</span><br></pre></td></tr></table></figure>

<p><code>/etc/sysctl.d/k8s.conf</code> 中追加内核参数</p>
<figure class="highlight shell"><figcaption><span>/etc/sysctl.d/k8s.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br></pre></td></tr></table></figure>

<p>执行以下命令重新载入 sysctl 参数而无需重启系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sysctl --system</span><br></pre></td></tr></table></figure>

<!-- more -->

<h2 id="安装-Docker-Engine"><a href="#安装-Docker-Engine" class="headerlink" title="安装 Docker Engine"></a>安装 Docker Engine</h2><p>以下操作需要在 kubernetes 集群中的所有节点操作<br>参考以下链接，在每个节点上安装 Docker Engine<br><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/centos/">Centos 安装 Docker Engine 官网参考文档</a></p>
<a href="/202208041317/" title="docker 安装及常用命令介绍">docker 安装及常用命令介绍</a>

<h2 id="安装-cri-dockerd"><a href="#安装-cri-dockerd" class="headerlink" title="安装 cri-dockerd"></a>安装 cri-dockerd</h2><p>Docker Engine 没有实现 CRI，因此 Kubernetes 无法直接使用 Docker Engine，需要先安装 cri-dockerd，以让 Kubernetes 可以通过 Kubernetes 的 CRI 操作 Docker。</p>
<p>以下操作需要在 kubernetes 集群中的所有节点操作 </p>
<p>按照源代码仓库中的说明安装 <a target="_blank" rel="noopener" href="https://github.com/Mirantis/cri-dockerd">cri-dockerd</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/Mirantis/cri-dockerd.git</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Run these commands as root</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##Install GO###</span></span></span><br><span class="line">wget https://storage.googleapis.com/golang/getgo/installer_linux</span><br><span class="line">chmod +x ./installer_linux</span><br><span class="line">./installer_linux</span><br><span class="line">source ~/.bash_profile</span><br><span class="line"></span><br><span class="line">cd cri-dockerd</span><br><span class="line">mkdir bin</span><br><span class="line">go build -o bin/cri-dockerd</span><br><span class="line">mkdir -p /usr/local/bin</span><br><span class="line">install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd</span><br><span class="line">cp -a packaging/systemd/* /etc/systemd/system</span><br><span class="line">sed -i -e &#x27;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 使用 iptables 替换 firewalld</span></span></span><br><span class="line">sed -i -e &#x27;s,firewalld.service,iptables.service,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable --now cri-docker.service</span><br><span class="line">systemctl enable --now cri-docker.socket</span><br></pre></td></tr></table></figure>

<p>对于 <code>cri-dockerd</code>，默认情况下，CRI 套接字是 <code>/run/cri-dockerd.sock</code></p>
<h1 id="Kubernetes-安装配置"><a href="#Kubernetes-安装配置" class="headerlink" title="Kubernetes 安装配置"></a>Kubernetes 安装配置</h1><h2 id="安装-kubeadm、kubelet-和-kubectl"><a href="#安装-kubeadm、kubelet-和-kubectl" class="headerlink" title="安装 kubeadm、kubelet 和 kubectl"></a>安装 kubeadm、kubelet 和 kubectl</h2><p>需要在每台机器上安装以下的软件包：</p>
<ul>
<li><p><code>kubeadm</code> ： 用来初始化集群的指令。</p>
</li>
<li><p><code>kubelet</code> ： 在集群中的每个节点上用来启动 Pod 和容器等。</p>
</li>
<li><p><code>kubectl</code> ： 用来与集群通信的命令行工具。</p>
</li>
</ul>
<p>添加 <code>yum</code> 源</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">exclude=kubelet kubeadm kubectl</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>安装软件包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y kubelet-1.24.7 kubeadm-1.24.7 kubectl-1.24.7 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<p>启动服务并配置开机启动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl enable --now kubelet</span><br></pre></td></tr></table></figure>
<p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 <code>kubeadm</code> 指令的死循环。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl status kubelet</span></span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /usr/lib/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br><span class="line">   Active: activating (auto-restart) (Result: exit-code) since Mon 2022-09-12 14:35:58 CST; 7s ago</span><br><span class="line">     Docs: https://kubernetes.io/docs/</span><br><span class="line">  Process: 2056 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 2056 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Sep 12 14:35:58 ip-172-31-14-115.us-west-1.compute.internal systemd[1]: Unit kubelet.service entered failed state.</span><br><span class="line">Sep 12 14:35:58 ip-172-31-14-115.us-west-1.compute.internal systemd[1]: kubelet.service failed.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="初始化控制平面节点"><a href="#初始化控制平面节点" class="headerlink" title="初始化控制平面节点"></a>初始化控制平面节点</h2><h3 id="创建单控制平面集群"><a href="#创建单控制平面集群" class="headerlink" title="创建单控制平面集群"></a>创建单控制平面集群</h3><p>控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server （命令行工具 kubectl 与之通信）。</p>
<ol>
<li><p>初始化控制平面节点</p>
<p> 要初始化控制平面节点，请在 master 节点上（<code>kubernetes1</code>）运行以下命令，<a href="https://csms.tech/202209131536/#创建集群">命令参数说明</a>：</p>
 <figure class="highlight shell"><figcaption><span>kubernetes1</span></figcaption><table><tr><td class="code"><pre><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
<ul>
<li><code>--pod-network-cidr=10.244.0.0/16</code> 指定 pod 使用的网络段，后面配置网络（CNI）时配置的网段要和此处一致</li>
<li><code>--cri-socket=unix:///var/run/cri-dockerd.sock</code> 指定使用的 CRI 为 Docker</li>
</ul>
<blockquote>
<p>使用 <code>kubeadm init --config=./kubeadm-config.yml</code> 的情况下，对应配置文件中内容示例：</p>
<figure class="highlight shell"><figcaption><span>kubeadm-config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: stable</span><br><span class="line">controlPlaneEndpoint: &quot;192.168.254.106:6443&quot;</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: &quot;10.244.0.0/16&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &quot;192.168.254.106&quot;</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: /var/run/containerd/containerd.sock</span><br></pre></td></tr></table></figure>
<p> 输出结果如下：<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.25.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubernetes1] and IPs [10.96.0.1 172.31.10.19]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [localhost kubernetes1] and IPs [172.31.10.19 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [localhost kubernetes1] and IPs [172.31.10.19 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 17.003297 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node kubernetes1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]</span><br><span class="line">[mark-control-plane] Marking the node kubernetes1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: 8ca35s.butdpihinkdczvqb</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, if you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">  beadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6e4fef99fa2407241e1a0e8ea652149 </span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
</blockquote>
<p> 根据 <code>kubeadm init</code> 输出提示，配置 <code>kubectl</code> 需要的环境变量，root 用户执行以下命令<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure><br> 为永久生效，可将其添加到 <code>~/.bash_profile</code></p>
<p> 此时，执行以下命令查看集群节点信息<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-565d847f94-dc8tl                  0/1     Pending   0          5m42s</span><br><span class="line">kube-system   coredns-565d847f94-zqctg                  0/1     Pending   0          5m42s</span><br><span class="line">kube-system   etcd-kubernetes1                          1/1     Running   0          5m54s</span><br><span class="line">kube-system   kube-apiserver-kubernetes1                1/1     Running   0          5m53s</span><br><span class="line">kube-system   kube-controller-manager-kubernetes1       1/1     Running   0          5m54s</span><br><span class="line">kube-system   kube-proxy-6kwdx                          1/1     Running   0          5m43s</span><br><span class="line">kube-system   kube-scheduler-kubernetes1                1/1     Running   0          5m54s</span><br><span class="line"></span><br></pre></td></tr></table></figure><br> 其中，<code>coredns</code> 的 pod 处于 <code>Pending</code> 状态，是因为网络还没配置。</p>
<p> 因为 CRI 使用 docker，此时使用以下命令，可以查看到启动的所有容器<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a</span></span><br><span class="line">CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS     NAMES</span><br><span class="line">741a6d32b2cd   58a9a0c6d96f           &quot;/usr/local/bin/kube…&quot;   5 minutes ago   Up 5 minutes             k8s_kube-proxy_kube-proxy-6kwdx_kube-system_93101b10-7ee5-437c-a234-3e31edc7cfa9_0</span><br><span class="line">31509b3f06cc   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 5 minutes ago   Up 5 minutes             k8s_POD_kube-proxy-6kwdx_kube-system_93101b10-7ee5-437c-a234-3e31edc7cfa9_0</span><br><span class="line">fb3ec15950b6   bef2cf311509           &quot;kube-scheduler --au…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-scheduler_kube-scheduler-kubernetes1_kube-system_c455960b65afeadd009ff9ba9e7ab7b0_0</span><br><span class="line">333188677c01   4d2edfd10d3e           &quot;kube-apiserver --ad…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-apiserver_kube-apiserver-kubernetes1_kube-system_11596873d958a699a1b923df2333eaad_0</span><br><span class="line">4bdbf8689bbb   1a54c86c03a6           &quot;kube-controller-man…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-controller-manager_kube-controller-manager-kubernetes1_kube-system_23ce2f60ac97b06bde25c1662e88e409_0</span><br><span class="line">a399d3484c17   a8a176a5d5d6           &quot;etcd --advertise-cl…&quot;   6 minutes ago   Up 6 minutes             k8s_etcd_etcd-kubernetes1_kube-system_84da44e552601c02573afe1dc1e3b0a2_0</span><br><span class="line">28aae0e41a7d   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-apiserver-kubernetes1_kube-system_11596873d958a699a1b923df2333eaad_0</span><br><span class="line">3f4f378ed731   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-scheduler-kubernetes1_kube-system_c455960b65afeadd009ff9ba9e7ab7b0_0</span><br><span class="line">eaa6d312a174   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_etcd-kubernetes1_kube-system_84da44e552601c02573afe1dc1e3b0a2_0</span><br><span class="line">707e84291ac2   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-controller-manager-kubernetes1_kube-system_23ce2f60ac97b06bde25c1662e88e409_0</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
</li>
<li><p><a href="#%E5%AE%89%E8%A3%85-kube-flannel">安装 kube-flannel</a></p>
<p> 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。</p>
</li>
<li><p>将节点加入集群</p>
<p> 在 work 节点上执行以下命令加入集群</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6 \ </span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
<p> 加入集群成功后，在 master 上查看所有节点</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES           AGE   VERSION</span><br><span class="line">kubernetes1   Ready      control-plane   36m   v1.25.0</span><br><span class="line">kubernetes2   NotReady   &lt;none&gt;          21s   v1.25.0</span><br><span class="line">kubernetes3   NotReady   &lt;none&gt;          18s   v1.25.0</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="创建高可用控制平面的集群"><a href="#创建高可用控制平面的集群" class="headerlink" title="创建高可用控制平面的集群"></a>创建高可用控制平面的集群</h3><p>创建 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology"><code>堆叠（Stacked）etcd 拓扑</code></a> 的高可用控制平面集群</p>
<p><code>堆叠（Stacked）etcd 拓扑</code> 主要有以下特点：</p>
<ul>
<li><code>etcd</code> 分布式数据存储集群堆叠在 <code>kubeadm</code> 管理的控制平面节点上，作为控制平面的一个组件运行。</li>
<li>每个控制平面节点运行 <code>etcd</code>、 <code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。 <code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</li>
<li>每个控制平面节点创建一个本地 <code>etcd</code> 成员（member），这个 <code>etcd</code> 成员只与该节点的 <code>kube-apiserver</code> 通信。 这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</li>
<li>堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 <code>etcd</code> 成员和控制平面实例都将丢失， 并且冗余会受到影响。你可以通过添加更多控制平面节点来降低此风险。</li>
</ul>
<p><code>堆叠（Stacked）etcd 拓扑</code><br><img src="https://i.csms.tech/img_77.png"></p>
<p>为 kube-apiserver 创建负载均衡器，该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。 API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 :6443） 上进行的一个 TCP 检查。 <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[为 kube-apiserver 创建负载均衡器](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/#%E4%B8%BA-kube-apiserver-%E5%88%9B%E5%BB%BA%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8)">[2]</span></a></sup></p>
<p>此处假设 kube-apiserver 的负载均衡地址为 <code>kube-apiserver.my.com:6443</code>。</p>
<ol>
<li><p>初始化控制平面：</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock \</span><br><span class="line">             --control-plane-endpoint &quot;kube-apiserver.my.com:6443&quot; \</span><br><span class="line">             --upload-certs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。<em><strong>如果不使用此选项，需要手动拷贝证书到其他节点</strong></em> <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Manual certificate distribution](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#manual-certs)">[12]</span></a></sup></p>
<p> 根据 <code>kubeadm init</code> 输出提示，配置 <code>kubectl</code> 需要的环境变量，root 用户执行以下命令</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure>
<p> 为永久生效，可将其添加到 <code>~/.bash_profile</code></p>
</li>
<li><p><a href="#%E5%AE%89%E8%A3%85-kube-flannel">安装 kube-flannel</a></p>
<p> 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。</p>
<p> 输入以下内容，并查看控制平面组件的 Pod 启动：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system -w</span><br></pre></td></tr></table></figure></li>
<li><p>其余控制平面节点上的操作</p>
<p> 执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给你的 <code>join</code> 命令。 在 CRI 是 <code>cri-dockerd</code> 的场景下，要添加 <code>--cri-socket=unix:///var/run/cri-dockerd.sock</code>。它看起来应该像这样：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv \</span><br><span class="line">             --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 \</span><br><span class="line">             --control-plane \</span><br><span class="line">             --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 \</span><br><span class="line">             --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
</li>
<li><p>工作节点上的操作</p>
<p> 在工作节点上执行以下命令，添加工作节点到集群中。在 CRI 是 <code>cri-dockerd</code> 的场景下，要添加 <code>--cri-socket=unix:///var/run/cri-dockerd.sock</code>。它看起来应该像这样：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm join kube-apiserver.uat.148962587001:6443 \</span><br><span class="line">        --token 0nf24o.fb98ll5qkhpcxd70 \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:a5d589a3476777df757e38334b035a93811d94e75131e3d9cc1d7efad22fc793 \</span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="安装-kube-flannel"><a href="#安装-kube-flannel" class="headerlink" title="安装 kube-flannel"></a>安装 kube-flannel</h3><p>Kubernetes 安装时已经安装了网络相关驱动，位于 <code>/opt/cni/bin/flannel</code>，此时只需要根据相关配置文件生成 <code>kube-flannel</code> 的 pod 即可</p>
<p>请在 master 节点上（<code>kubernetes1</code>）运行以下命令创建 <code>kube-flannel</code> 相关 POD</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml</span></span><br><span class="line"></span><br><span class="line">namespace/kube-flannel created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.apps/kube-flannel-ds created</span><br></pre></td></tr></table></figure>
<p>使用默认的 <code>kube-flannel.yml</code>，默认的 Network 为 <code>10.244.0.0/16</code>，要变更默认网段，更改 <code>kube-flannel.yml</code> 中的以下内容即可：</p>
<figure class="highlight shell"><figcaption><span>kube-flannel.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">net-conf.json: |</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span><br><span class="line">    &quot;Backend&quot;: &#123;</span><br><span class="line">      &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此处的网段配置需要和 <a href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%8A%82%E7%82%B9">初始化集群时定义的 pod 网段</a> 保持一致</p>
<p>创建完成 <code>kube-flannel</code> 后，再次查看集群中的 pod 信息，可以看到 <code>coredns</code> 已经处于运行状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-rg969                     1/1     Running   0          20s</span><br><span class="line">kube-system    coredns-565d847f94-dc8tl                  0/1     Running   0          22m</span><br><span class="line">kube-system    coredns-565d847f94-zqctg                  0/1     Running   0          22m</span><br><span class="line">kube-system    etcd-kubernetes1                          1/1     Running   0          22m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1                1/1     Running   0          22m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1       1/1     Running   0          22m</span><br><span class="line">kube-system    kube-proxy-6kwdx                          1/1     Running   0          22m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1                1/1     Running   0          22m</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="安装-dashboard"><a href="#安装-dashboard" class="headerlink" title="安装 dashboard"></a>安装 dashboard</h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard">kubernetes-dashboard 项目地址</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure>

<p>修改 <code>recommended.yaml</code> 以下内容</p>
<figure class="highlight shell"><figcaption><span>recommended.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort     </span><br><span class="line">  ports:</span><br><span class="line">    - port: 443</span><br><span class="line">      targetPort: 8443</span><br><span class="line">      nodePort: 30443</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上修改主要是新加以下 2 行，配置对外的端口，可用范围为 30000-32767：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type: NodePort </span><br><span class="line">    nodePort: 30443</span><br></pre></td></tr></table></figure>
<p>以下 2 处新增配置 <code>nodeName: kubernetes1</code>，其中 <code>kubernetes1</code> 为 master 节点名称，可以通过 <code>kubectl get nodes</code> 查看</p>
<figure class="highlight shell"><figcaption><span>recommended.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  securityContext:</span><br><span class="line">    seccompProfile:</span><br><span class="line">      type: RuntimeDefault</span><br><span class="line">  nodeName: kubernetes1</span><br><span class="line">  containers:</span><br><span class="line">    - name: kubernetes-dashboard</span><br><span class="line">      image: kubernetesui/dashboard:v2.6.1</span><br><span class="line">      imagePullPolicy: Always</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">          protocol: TCP</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以上 2 处修改主要是配置 <code>kubernetes-dashboard</code> 运行在 master 节点上，否则可能运行在其他节点上，会因为网络问题导致 <code>kubernetes-dashboard</code> 无法正常启动，查看日志会报以下错误：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs -n kubernetes-dashboard kubernetes-dashboard-66c887f759-5rbbb</span></span><br><span class="line">2022/09/14 06:59:17 Starting overwatch</span><br><span class="line">2022/09/14 06:59:17 Using namespace: kubernetes-dashboard</span><br><span class="line">2022/09/14 06:59:17 Using in-cluster config to connect to apiserver</span><br><span class="line">2022/09/14 06:59:17 Using secret token for csrf signing</span><br><span class="line">2022/09/14 06:59:17 Initializing csrf token from kubernetes-dashboard-csrf secret</span><br><span class="line">panic: Get &quot;https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf&quot;: dial tcp 10.96.0.1:443: connect: no route to host</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键日志 ： <code>panic: Get &quot;https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf&quot;: dial tcp 10.96.0.1:443: connect: no route to host </code></p>
<p>以上报错也有可能是因为防火墙未放通各个 <code>service</code> 的 <code>CLUSTER-IP</code> 网段导致，可以在防火墙中放通相应网段</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -A</span></span><br><span class="line">NAMESPACE              NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">default                kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                  3h9m</span><br><span class="line">kube-system            kube-dns                    ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   3h9m</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.107.39.231   &lt;none&gt;        8000/TCP                 88m</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard        NodePort    10.101.165.61   &lt;none&gt;        443:30443/TCP            88m</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>iptables</code> 中放通对应网段</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -I INPUT 6 -s 10.0.0.0/8 -j ACCEPT</span><br></pre></td></tr></table></figure>

<p>使用修改后的配置文件 <code>recommended.yaml</code> 启动 <code>kubernetes-dashboard</code> pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f recommended.yaml</span></span><br><span class="line"></span><br><span class="line">namespace/kubernetes-dashboard created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">secret/kubernetes-dashboard-csrf created</span><br><span class="line">secret/kubernetes-dashboard-key-holder created</span><br><span class="line">configmap/kubernetes-dashboard-settings created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/dashboard-metrics-scraper created</span><br><span class="line">deployment.apps/dashboard-metrics-scraper created</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看 pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE</span><br><span class="line">kube-flannel           kube-flannel-ds-bdms5                        1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-flannel           kube-flannel-ds-kq7gz                        1/1     Running   1             19h</span><br><span class="line">kube-flannel           kube-flannel-ds-rg969                        1/1     Running   2 (17h ago)   19h</span><br><span class="line">kube-system            coredns-565d847f94-dc8tl                     1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            coredns-565d847f94-zqctg                     1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            etcd-kubernetes1                             1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-apiserver-kubernetes1                   1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-controller-manager-kubernetes1          1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-proxy-6kwdx                             1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-proxy-7lk7c                             1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-system            kube-proxy-rjr76                             1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-system            kube-scheduler-kubernetes1                   1/1     Running   3 (17h ago)   20h</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-746f6b45bf-ndvbr   1/1     Running   0             40s</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-64f444d4f9-2mjdb        1/1     Running   0             40s</span><br></pre></td></tr></table></figure>

<p><code>kubernetes-dashboard</code> 运行正常后，在防火墙放通 <code>kubernetes-dashboard</code> 对外的端口（30443）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -I INPUT 6 -p tcp --dport 30443  -j ACCEPT</span><br></pre></td></tr></table></figure>

<p>浏览器通过访问 master 节点的公网 ip 地址和端口（<a href="https://ip:30443）">https://ip:30443）</a> ，可以打开 <code>kubernetes-dashboard</code> web 界面<br><img src="https://i.csms.tech/img_56.png"></p>
<p>此时要验证 Token。需要首先创建管理员用户，创建以下配置文件，文件命名为 <code>kubernetes-dashboard-adminuser.yaml</code>，<a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">参考文档</a></p>
<figure class="highlight shell"><figcaption><span>kubernetes-dashboard-adminuser.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以上配置创建了一个 <code>admin</code> 用户（用户名字随便起），赋予 <code>ClusterRoleBinding</code> 角色权限，关联到 <code>clusert-admin</code>（名称是固定的不能修改）。</p>
<p>根据此配置创建账号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f kubernetes-dashboard-adminuser.yaml</span></span><br><span class="line">serviceaccount/admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/admin created</span><br></pre></td></tr></table></figure>

<p>获取 <code>Bearer Token</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl -n kubernetes-dashboard create token admin</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6InUxMDNNUmZmU3BFenZYTEZjNjk2LUR0S1Q..</span><br></pre></td></tr></table></figure>
<p>将生成的 <code>Token</code> 输入浏览器进行验证，验证成功后可以登入 Dashboard<br><img src="https://i.csms.tech/img_57.png"></p>
<p>默认的 token 有效期很短，要修改 token 的有限时间，可以在登陆 Dashboard 后，编辑 <code>kubernetes-dashboard</code> 的 <code>Deployment</code><br><img src="https://i.csms.tech/img_58.png"></p>
<p>在 <code>spec:template:spec:containers:args</code> 下新增 <code>- &#39;--token-ttl=2592000&#39;</code><br><img src="https://i.csms.tech/img_59.png"></p>
<h2 id="Kubernetes-Dashboard-v1-32-部署"><a href="#Kubernetes-Dashboard-v1-32-部署" class="headerlink" title="Kubernetes Dashboard v1.32 部署"></a>Kubernetes Dashboard v1.32 部署</h2><p>当前版本的 Kubernetes Dashboard 只支持使用 Helm 方式部署。参考以下命令安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Add kubernetes-dashboard repository</span></span><br><span class="line">helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Deploy a Helm Release named <span class="string">&quot;kubernetes-dashboard&quot;</span> using the kubernetes-dashboard chart</span></span><br><span class="line">helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard</span><br></pre></td></tr></table></figure>

<p>为了从外部登陆 Kubernetes-Dashboard，参考以下内容为其部署 Ingress</p>
<figure class="highlight shell"><figcaption><span>kubernetes-dashboard-nginx-ingress.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard-nginx-ingress</span><br><span class="line">  annotations:</span><br><span class="line">    # nginx.ingress.kubernetes.io/rewrite-target: /$1</span><br><span class="line">    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: nginx</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - kubernetes.dashboard.com</span><br><span class="line">    secretName: kubernetes.dashboard.com.tls.secret</span><br><span class="line">  rules:</span><br><span class="line">    - host: kubernetes.dashboard.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - path: /</span><br><span class="line">            pathType: Prefix</span><br><span class="line">            backend:</span><br><span class="line">              service:</span><br><span class="line">                name: kubernetes-dashboard-kong-proxy</span><br><span class="line">                port:</span><br><span class="line">                  number: 443</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>添加 Annotations： <code>nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</code> 让 Ingress Nginx Controller 使用 HTTPS 协议请求 <code>kubernetes-dashboard-kong-proxy</code> ，否则默认以 HTTP 方式请求会失败，因为 Dashboard 未监听 HTTP 端口，会报错： <code>400 The plain HTTP request was sent to HTTPS port</code></strong></p>
</blockquote>
<p>从 WEB 访问需要 Token 认证，可以参考以下步骤创建 Token</p>
<ol>
<li><p><strong>查看已有的 ServiceAccount</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl -n kubernetes-dashboard get serviceaccount</span></span><br><span class="line">NAME                                   SECRETS   AGE</span><br><span class="line">default                                0         5d23h</span><br><span class="line">kubernetes-dashboard-api               0         5d23h</span><br><span class="line">kubernetes-dashboard-kong              0         5d23h</span><br><span class="line">kubernetes-dashboard-metrics-scraper   0         5d23h</span><br><span class="line">kubernetes-dashboard-web               0         5d23h</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>创建名为 <code>admin</code> 的 ServiceAccount</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl -n kubernetes-dashboard create serviceaccount admin</span></span><br><span class="line">serviceaccount/admin created</span><br><span class="line">   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl -n kubernetes-dashboard get serviceaccount</span></span><br><span class="line">NAME                                   SECRETS   AGE</span><br><span class="line">admin                                  0         4s</span><br><span class="line">default                                0         5d23h</span><br><span class="line">kubernetes-dashboard-api               0         5d23h</span><br><span class="line">kubernetes-dashboard-kong              0         5d23h</span><br><span class="line">kubernetes-dashboard-metrics-scraper   0         5d23h</span><br><span class="line">kubernetes-dashboard-web               0         5d23h</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>给 ServiceAccount 赋予权限</strong> 。默认情况下，ServiceAccount 可能没有足够权限访问 Kubernetes 资源。如果需要管理员权限，可以绑定 <code>cluster-admin</code> 角色：</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl create clusterrolebinding kubernetes-dashboard-admin-binding --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin-binding created</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>根据 ServiceAccount 生成 Token</strong> 。默认 Token 有效期为 1H，可以使用参数 <code>--duration=8760h</code> 创建有效期更长的 Token</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl -n kubernetes-dashboard create token admin --duration=8760h</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IjhFRkdaazdfN3MtcElJSTVPcEFPS2VsdHBZdmdIcXh0QmlwLUo1T...</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="安装-Kubernetes-Metrics-Server"><a href="#安装-Kubernetes-Metrics-Server" class="headerlink" title="安装 Kubernetes Metrics Server"></a>安装 Kubernetes Metrics Server</h1><p>安装 <code>Kubernetes Metrics Server</code> 可以支持使用 <code>kubectl top</code> 命令来查看集群使用的资源情况。 <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Kubernetes Metrics Server](https://github.com/kubernetes-sigs/metrics-server)">[5]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</span><br><span class="line">mv components.yaml kubernetes-mitrics-server.yaml</span><br><span class="line">kubectl apply -f kubernetes-mitrics-server.yaml</span><br></pre></td></tr></table></figure>
<p>部署后为了解决证书问题，可以临时配置不使用安全证书进行通信，修改 <code>metrics-server</code> 的 <code>Deployment</code>，在 <code>metrics-server</code> 启动时添加参数 <code>--kubelet-insecure-tls</code></p>
<figure class="highlight shell"><figcaption><span>kubernetes-mitrics-server.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - args:</span><br><span class="line">    - --cert-dir=/tmp</span><br><span class="line">    - --secure-port=4443</span><br><span class="line">    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><br><span class="line">    - --kubelet-use-node-status-port</span><br><span class="line">    - --metric-resolution=15s</span><br><span class="line">    - --kubelet-insecure-tls</span><br></pre></td></tr></table></figure>

<h1 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h1><h2 id="Found-multiple-CRI-endpoints-on-the-host"><a href="#Found-multiple-CRI-endpoints-on-the-host" class="headerlink" title="Found multiple CRI endpoints on the host"></a>Found multiple CRI endpoints on the host</h2><p><strong>错误场景</strong> ： 执行以下命令将节点加入集群时报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm <span class="built_in">join</span> 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042</span></span><br><span class="line"></span><br><span class="line">Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the &#x27;criSocket&#x27; field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>报错原因</strong> ： 在没有明确指定 Kubernetes 要使用的 CRI 情况下，会自动扫描主机上面安装的 CRI，如果出现多个可用的 CRI，会报错并提示确定使用哪个 CRI。</p>
<p><strong>解决方法</strong> ： 使用如下命令，指定要使用的 CRI</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6 \ </span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>

<h2 id="kube-flannel-状态为-CrashLoopBackOff"><a href="#kube-flannel-状态为-CrashLoopBackOff" class="headerlink" title="kube-flannel 状态为 CrashLoopBackOff"></a>kube-flannel 状态为 CrashLoopBackOff</h2><p><strong>错误场景</strong> ：<br><code>kube-flannel</code> 一直重启，状态为 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods --all-namespaces</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS              RESTARTS         AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-7q2hp                     0/1     CrashLoopBackOff    40 (3m39s ago)   3h4m</span><br><span class="line">kube-flannel   kube-flannel-ds-k8wd6                     0/1     CrashLoopBackOff    35 (53s ago)     76m</span><br><span class="line">kube-flannel   kube-flannel-ds-x6ck2                     0/1     CrashLoopBackOff    18 (106s ago)    69m</span><br><span class="line">kube-system    coredns-565d847f94-b4sgn                  0/1     ContainerCreating   0                3h40m</span><br><span class="line">kube-system    coredns-565d847f94-ml6k5                  0/1     ContainerCreating   0                3h40m</span><br><span class="line">kube-system    etcd-kubernetes1                          1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1                1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1       1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-proxy-9vwxl                          1/1     Running             0                76m</span><br><span class="line">kube-system    kube-proxy-qxsc7                          1/1     Running             0                69m</span><br><span class="line">kube-system    kube-proxy-v5msf                          1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1                1/1     Running             0                3h40m</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>排查步骤</strong> ：<br>查看日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs kube-flannel-ds-7q2hp -n kube-flannel</span></span><br><span class="line">Defaulted container &quot;kube-flannel&quot; out of: kube-flannel, install-cni-plugin (init), install-cni (init)</span><br><span class="line">I0913 06:42:19.799473       1 main.go:207] CLI flags config: &#123;etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true&#125;</span><br><span class="line">W0913 06:42:19.799563       1 client_config.go:614] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.</span><br><span class="line">I0913 06:42:19.903750       1 kube.go:120] Waiting 10m0s for node controller to sync</span><br><span class="line">I0913 06:42:19.903882       1 kube.go:401] Starting kube subnet manager</span><br><span class="line">I0913 06:42:20.903967       1 kube.go:127] Node controller sync successful</span><br><span class="line">I0913 06:42:20.903995       1 main.go:227] Created subnet manager: Kubernetes Subnet Manager - kubernetes1</span><br><span class="line">I0913 06:42:20.904004       1 main.go:230] Installing signal handlers</span><br><span class="line">I0913 06:42:20.904152       1 main.go:467] Found network config - Backend type: vxlan</span><br><span class="line">I0913 06:42:20.904195       1 match.go:206] Determining IP address of default interface</span><br><span class="line">I0913 06:42:20.904542       1 match.go:259] Using interface with name eth0 and address 172.31.10.19</span><br><span class="line">I0913 06:42:20.904570       1 match.go:281] Defaulting external address to interface address (172.31.10.19)</span><br><span class="line">I0913 06:42:20.904651       1 vxlan.go:138] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</span><br><span class="line">E0913 06:42:20.904962       1 main.go:330] Error registering network: failed to acquire lease: node &quot;kubernetes1&quot; pod cidr not assigned</span><br><span class="line">I0913 06:42:20.905100       1 main.go:447] Stopping shutdownHandler...</span><br><span class="line">W0913 06:42:20.905251       1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:402: watch of *v1.Node ended with: an error on the server (&quot;unable to decode an event from the watch stream: context canceled&quot;) has prevented the request from succeeding</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键日志： <code>Error registering network: failed to acquire lease: node &quot;kubernetes1&quot; pod cidr not assigned</code></p>
<p><strong>问题原因</strong> ： worker 节点的 flannel 组件无法正常获取 podCIDR 的定义</p>
<p><strong>解决方法</strong> ： 编辑控制节点上的配置文件 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code>，在 <code>- command</code> 下添加以下内容：</p>
<figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/kube-controller-manager.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">- --allocate-node-cidrs=true</span><br><span class="line">- --cluster-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure>
<p>如果内容已存在的话，更改 cidr 的网段和 <a href="#%E5%AE%89%E8%A3%85-kube-flannel"><code>kube-flannel.yml</code> 中的 cidr</a> 一致</p>
<p>更改配置后，重启所有节点的 <code>kubelet</code> 服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<p>重新查看所有 pod 状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS    RESTARTS         AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-7q2hp                     1/1     Running   62 (2m20s ago)   4h43m</span><br><span class="line">kube-flannel   kube-flannel-ds-k8wd6                     1/1     Running   55 (4m33s ago)   175m</span><br><span class="line">kube-flannel   kube-flannel-ds-x6ck2                     1/1     Running   38 (2m43s ago)   168m</span><br><span class="line">kube-system    coredns-565d847f94-b4sgn                  0/1     Running   0                5h19m</span><br><span class="line">kube-system    coredns-565d847f94-ml6k5                  0/1     Running   0                5h19m</span><br><span class="line">kube-system    etcd-kubernetes1                          1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1                1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1       1/1     Running   0                2m39s</span><br><span class="line">kube-system    kube-proxy-9vwxl                          1/1     Running   0                175m</span><br><span class="line">kube-system    kube-proxy-qxsc7                          1/1     Running   0                168m</span><br><span class="line">kube-system    kube-proxy-v5msf                          1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1                1/1     Running   0                5h19m</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="master-节点状态为-NotReady"><a href="#master-节点状态为-NotReady" class="headerlink" title="master 节点状态为 NotReady"></a>master 节点状态为 NotReady</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME              STATUS     ROLES                  AGE    VERSION</span><br><span class="line">k8s-work1         Ready      &lt;none&gt;                 99m    v1.21.2</span><br><span class="line">k8s-master        NotReady   control-plane,master   102m   v1.21.2</span><br><span class="line">k8s-work2         Ready      &lt;none&gt;                 99m    v1.21.2</span><br></pre></td></tr></table></figure>

<p>查看节点详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-master</span></span><br><span class="line"></span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message</span><br><span class="line">  ----             ------    -----------------                 ------------------                ------              -------</span><br><span class="line">  MemoryPressure   Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  DiskPressure     Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  PIDPressure      Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  Ready            Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看 <code>Pod</code> 状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-84s24                0/1     Pending   0          22m</span><br><span class="line">kube-flannel   kube-flannel-ds-qzd9g                1/1     Running   1          22m</span><br><span class="line">kube-flannel   kube-flannel-ds-sbtrr                1/1     Running   1          22m</span><br><span class="line">kube-system    coredns-558bd4d5db-8mbl5             1/1     Running   1          105m</span><br><span class="line">kube-system    coredns-558bd4d5db-gzrrx             1/1     Running   1          105m</span><br><span class="line">kube-system    etcd-k8s-master                      1/1     Running   0          105m</span><br><span class="line">kube-system    kube-apiserver-k8s-master            1/1     Running   0          105m</span><br><span class="line">kube-system    kube-proxy-747cx                     1/1     Running   1          103m</span><br><span class="line">kube-system    kube-proxy-8bs8l                     1/1     Running   1          103m</span><br><span class="line">kube-system    kube-proxy-mvqjq                     1/1     Running   0          105m</span><br><span class="line">kube-system    kube-scheduler-k8s-master            1/1     Running   0          105m</span><br></pre></td></tr></table></figure>
<p>结果显示 <code>kube-flannel</code> 位于 master 上的 <code>Pod</code> 状态异常。</p>
<p>查看 <code>kubelet</code> 日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line"> E1011 16:28:53.898132     796 kubelet_node_status.go:93] &quot;Unable to register node with API server&quot; err=&quot;nodes \&quot;k8s-admin\&quot; is forbidden: node \&quot;k8s-master\&quot; is not allowed to modify node \&quot;k8s-admin\&quot;&quot; node=&quot;k8s-admin&quot;</span><br><span class="line"> E1011 16:28:53.900459     796 kubelet.go:2291] &quot;Error getting node&quot; err=&quot;node \&quot;k8s-admin\&quot; not found&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看主机名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">hostname status</span></span><br><span class="line"></span><br><span class="line">   Static hostname: k8s-master</span><br><span class="line">Transient hostname: k8s-admin</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: b1527a5456aab241a74a8a3dc31395c0</span><br><span class="line">           Boot ID: f8428003692349298cf2bb9efae8a664</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.76.1.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>根据 <code>kubelet</code> 日志，Kubenetes 节点名称和主机名不一致。修改节点主机名。</p>
<h2 id="集群之外的服务器使用-kubectl-报错"><a href="#集群之外的服务器使用-kubectl-报错" class="headerlink" title="集群之外的服务器使用 kubectl 报错"></a>集群之外的服务器使用 kubectl 报错</h2><p><strong>问题场景</strong>：</p>
<p>将集群的管理配置文件 (<code>/etc/kubernetes/admin.conf</code>) 拷贝到集群之外的服务器，并命名为指定文件 <code>~/.kube/config</code>，修改 <code>~/.kube/config</code> 中 <code>server</code> 的 IP 为 Kubernetes API Server 的实际 IP，使用 <code>kubectl</code> 命令时，报错 <code>Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 10.150.0.21, not </code>。</p>
<p><strong>问题原因</strong>：</p>
<p>报错表示，当使用安全端口 6443 访问 Kubernetes API Server 时，默认证书中的 DNS 包含了 API Server 服务的 CLUSTER-IP 和 服务器的 IP ，如果是云主机，则为云服务器的私有 IP，不包含其公网 IP，如果使用公网 IP 访问 6443 端口，会报此错误</p>
<p>通过以下命令，可以看到默认的 API Server 的 HTTPS 证书中包含的 DNS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> /etc/kubernetes/pki</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">openssl x509 -noout -text -<span class="keyword">in</span> apiserver.crt</span></span><br><span class="line">...</span><br><span class="line">    DNS:k8s-master, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:10.150.0.21</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><strong>解决方法</strong>：</p>
<ul>
<li><p>使用 <code>kubectl</code> 的命令行选项 <code>--insecure-skip-tls-verify</code> 可跳过证书验证。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 10.150.0.21, not 34.150.1.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes --insecure-skip-tls-verify</span></span><br><span class="line"></span><br><span class="line">NAME         STATUS   ROLES                  AGE    VERSION</span><br><span class="line">k8s-master   Ready    control-plane,master   6d5h   v1.21.2</span><br><span class="line">k8s-work1    Ready    &lt;none&gt;                 6d5h   v1.21.2</span><br><span class="line">k8s-work2    Ready    &lt;none&gt;                 6d5h   v1.21.2</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>修改 <code>~/.kube/config</code> 中 <code>server</code> 地址为证书中包含的 DNS 名称，如 <code>k8s-master</code>，并确保域名本地可解析</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">grep server .kube/config</span></span><br><span class="line">  server: https://k8s-master:6443</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">  NAME         STATUS   ROLES                  AGE    VERSION</span><br><span class="line">  k8s-master   Ready    control-plane,master   6d6h   v1.21.2</span><br><span class="line">  k8s-work1    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line">  k8s-work2    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新生成 API Server 证书</p>
<ol>
<li><p>备份当前证书，并删除 <code>apiserver.crt</code> 和 <code>apiserver.key</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /etc/kubernetes/pki</span><br><span class="line">mkdir /data/k8s/backup/pki</span><br><span class="line">mv apiserver.* /data/k8s/backup/pki/</span><br></pre></td></tr></table></figure></li>
<li><p>生成新的 API Server 证书，默认 SAN 包括 <code>[节点名称 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1]</code>，如果不在此列表中的域名或者 IP 需要添加。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"> $ </span><span class="language-bash">kubeadm init phase certs apiserver \</span></span><br><span class="line"><span class="language-bash">         --apiserver-advertise-address  10.150.0.21 \</span></span><br><span class="line"><span class="language-bash">         --apiserver-cert-extra-sans 34.150.1.1</span></span><br><span class="line">     </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.21 34.150.1.1]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>10.150.0.21</code> - 为云主机的私有（内网） IP 地址，同时也是 <code>--apiserver-advertise-address</code>，必须要有</li>
<li><code>10.96.0.1</code> - 为 Kubernetes 集群中 API Server 对应的 Service 的 ClusterIP，必须要有</li>
<li><code>34.150.1.1</code> - 为云主机的公网（弹性） IP，是本次要添加的 IP</li>
</ul>
<p>如果还有其他 IP 或者域名，可以参照格式 <code>--apiserver-cert-extra-sans 34.150.1.1</code> 添加。</p>
<p> 命令执行成功后，会生成新的证书。<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ls apiserver*</span><br><span class="line">   apiserver.crt              apiserver-etcd-client.key  apiserver-kubelet-client.crt</span><br><span class="line">   apiserver-etcd-client.crt  apiserver.key              apiserver-kubelet-client.key</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>重启 kubelet 服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证</p>
<p> 在远端服务器执行以下命令验证效果 </p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ rm -rf .kube/cache/</span><br><span class="line">     </span><br><span class="line">$ grep server .kube/config </span><br><span class="line">  server: https://34.150.1.1:6443</span><br><span class="line">     </span><br><span class="line">$ kubectl get nodes</span><br><span class="line">   NAME                  STATUS   ROLES         AGE    VERSION</span><br><span class="line">   k8s-master   Ready    control-plane,master   6d6h   v1.21.2</span><br><span class="line">   k8s-work1    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line">   k8s-work2    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="kubelet-启动失败"><a href="#kubelet-启动失败" class="headerlink" title="kubelet 启动失败"></a>kubelet 启动失败</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet</span></span><br><span class="line">&quot;command failed&quot; err=&quot;failed to run Kubelet: running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swas contained: [Filename\t\t\t\tType\t\tSize\tUsed\tPriority /dev/dm-1</span><br></pre></td></tr></table></figure>
<p>根据 kubelet 服务日志，导致失败的原因为系统启用了 swap，关闭 swap，重新启动正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.9G        306M        120M        8.7M        1.5G        1.4G</span><br><span class="line">Swap:          3.9G        2.0M        3.9G</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">swapoff -a</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.9G        350M         71M        9.0M        1.5G        1.4G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>

<h2 id="集群证书未同步导致集群添加节点失败"><a href="#集群证书未同步导致集群添加节点失败" class="headerlink" title="集群证书未同步导致集群添加节点失败"></a>集群证书未同步导致集群添加节点失败</h2><p><strong>环境信息</strong></p>
<ul>
<li>kubeadm v1.32.2</li>
</ul>
<p>使用以下 <code>Configuration</code> 创建的集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">  imagePullSerial: true</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">name: node</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">taints: null</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">encryptionAlgorithm: RSA-2048</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node1 公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node1 公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node1 name&gt;=https://&lt;master node1 公网 IP&gt;:2380&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imageRepository: registry.k8s.io</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">proxy: &#123;&#125;</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: apiserver.k8s.ops</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用以下 <code>JoinConfiguration</code> 向集群中添加第二个 Master 节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">encryptionAlgorithm: RSA-2048</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node2 公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node2 公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node1 name&gt;=https://&lt;master node1 公网 IP&gt;:2380,&lt;master node2 name&gt;=https://&lt;master node2 公网 IP&gt;:2380&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imageRepository: registry.k8s.io</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">proxy: &#123;&#125;</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: apiserver.k8s.ops</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: JoinConfiguration</span><br><span class="line">nodeRegistration: </span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  </span><br><span class="line">controlPlane: </span><br><span class="line">  localAPIEndpoint:</span><br><span class="line">    advertiseAddress: &lt;master node2 公网 IP&gt;</span><br><span class="line">    bindPort: 6443</span><br><span class="line">    </span><br><span class="line">discovery:</span><br><span class="line">  bootstrapToken:</span><br><span class="line">    apiServerEndpoint: apiserver.k8s.ops:6443</span><br><span class="line">    token: pc0io6.d5j7zap89plt467t      # bootstrap token 和 CA Hash 使用命令 `kubeadm token create --print-join-command` 生成</span><br><span class="line">    caCertHashes:</span><br><span class="line">      - sha256:e16ebd4573d496c443c684637c21f3dccc569a71929e48a81515cfe43489c5f5</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>执行以下命令将节点加入集群报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm <span class="built_in">join</span> --config ./ops_k8s_join_config.yaml</span> </span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line"></span><br><span class="line">[preflight] Reading configuration from the &quot;kubeadm-config&quot; ConfigMap in namespace &quot;kube-system&quot;...</span><br><span class="line">[preflight] Use &#x27;kubeadm init phase upload-config --config your-config.yaml&#x27; to re-upload it.</span><br><span class="line">error execution phase preflight: </span><br><span class="line">One or more conditions for hosting a new control plane instance is not satisfied.</span><br><span class="line"></span><br><span class="line">[failure loading certificate for CA: couldn&#x27;t load the certificate file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory, failure loading key for service account: couldn&#x27;t load the private key file /etc/kubernetes/pki/sa.key: open /etc/kubernetes/pki/sa.key: no such file or directory, failure loading certificate for front-proxy CA: couldn&#x27;t load the certificate file /etc/kubernetes/pki/front-proxy-ca.crt: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory, failure loading certificate for etcd CA: couldn&#x27;t load the certificate file /etc/kubernetes/pki/etcd/ca.crt: open /etc/kubernetes/pki/etcd/ca.crt: no such file or directory]</span><br><span class="line"></span><br><span class="line">Please ensure that:</span><br><span class="line">* The cluster has a stable controlPlaneEndpoint address.</span><br><span class="line">* The certificates that must be shared among control plane instances are provided.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure>
<p>根据错误信息可知是因为 <strong>集群证书未部署到新节点</strong> ，可通过以下步骤解决：</p>
<ol>
<li>在 Master 节点（初始化集群的节点）生成 <code>certificateKey</code> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs certificate-key</span></span><br><span class="line">41529138a1ed7c84af5af73093cece23737ae63fb5362696d77618d</span><br></pre></td></tr></table></figure>
这个 <code>certificateKey</code> 是用于 <strong>加密并自动分发 Kubernetes 证书</strong> 的密钥。</li>
<li><strong>重新上传证书</strong>  。使用 <code>kubeadm init phase upload-certs</code> 命令，将现有集群证书加密并上传： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase upload-certs --upload-certs --certificate-key 41529138a1ed7c84af5af73093cece23737ae63fb5362696d77618d</span></span><br><span class="line">[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">41529138a1ed7c84af5af73093cece23737ae63fb5362696d77618d</span><br></pre></td></tr></table></figure>
此操作会：<ul>
<li>将 <code>/etc/kubernetes/pki/</code> 下的 证书 <strong>加密后上传到 <code>kube-system</code> 命名空间的 <code>kubeadm-certs</code> Secret 中</strong> 。  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get secret -A</span></span><br><span class="line">NAMESPACE              NAME                                         TYPE                            DATA   AGE</span><br><span class="line">kube-system            bootstrap-token-3cshhm                       bootstrap.kubernetes.io/token   4      2m12s</span><br><span class="line">kube-system            bootstrap-token-pc0io6                       bootstrap.kubernetes.io/token   6      160m</span><br><span class="line">kube-system            kubeadm-certs                                Opaque                          8      2m12s</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-csrf                    Opaque                          1      7d23h</span><br><span class="line">kubernetes-dashboard   sh.helm.release.v1.kubernetes-dashboard.v1   helm.sh/release.v1              1      7d23h</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>其他 Control Plane 节点在加入时可以自动下载这些证书。</li>
</ul>
</li>
<li>加入新的 Control Plane 节点 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join apiserver.k8s.ops:6443 --control-plane --token &lt;你的 bootstrap token&gt; \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:&lt;你的 CA 证书哈希&gt; \</span><br><span class="line">    --certificate-key 60f49c3fcb2a1c5e3e75e48aeda4dfb7c3a1c7b93baf1f6e3c9e5e45a6c7f4a5</span><br><span class="line"></span><br></pre></td></tr></table></figure>
或使用以下 <code>JoinConfiguration</code> <figure class="highlight shell"><figcaption><span>JoinConfiguration</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: JoinConfiguration</span><br><span class="line">nodeRegistration: </span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  </span><br><span class="line">controlPlane: </span><br><span class="line">  localAPIEndpoint:</span><br><span class="line">    advertiseAddress: &lt;master node2 公网 IP&gt;</span><br><span class="line">    bindPort: 6443</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">启用证书自动同步</span></span><br><span class="line">  certificateKey: &quot;&lt;你的证书密钥&gt;&quot;</span><br><span class="line"></span><br><span class="line">discovery:</span><br><span class="line">  bootstrapToken:</span><br><span class="line">    apiServerEndpoint: apiserver.k8s.ops:6443</span><br><span class="line">    token: pc0io6.d5j7zap89plt467t      # bootstrap token 和 CA Hash 使用命令 `kubeadm token create --print-join-command` 生成</span><br><span class="line">    caCertHashes:</span><br><span class="line">      - sha256:e16ebd4573d496c443c684637c21f3dccc569a71929e48a81515cfe43489c5f5</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
建议在 <strong>创建集群时自动上传证书，方便证书同步到新节点</strong><figure class="highlight shell"><figcaption><span>InitConfiguration</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">  imagePullSerial: true</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">name: node</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">taints: null</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启用证书自动上传，证书密钥使用 `kubeadm certs certificate-key` 生成</span></span><br><span class="line">certificateKey: &quot;&lt;你的证书密钥&gt;&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="节点名称不在-kube-apiserver-证书中导致添加节点失败"><a href="#节点名称不在-kube-apiserver-证书中导致添加节点失败" class="headerlink" title="节点名称不在 kube-apiserver 证书中导致添加节点失败"></a>节点名称不在 kube-apiserver 证书中导致添加节点失败</h2><p><strong>环境信息</strong></p>
<ul>
<li>kubeadm v1.32.2</li>
</ul>
<p>添加节点到集群报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm <span class="built_in">join</span> --config ./ops_k8s_config/ops_k8s_join_config.yaml</span> </span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the &quot;kubeadm-config&quot; ConfigMap in namespace &quot;kube-system&quot;...</span><br><span class="line">[preflight] Use &#x27;kubeadm init phase upload-config --config your-config.yaml&#x27; to re-upload it.</span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">W0311 08:47:22.571697 2190851 checks.go:846] detected that the sandbox image &quot;registry.k8s.io/pause:3.6&quot; of the container runtime is inconsistent with that used by kubeadm.It is recommended to use &quot;registry.k8s.io/pause:3.10&quot; as the CRI sandbox image.</span><br><span class="line">[download-certs] Downloading the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[download-certs] Saving the certificates to the folder: &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">error execution phase control-plane-prepare/certs: error creating PKI assets: failed to write or validate certificate &quot;apiserver&quot;: certificate apiserver is invalid: x509: certificate is valid for apiserver.k8s.ops, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local, k8s-master-1, not k8s-master-2</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>根据关键报错信息： <code>error execution phase control-plane-prepare/certs: error creating PKI assets: failed to write or validate certificate &quot;apiserver&quot;: certificate apiserver is invalid: x509: certificate is valid for apiserver.k8s.ops, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local, k8s-master-1, not k8s-master-2</code> 可知，错误是因为 <strong>新的节点名称 <code>k8s-master-2</code> 不在 <code>kube-apiserver</code> 证书的 SAN 列表中</strong></p>
<p>可以参考以下解决方案更新 <code>kube-apiserver</code> 证书：</p>
<ol>
<li><p><strong>在主节点（<code>kubeadm init</code> 节点）上备份证书</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/kubernetes/backup</span><br><span class="line"></span><br><span class="line">cp -r /etc/kubernetes/pki/* /opt/kubernetes/backup/</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>删除现有 <code>apiserver</code> 证书</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -f /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>使用 <code>kubeadm init phase certs</code> 重新生成 <code>apiserver</code> 证书</strong></p>
 <figure class="highlight shell"><figcaption><span>ClusterConfiguration</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">apiserver 证书中的 SANs 列表</span></span><br><span class="line">  certSANs:</span><br><span class="line">    - &quot;apiserver.k8s.ops&quot;</span><br><span class="line">    - &quot;kubernetes&quot;</span><br><span class="line">    - &quot;kubernetes.default&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    - &quot;&lt;master-1 主机名称&gt;&quot;</span><br><span class="line">    - &quot;&lt;master-2 主机名称&gt;&quot;  # 这里新增你的节点</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 根据 <code>ClusterConfiguration</code> 重新生成 <code>apiserver</code> 证书</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs apiserver --config ./ops_k8s_config/k8s_kubeadm_init_config.yaml</span> </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [apiserver.k8s.ops kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local &lt;master-1 主机名称&gt; &lt;master-2 主机名称&gt;] and IPs []</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>重新上传证书</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs certificate-key</span></span><br><span class="line">a688533badf0e0631af433723d576a47297b131d479548d9afc8f97c5f237bb7</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase upload-certs --upload-certs --certificate-key a688533badf0e0631af433723d576a47297b131d479548d9afc8f97c5f237bb7</span></span><br><span class="line">[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">a688533badf0e0631af433723d576a47297b131d479548d9afc8f97c5f237bb7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>在 <master-2> 节点上更新节点的 <code>JoinConfiguration</code> 配置并重新加入集群</strong></p>
 <figure class="highlight shell"><figcaption><span>JoinConfiguration</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: JoinConfiguration</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line"></span><br><span class="line">controlPlane:</span><br><span class="line">  localAPIEndpoint:</span><br><span class="line">    advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">    bindPort: 6443</span><br><span class="line">  certificateKey: &quot;a688533badf0e0631af433723d576a47297b131d479548d9afc8f97c5f237bb7&quot;</span><br><span class="line"></span><br><span class="line">discovery:</span><br><span class="line">  bootstrapToken:</span><br><span class="line">    apiServerEndpoint: apiserver.k8s.ops:6443</span><br><span class="line">    token: pc0io6.d5j7zap89plt467t</span><br><span class="line">    caCertHashes:</span><br><span class="line">      - sha256:e16ebd4573d496c443c684637c21f3dccc569a71929e48a81515cfe43489c5f5</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（删除旧的证书）成功加入集群</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">rm</span> -rf /etc/kubernetes/pki/*</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm <span class="built_in">join</span> --config ./ops_k8s_config/ops_k8s_join_config.yaml</span> </span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="etcd-使用了-http-协议导致添加节点失败"><a href="#etcd-使用了-http-协议导致添加节点失败" class="headerlink" title="etcd 使用了 http 协议导致添加节点失败"></a>etcd 使用了 http 协议导致添加节点失败</h2><p><strong>环境信息</strong></p>
<ul>
<li>kubeadm v1.32.2</li>
</ul>
<p>集群初始化使用以下 <code>Configuration</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">  imagePullSerial: true</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">name: node</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">taints: null</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启用证书自动上传，证书密钥使用 `kubeadm certs certificate-key` 生成</span></span><br><span class="line">certificateKey: &quot;&lt;你的证书密钥&gt;&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">apiserver 证书中的 SANs 列表</span></span><br><span class="line">  certSANs:</span><br><span class="line">    - &quot;apiserver.k8s.ops&quot;</span><br><span class="line">    - &quot;kubernetes&quot;</span><br><span class="line">    - &quot;kubernetes.default&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    - &quot;&lt;master-1 主机名称&gt;&quot;</span><br><span class="line">    - &quot;&lt;master-2 主机名称&gt;&quot;  # 这里新增你的节点</span><br><span class="line"></span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">encryptionAlgorithm: RSA-2048</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;http://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;http://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;http://&lt;公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;http://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node name&gt;=http://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imageRepository: registry.k8s.io</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">proxy: &#123;&#125;</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: apiserver.k8s.ops</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br></pre></td></tr></table></figure>

<p>使用以下 <code>Configuration</code> 加入新的管理节点到集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">encryptionAlgorithm: RSA-2048</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node2 公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;master node2 公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node1 name&gt;=https://&lt;master node1 公网 IP&gt;:2380,&lt;master node2 name&gt;=https://&lt;master node2 公网 IP&gt;:2380&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imageRepository: registry.k8s.io</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">proxy: &#123;&#125;</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: apiserver.k8s.ops</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: JoinConfiguration</span><br><span class="line">nodeRegistration: </span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  </span><br><span class="line">controlPlane: </span><br><span class="line">  localAPIEndpoint:</span><br><span class="line">    advertiseAddress: &lt;master node2 公网 IP&gt;</span><br><span class="line">    bindPort: 6443</span><br><span class="line">  certificateKey: &quot;a688533badf0e0631af433723d576a47297b131d479548d9afc8f97c5f237bb7&quot;</span><br><span class="line"></span><br><span class="line">discovery:</span><br><span class="line">  bootstrapToken:</span><br><span class="line">    apiServerEndpoint: apiserver.k8s.ops:6443</span><br><span class="line">    token: pc0io6.d5j7zap89plt467t </span><br><span class="line">    caCertHashes:</span><br><span class="line">      - sha256:e16ebd4573d496c443c684637c21f3dccc569a71929e48a81515cfe43489c5f5</span><br></pre></td></tr></table></figure>

<p>加入集群报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm <span class="built_in">join</span> --v=5 --config ./ops_k8s_join_config.yaml</span> </span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">I0311 09:52:04.963005 2196770 local.go:72] [etcd] Checking etcd cluster health</span><br><span class="line">I0311 09:52:04.963011 2196770 local.go:75] creating etcd client that connects to etcd pods</span><br><span class="line">I0311 09:52:04.963022 2196770 etcd.go:215] retrieving etcd endpoints from &quot;kubeadm.kubernetes.io/etcd.advertise-client-urls&quot; annotation in etcd Pods</span><br><span class="line">I0311 09:52:04.994193 2196770 etcd.go:149] etcd endpoints read from pods: https://&lt;master node1 公网 IP&gt;:2379</span><br><span class="line">context deadline exceeded</span><br><span class="line">error syncing endpoints with etcd</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/util/etcd.NewFromCluster</span><br><span class="line">	k8s.io/kubernetes/cmd/kubeadm/app/util/etcd/etcd.go:165</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/phases/etcd.CheckLocalEtcdClusterStatus</span><br><span class="line">	k8s.io/kubernetes/cmd/kubeadm/app/phases/etcd/local.go:76</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join.runCheckEtcdPhase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure>
<p>关键错误信息： <code>etcd endpoints read from pods: https://&lt;master node1 公网 IP&gt;:2379 context deadline exceeded, error syncing endpoints with etcd</code>，可知问题主要出现在 <strong><code>etcd</code> 集群上，接着检查 <code>etcd</code> 集群的状态</strong></p>
<p>在 <code>&lt;master node1&gt;</code> 上检查 <code>etcd</code> 发现 <code>etcd</code> 监听在 http 协议，而不是 <code>https</code> 协议，加入新节点到集群默认使用 HTTPS 协议连接 <code>etcd</code> 集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -n kube-system -it &lt;master node1&gt; -- sh -c <span class="string">&quot;ETCDCTL_API=3 etcdctl endpoint status --endpoints=http://127.0.0.1:2379&quot;</span></span></span><br><span class="line">http://127.0.0.1:2379, dfbf99b655630cf3, 3.5.16, 2.8 MB, true, false, 2, 1154612, 1154612, </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -n kube-system -it &lt;master node1&gt; -- sh -c <span class="string">&quot;ETCDCTL_API=3 etcdctl endpoint status --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key&quot;</span></span></span><br><span class="line">&#123;&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2025-03-12T01:52:16.250711Z&quot;,&quot;logger&quot;:&quot;etcd-client&quot;,&quot;caller&quot;:&quot;v3@v3.5.16/retry_interceptor.go:63&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;etcd-endpoints://0xc0004c6000/127.0.0.1:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \&quot;transport: authentication handshake failed: EOF\&quot;&quot;&#125;</span><br><span class="line">Error: context deadline exceeded</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>检查初始化集群使用的 <code>etcd</code> 配置发现初始化时，<code>etcd</code> 使用了 HTTP 协议监听，而不是 HTTPS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;http://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;http://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;http://&lt;公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;http://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node name&gt;=http://&lt;公网 IP&gt;:2380&quot;</span><br></pre></td></tr></table></figure>

<p>在 Kubernetes 中，要将 <code>etcd</code> 的监听协议由 HTTP 改为 HTTPS，可以参考以下步骤：</p>
<ol>
<li><p>确保 <code>etcd</code> 所需证书已经存在，如不存在，重新生成相关证书</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">ls</span> -l /etc/kubernetes/pki/etcd/</span></span><br><span class="line">total 32</span><br><span class="line">-rw-r--r-- 1 root root 1094 Mar  3 16:09 ca.crt</span><br><span class="line">-rw------- 1 root root 1679 Mar  3 16:09 ca.key</span><br><span class="line">-rw-r--r-- 1 root root 1123 Mar  3 16:09 healthcheck-client.crt</span><br><span class="line">-rw------- 1 root root 1675 Mar  3 16:09 healthcheck-client.key</span><br><span class="line">-rw-r--r-- 1 root root 1212 Mar  3 16:09 peer.crt</span><br><span class="line">-rw------- 1 root root 1679 Mar  3 16:09 peer.key</span><br><span class="line">-rw-r--r-- 1 root root 1212 Mar  3 16:09 server.crt</span><br><span class="line">-rw------- 1 root root 1679 Mar  3 16:09 server.key</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 如果不存在证书，使用以下命令生成证书</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs etcd-ca</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs etcd-server</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase certs etcd-peer</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 然后重新上传</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm init phase upload-certs --upload-certs --certificate-key &lt;你的证书密钥&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>修改 <code>etcd</code> 静态 Pod 配置（<code>vim /etc/kubernetes/manifests/etcd.yaml</code>）</strong> ，确保监听协议使用 HTTPS</p>
 <figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/etcd.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://&lt;公网 IP&gt;:2379</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: etcd</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: etcd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - etcd</span><br><span class="line">    - --advertise-client-urls=https://&lt;公网 IP&gt;:2379</span><br><span class="line">    - --cert-file=/etc/kubernetes/pki/etcd/server.crt</span><br><span class="line">    - --client-cert-auth=true</span><br><span class="line">    - --data-dir=/var/lib/etcd</span><br><span class="line">    - --experimental-initial-corrupt-check=true</span><br><span class="line">    - --experimental-watch-progress-notify-interval=5s</span><br><span class="line">    - --initial-advertise-peer-urls=https://&lt;公网 IP&gt;:2380</span><br><span class="line">    - --initial-cluster=qqc-bw-js-ngx=https://&lt;公网 IP&gt;:2380</span><br><span class="line">    - --key-file=/etc/kubernetes/pki/etcd/server.key</span><br><span class="line">    - --listen-client-urls=https://0.0.0.0:2379</span><br><span class="line">    - --listen-metrics-urls=http://127.0.0.1:2381</span><br><span class="line">    - --listen-peer-urls=https://0.0.0.0:2380</span><br><span class="line">    - --name=qqc-bw-js-ngx</span><br><span class="line">    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt</span><br><span class="line">    - --peer-client-cert-auth=true</span><br><span class="line">    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key</span><br><span class="line">    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    - --snapshot-count=10000</span><br><span class="line">    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>--listen-metrics-urls=http://127.0.0.1:2381</code>  要使用 HTTP 协议，否则会导致 <code>etcd</code> 容器因健康检查失败一直重启</p>
</blockquote>
</li>
<li><p><strong>修改 <code>kube-apiserver</code> 静态 Pod 配置（<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>）</strong> ，确保 <code>kube-apiserver</code> 使用 HTTPS 协议连接 <code>etcd</code></p>
 <figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/kube-apiserver.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">- --etcd-servers=https://&lt;公网 IP&gt;:2379</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>重启 <code>kubelet</code> 服务</strong></p>
</li>
</ol>
<h2 id="kube-proxy-失败"><a href="#kube-proxy-失败" class="headerlink" title="kube-proxy 失败"></a>kube-proxy 失败</h2><p><strong>环境信息</strong></p>
<ul>
<li>CentOS Linux 7 5.15.80-200.el7.x86_64</li>
<li>kubeadm v1.32.3</li>
<li>containerd v2.0.0</li>
<li>runc spec: 1.0.1-dev</li>
</ul>
<p>将节点作为管理节点加入集群后，新节点上的 Pod <code>kube-proxy</code> 一直处于重启状态，检查 Pod 状态信息如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe pod -n kube-system  kube-proxy-g7vln</span></span><br><span class="line">Name:                 kube-proxy-g7vln</span><br><span class="line">Namespace:            kube-system</span><br><span class="line">Priority:             2000001000</span><br><span class="line">Priority Class Name:  system-node-critical</span><br><span class="line">Service Account:      kube-proxy</span><br><span class="line">Node:                 k8s-master-2/172.31.36.21</span><br><span class="line">Start Time:           Wed, 12 Mar 2025 16:24:16 +0800</span><br><span class="line">Labels:               controller-revision-hash=64b9dbc74b</span><br><span class="line">                      k8s-app=kube-proxy</span><br><span class="line">                      pod-template-generation=1</span><br><span class="line">Annotations:          &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Controlled By:  DaemonSet/kube-proxy</span><br><span class="line">Containers:</span><br><span class="line">  kube-proxy:</span><br><span class="line">    Container ID:  containerd://6baf4e0502ac8cd28c57f36e1910e8adf338f563e5a93b05f116bdc7cdffd10a</span><br><span class="line">    Image:         registry.k8s.io/kube-proxy:v1.32.0</span><br><span class="line">    Image ID:      registry.k8s.io/kube-proxy@sha256:6aee00d0c7f4869144d1bdbbed7572cd55fd1a4d58fef5a21f53836054cb39b4</span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      /usr/local/bin/kube-proxy</span><br><span class="line">      --config=/var/lib/kube-proxy/config.conf</span><br><span class="line">      --hostname-override=$(NODE_NAME)</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       RunContainerError</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       StartError</span><br><span class="line">      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;unknown capability \&quot;CAP_PERFMON\&quot;&quot;</span><br><span class="line">      Exit Code:    128</span><br><span class="line">      Started:      Thu, 01 Jan 1970 08:00:00 +0800</span><br><span class="line">      Finished:     Wed, 12 Mar 2025 16:24:33 +0800</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  2</span><br><span class="line">    Environment:</span><br><span class="line">      NODE_NAME:   (v1:spec.nodeName)</span><br><span class="line">    Mounts:</span><br><span class="line">      /lib/modules from lib-modules (ro)</span><br><span class="line">      /run/xtables.lock from xtables-lock (rw)</span><br><span class="line">      /var/lib/kube-proxy from kube-proxy (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s88vz (ro)</span><br><span class="line">QoS Class:                   BestEffort</span><br><span class="line">Node-Selectors:              kubernetes.io/os=linux</span><br><span class="line">Tolerations:                 op=Exists</span><br><span class="line">                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists</span><br><span class="line">                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists</span><br><span class="line">                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists</span><br><span class="line">                             node.kubernetes.io/not-ready:NoExecute op=Exists</span><br><span class="line">                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists</span><br><span class="line">                             node.kubernetes.io/unreachable:NoExecute op=Exists</span><br><span class="line">                             node.kubernetes.io/unschedulable:NoSchedule op=Exists</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age               From               Message</span><br><span class="line">  ----     ------     ----              ----               -------</span><br><span class="line">  Normal   Scheduled  22s               default-scheduler  Successfully assigned kube-system/kube-proxy-g7vln to wjs-adm</span><br><span class="line">  Normal   Pulled     5s (x3 over 21s)  kubelet            Container image &quot;registry.k8s.io/kube-proxy:v1.32.0&quot; already present on machine</span><br><span class="line">  Normal   Created    5s (x3 over 21s)  kubelet            Created container: kube-proxy</span><br><span class="line">  Warning  Failed     5s (x3 over 21s)  kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;unknown capability \&quot;CAP_PERFMON\&quot;&quot;</span><br><span class="line">  Warning  BackOff    5s (x2 over 20s)  kubelet            Back-off restarting failed container kube-proxy in pod kube-proxy-g7vln_kube-system(c4929b83-bec4-44f0-a640-38a0b52da540)</span><br></pre></td></tr></table></figure>

<p>关键错误信息： <code>failed to create containerd task: failed to create shim task: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;unknown capability \&quot;CAP_PERFMON\&quot;&quot;</code></p>
<p><strong><code>CAP_PERFMON</code> 是一个较新的 Linux 能力，用于性能监控相关的操作。它是在 Linux 内核 5.8 版本中引入的。如果你的节点内核版本低于 5.8，或者容器运行时（如 containerd 或 runc）版本较旧，可能无法识别 <code>CAP_PERFMON</code> ，从而导致容器启动失败。</strong></p>
<p>本节点内核版本 <code>5.15.80-200.el7.x86_64</code> ，符合要求。 <code>containerd</code> 已是当前最新版本 <code>v2.0.0</code>， <strong><code>runc</code> 版本为 <code>1.0.1-dev</code> ，属于较旧版本，如果版本低于 <code>1.1.11</code> ，需要升级</strong> 。执行以下命令升级 <code>runc</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VERSION=$(curl -s https://api.github.com/repos/opencontainers/runc/releases/latest | grep tag_name | cut -d &#x27;&quot;&#x27; -f 4)</span><br><span class="line"></span><br><span class="line">wget https://github.com/opencontainers/runc/releases/download/$&#123;VERSION&#125;/runc.amd64 -O runc</span><br><span class="line"></span><br><span class="line">chmod +x runc</span><br><span class="line"></span><br><span class="line">sudo mv runc /usr/local/sbin/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>然后重新启动 <code>containerd</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart containerd</span><br></pre></td></tr></table></figure>
<p>升级 <code>runc</code> 到最新版本（<code>1.3.0-rc.1</code>）后，<code>kube-proxy</code> 状态正常。</p>
<h2 id="etcd-节点加入-etcd-集群失败"><a href="#etcd-节点加入-etcd-集群失败" class="headerlink" title="etcd 节点加入 etcd 集群失败"></a>etcd 节点加入 etcd 集群失败</h2><p>新的 <code>etcd</code> 节点未正确的加入到 <code>etcd</code> 集群中，检查日志信息如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nerdctl logs 979519e94b9c</span></span><br><span class="line">&#123;Running http and grpc server on single port. This is not recommended for production.&quot;&#125;</span><br><span class="line">&#123;&quot;Running: &quot;,&quot;args&quot;:[&quot;etcd&quot;,&quot;--advertise-client-urls=https://&lt;新 etcd 节点  IP&gt;:2379&quot;,&quot;--cert-file=/etc/kubernetes/pki/etcd/server.crt&quot;,&quot;--client-cert-auth=true&quot;,&quot;--data-dir=/var/lib/etcd&quot;,&quot;--experimental-initial-corrupt-check=true&quot;,&quot;--experimental-watch-progress-notify-interval=5s&quot;,&quot;--initial-advertise-peer-urls=https://&lt;新 etcd 节点  IP&gt;:2380&quot;,&quot;--initial-cluster=qqc-bw-js-ngx=https://&lt;新 etcd 节点  IP&gt;:2380&quot;,&quot;--initial-cluster-state=existing&quot;,&quot;--key-file=/etc/kubernetes/pki/etcd/server.key&quot;,&quot;--listen-client-urls=https://0.0.0.0:2379&quot;,&quot;--listen-metrics-urls=http://127.0.0.1:2381&quot;,&quot;--listen-peer-urls=https://0.0.0.0:2380&quot;,&quot;--name=&lt;新 etcd 节点名称&gt;&quot;,&quot;--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt&quot;,&quot;--peer-client-cert-auth=true&quot;,&quot;--peer-key-file=/etc/kubernetes/pki/etcd/peer.key&quot;,&quot;--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt&quot;,&quot;--snapshot-count=10000&quot;,&quot;--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt&quot;]&#125;</span><br><span class="line">&#123;&quot;server has been already initialized&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;dir-type&quot;:&quot;member&quot;&#125;</span><br><span class="line">&#123;&quot;Running http and grpc server on single port. This is not recommended for production.&quot;&#125;</span><br><span class="line">&#123;&quot;configuring peer listeners&quot;,&quot;listen-peer-urls&quot;:[&quot;https://0.0.0.0:2380&quot;]&#125;</span><br><span class="line">&#123;&quot;starting with peer TLS&quot;,&quot;tls-info&quot;:&quot;cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = &quot;,&quot;cipher-suites&quot;:[]&#125;</span><br><span class="line">&#123;&quot;configuring client listeners&quot;,&quot;listen-client-urls&quot;:[&quot;https://0.0.0.0:2379&quot;]&#125;</span><br><span class="line">&#123;&quot;starting an etcd server&quot;,&quot;etcd-version&quot;:&quot;3.5.16&quot;,&quot;git-sha&quot;:&quot;f20bbad&quot;,&quot;go-version&quot;:&quot;go1.22.7&quot;,&quot;go-os&quot;:&quot;linux&quot;,&quot;go-arch&quot;:&quot;amd64&quot;,&quot;max-cpu-set&quot;:4,&quot;max-cpu-available&quot;:4,&quot;member-initialized&quot;:false,&quot;name&quot;:&quot;&lt;新 etcd 节点名称&gt;&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;wal-dir&quot;:&quot;&quot;,&quot;wal-dir-dedicated&quot;:&quot;&quot;,&quot;member-dir&quot;:&quot;/var/lib/etcd/member&quot;,&quot;force-new-cluster&quot;:false,&quot;heartbeat-interval&quot;:&quot;100ms&quot;,&quot;election-timeout&quot;:&quot;1s&quot;,&quot;initial-election-tick-advance&quot;:true,&quot;snapshot-count&quot;:10000,&quot;max-wals&quot;:5,&quot;max-snapshots&quot;:5,&quot;snapshot-catchup-entries&quot;:5000,&quot;initial-advertise-peer-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2380&quot;],&quot;listen-peer-urls&quot;:[&quot;https://0.0.0.0:2380&quot;],&quot;advertise-client-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2379&quot;],&quot;listen-client-urls&quot;:[&quot;https://0.0.0.0:2379&quot;],&quot;listen-metrics-urls&quot;:[&quot;http://127.0.0.1:2381&quot;],&quot;cors&quot;:[&quot;*&quot;],&quot;host-whitelist&quot;:[&quot;*&quot;],&quot;initial-cluster&quot;:&quot;qqc-bw-js-ngx=https://&lt;新 etcd 节点  IP&gt;:2380&quot;,&quot;initial-cluster-state&quot;:&quot;existing&quot;,&quot;initial-cluster-token&quot;:&quot;etcd-cluster&quot;,&quot;quota-backend-bytes&quot;:2147483648,&quot;max-request-bytes&quot;:1572864,&quot;max-concurrent-streams&quot;:4294967295,&quot;pre-vote&quot;:true,&quot;initial-corrupt-check&quot;:true,&quot;corrupt-check-time-interval&quot;:&quot;0s&quot;,&quot;compact-check-time-enabled&quot;:false,&quot;compact-check-time-interval&quot;:&quot;1m0s&quot;,&quot;auto-compaction-mode&quot;:&quot;periodic&quot;,&quot;auto-compaction-retention&quot;:&quot;0s&quot;,&quot;auto-compaction-interval&quot;:&quot;0s&quot;,&quot;discovery-url&quot;:&quot;&quot;,&quot;discovery-proxy&quot;:&quot;&quot;,&quot;downgrade-check-interval&quot;:&quot;5s&quot;&#125;</span><br><span class="line">&#123;&quot;msg&quot;:&quot;opened backend db&quot;,&quot;path&quot;:&quot;/var/lib/etcd/member/snap/db&quot;,&quot;took&quot;:&quot;800.305µs&quot;&#125;</span><br><span class="line">&#123;&quot;closing etcd server&quot;,&quot;name&quot;:&quot;&lt;新 etcd 节点名称&gt;&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;advertise-peer-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2380&quot;],&quot;advertise-client-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2379&quot;]&#125;</span><br><span class="line">&#123;&quot;closed etcd server&quot;,&quot;name&quot;:&quot;&lt;新 etcd 节点名称&gt;&quot;,&quot;data-dir&quot;:&quot;/var/lib/etcd&quot;,&quot;advertise-peer-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2380&quot;],&quot;advertise-client-urls&quot;:[&quot;https://&lt;新 etcd 节点  IP&gt;:2379&quot;]&#125;</span><br><span class="line">&#123;&quot;discovery failed&quot;,&quot;error&quot;:&quot;couldn&#x27;t find local name \&quot;&lt;新 etcd 节点名称&gt;\&quot; in the initial cluster configuration&quot;,&quot;stacktrace&quot;:&quot;go.etcd.io/etcd/server/v3/etcdmain.startEtcdOrProxyV2\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:204\ngo.etcd.io/etcd/server/v3/etcdmain.Main\n\tgo.etcd.io/etcd/server/v3/etcdmain/main.go:40\nmain.main\n\tgo.etcd.io/etcd/server/v3/main.go:31\nruntime.main\n\truntime/proc.go:271&quot;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键报错信息： <code>&quot;discovery failed&quot;,&quot;error&quot;:&quot;couldn&#39;t find local name \&quot;&lt;新 etcd 节点名称&gt;\&quot; in the initial cluster configuration&quot;</code>，表示新的节点名称未加入 <code>etcd</code> 的 <code>initial-cluster</code> 中。检查新节点上的 <code>etcd</code> 静态 Pod 配置（<code>/etc/kubernetes/manifests/etcd.yaml</code>）发现新节点的信息不存在，添加新的节点信息即可</p>
<figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/etcd.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">--initial-cluster=k8s-master-1=https://&lt;k8s-master-1 IP&gt;:2380,k8s-master-2=https://&lt;k8s-master-2 IP&gt;:2380,&lt;新 etcd 节点名称&gt;=https://&lt;新 etcd 节点  IP&gt;:2380</span><br></pre></td></tr></table></figure>


<h1 id="其他常用配置"><a href="#其他常用配置" class="headerlink" title="其他常用配置"></a>其他常用配置</h1><h2 id="新增节点"><a href="#新增节点" class="headerlink" title="新增节点"></a>新增节点</h2><p>要为集群新增 worker 节点，参考以下步骤</p>
<ol>
<li>加入前准备工作<br><a href="#kubernetes-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%89%8D%E9%85%8D%E7%BD%AE">参考安装步骤，配置新节点</a></li>
<li>在集群的 master 节点执行以下命令，获取加入集群的命令<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm token create --print-join-command</span></span><br><span class="line">kubeadm join cluster:6443 --token cuxvrexi24aejb --discovery-token-ca-cert-hash sha256:9e5b71bc392</span><br></pre></td></tr></table></figure></li>
<li>在新节点上执行加入集群的命令 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join cluster:6443 --token cuxvrexi24aejb --discovery-token-ca-cert-hash sha256:9e5b71bc392</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="将-CRI-由-containerd-变更为-Docker"><a href="#将-CRI-由-containerd-变更为-Docker" class="headerlink" title="将 CRI 由 containerd 变更为 Docker"></a>将 CRI 由 <code>containerd</code> 变更为 <code>Docker</code></h2><p>编辑 <code>/var/lib/kubelet/kubeadm-flags.env</code> 文件，在该文件中可以添加 <code>kubelet</code> 启动参数，将 <code>--container-runtime-endpoint</code> 标志，设置为 <code>unix:///var/run/cri-dockerd.sock</code> <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[配置 kubelet 使用 cri-dockerd](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/#configure-the-kubelet-to-use-cri-dockerd)">[1]</span></a></sup></p>
<figure class="highlight shell"><figcaption><span>/var/lib/kubelet/kubeadm-flags.env</span></figcaption><table><tr><td class="code"><pre><span class="line">KUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.8&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>kubeadm</code> 工具将节点上的套接字存储为控制面上 <code>Node</code> 对象的注解。 要为每个被影响的节点更改此套接字：</p>
<ol>
<li>编辑 Node 对象的 YAML 表示：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/path/to/admin.conf kubectl edit no &lt;NODE_NAME&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li><code>/path/to/admin.conf</code> ：指向 <code>kubectl</code> 配置文件 <code>admin.conf</code> 的路径；</li>
<li><code>&lt;NODE_NAME&gt;</code> ：你要修改的节点的名称。</li>
</ul>
<ol start="2">
<li>将 <code>kubeadm.alpha.kubernetes.io/cri-socket</code> 标志更改为 <code>unix:///var/run/cri-dockerd.sock</code>；</li>
</ol>
<p>配置完成后，重启 kubelet</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet </span><br></pre></td></tr></table></figure>

<p>查看 node 使用的 CRI</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME              STATUS   ROLES       AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">kubernetes1   Ready    control-plane   7d2h   v1.25.0   172.31.5.58    &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br><span class="line">kubernetes2   Ready    &lt;none&gt;          7d1h   v1.25.0   172.31.5.68    &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br><span class="line">kubernetes3   Ready    &lt;none&gt;          7d1h   v1.25.0   172.31.0.230   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br></pre></td></tr></table></figure>

<h2 id="修改-kubelet-使用的-CRI-为-containerd"><a href="#修改-kubelet-使用的-CRI-为-containerd" class="headerlink" title="修改 kubelet 使用的 CRI 为 containerd"></a>修改 kubelet 使用的 CRI 为 containerd</h2><p>修改之前，<code>kubelet</code> 使用的 CRI 为 <code>cri-docker</code>。修改步骤如下</p>
<ol>
<li><p>修改 kubelet 配置及相关节点配置。</p>
<p>编辑相关节点上的 kubelet 配置文件 <code>/var/lib/kubelet/kubeadm-flags.env</code> 修改 <code>--container-runtime-endpoint</code> 为 <code>containerd</code> 的 socket 地址</p>
 <figure class="highlight shell"><figcaption><span>/var/lib/kubelet/kubeadm-flags.env</span></figcaption><table><tr><td class="code"><pre><span class="line">--container-runtime-endpoint=unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure></li>
<li><p>在 Master 节点上，执行以下命令，编辑相关节点配置，修改配置 <code>kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock</code> <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[配置 kubelet 使用 containerd 作为其容器运行时](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/#%E9%85%8D%E7%BD%AE-kubelet-%E4%BD%BF%E7%94%A8-containerd-%E4%BD%9C%E4%B8%BA%E5%85%B6%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6)">[9]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl edit node master2</span></span><br><span class="line">kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启相关服务，无需使用 <code>docker</code> 时可停止 <code>docker</code> 服务</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart containerd  &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果启动 <code>kubelet</code> 失败，报错误： <code>Error: failed to run Kubelet: failed to create kubelet: get remote runtime typed version failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService</code>，删除 <code>containerd</code> 的默认配置文件 <code>/etc/containerd/config.toml</code> 后，重新启动 <sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Kubeadm unknown service runtime.v1alpha2.RuntimeService](https://github.com/containerd/containerd/issues/4581)">[10]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf /etc/containerd/config.toml</span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
<h2 id="修改-Service-可使用的-nodePort-端口范围"><a href="#修改-Service-可使用的-nodePort-端口范围" class="headerlink" title="修改 Service 可使用的 nodePort 端口范围"></a>修改 Service 可使用的 nodePort 端口范围</h2><p>默认情况下，<code>Service</code> 中可使用的 <code>nodePort</code> 端口的默认范围为 <code>30000-32767</code>，要修改此配置，参考以下步骤。</p>
<p>Master 节点上编辑 kube-apiserver 的 <code>Pod</code> 配置文件 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>，在 <code>.spec.containers.command</code> 下添加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- --service-node-port-range=1-65535</span><br></pre></td></tr></table></figure>
<p><img src="https://i.csms.tech/img_60.png"></p>
<p><code>apiserver</code> 是以静态 <code>Pod</code> 的形式运行，<code>/etc/kubernetes/manifests</code> 目录下是所有静态 <code>Pod</code> 文件的定义，<code>kubelet</code> 会监控该目录下文件的变动，只要发生变化，<code>Pod</code> 就会重建，响应相应的改动。所以我们修改 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> 文件，添加 <code>nodePort</code> 范围参数后会自动生效，无需进行其他操作</p>
<p><strong>高可用场景下，所有 Master 节点上都要修改，否则可能遇到部分时候依然报错： <code>nodePort: Invalid value: 65500: provided port is not in the valid range. The range of valid ports is 30000-32767</code></strong></p>
<h2 id="开启-corndns-日志记录"><a href="#开启-corndns-日志记录" class="headerlink" title="开启 corndns 日志记录"></a>开启 corndns 日志记录</h2><p>默认的 coredns 配置没有开启日志插件，这导致 kubernetes 集群中一些 dns 解析超时问题难以定位。要打开 coredns 的日志功能，可以通过以下命令开启日志功能</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl edit configmap -n kube-system  coredns</span><br></pre></td></tr></table></figure>
<p>添加以下配置：<br><img src="https://i.csms.tech/img_66.png"></p>
<p>接下来我们再使用命令查看日志，就可以看到 dns 解析的记录，无需重启 coredns</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs -f -n kube-system coredns-558bd4d5db-z6mst</span></span><br><span class="line">...</span><br><span class="line">[INFO] 10.244.2.23:39830 - 37988 &quot;A IN raw.githubusercontent.com.google.internal. udp 59 false 512&quot; NXDOMAIN qr,aa,rd,ra 164 0.000065843s</span><br><span class="line">[INFO] 10.244.2.23:56581 - 52144 &quot;A IN raw.githubusercontent.com. udp 43 false 512&quot; NOERROR qr,aa,rd,ra 207 0.000133489s</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="升级-cri-dockerd"><a href="#升级-cri-dockerd" class="headerlink" title="升级 cri-dockerd"></a>升级 cri-dockerd</h2><p>如果需要升级 <code>cri-dockerd</code> 版本，可以执行以下操作，如果之前的安装目录还在，则直接 <code>git pull</code> 更新代码，否则 clone 代码 <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[cri-dockerd 安装链接](https://github.com/Mirantis/cri-dockerd)">[11]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd cri-dockerd</span><br><span class="line">git pull</span><br><span class="line">make cri-dockerd</span><br><span class="line"></span><br><span class="line">install -o root -g root -m 0755 cri-dockerd /usr/local/bin/cri-dockerd</span><br><span class="line"></span><br><span class="line">install packaging/systemd/* /etc/systemd/system</span><br><span class="line">sed -i -e &#x27;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 iptables 替换 firewalld</span></span><br><span class="line">sed -i -e &#x27;s,firewalld.service,iptables.service,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable cri-docker.service --now</span><br><span class="line">systemctl enable --now cri-docker.socket</span><br></pre></td></tr></table></figure>

<h2 id="添加-Harbor-私有镜像仓库的认证信息"><a href="#添加-Harbor-私有镜像仓库的认证信息" class="headerlink" title="添加 Harbor 私有镜像仓库的认证信息"></a>添加 Harbor 私有镜像仓库的认证信息</h2><p>在命令行上提供凭证来创建 Secret <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[在命令行上提供凭证来创建 Secret](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line)">[3]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry $&#123;secretname&#125; \</span><br><span class="line">  --namespace default</span><br><span class="line">  --docker-server=https://index.docker.io/v1/ \</span><br><span class="line">  --docker-username=&lt;你的用户名&gt; \</span><br><span class="line">  --docker-password=&lt;你的密码&gt; \</span><br><span class="line">  --docker-email=&lt;你的邮箱地址&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>&lt;your-registry-server&gt;</code> 是你的私有 Docker 仓库全限定域名（FQDN）。 DockerHub 使用 <code>https://index.docker.io/v1/</code>。</li>
<li><code>&lt;your-name&gt;</code> 是你的 Docker 用户名。</li>
<li><code>&lt;your-pword&gt;</code> 是你的 Docker 密码。</li>
<li><code>&lt;your-email&gt;</code> 是你的 Docker 邮箱。</li>
</ul>
<p>这样你就成功地将集群中的 Docker 凭证设置为名为 <code>$&#123;secretname&#125;</code> 的 Secret。</p>
<p>Secret 属于 namespace 级别的资源，不能跨 namespace 使用。</p>
<p>检查创建的 Secret</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret <span class="variable">$&#123;secretname&#125;</span> --output=yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: eyJhdXRocfX19</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2022-11-04T08:55:51Z&quot;</span><br><span class="line">  name: $&#123;secretname&#125;</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;1679647&quot;</span><br><span class="line">  uid: a780ac6d-9525-4620-a171-b818021cc6ca</span><br><span class="line">type: kubernetes.io/dockerconfigjson</span><br></pre></td></tr></table></figure>

<p><code>.dockerconfigjson</code> 字段的值是 Docker 凭证的 base64 表示。要了解 <code>dockerconfigjson</code> 字段中的内容，请将 Secret 数据转换为可读格式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret <span class="variable">$&#123;secretname&#125;</span> --output=<span class="string">&quot;jsonpath=&#123;.data.\.dockerconfigjson&#125;&quot;</span> | <span class="built_in">base64</span> --decode</span></span><br><span class="line">&#123;&quot;auths&quot;:&#123;&quot;https://harbor1.my.com&quot;:&#123;&quot;username&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;password&quot;,&quot;email&quot;:&quot;docker@q.com&quot;,&quot;auth&quot;:&quot;YWRdsUQ==&quot;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="配置-Pod-拉取镜像的认证信息"><a href="#配置-Pod-拉取镜像的认证信息" class="headerlink" title="配置 Pod 拉取镜像的认证信息"></a>配置 Pod 拉取镜像的认证信息</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret -n frtg</span></span><br><span class="line">NAME                   TYPE                             DATA   AGE</span><br><span class="line">harbor1.1dergegh.com   kubernetes.io/dockerconfigjson   1      157d</span><br></pre></td></tr></table></figure>

<p>在 namespace 中配置了镜像仓库的 Secret 后，可以使用以下方法配置 Pod 拉取镜像时的认证信息</p>
<ol>
<li><p>在 Pod 的配置中使用 <code>imagePullSecrets</code> 指令，此种方式需要在每个 Pod 的配置中添加</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: $&#123;NAME&#125;</span><br><span class="line">  namespace: $&#123;NAMESPACE&#125;</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  containers:</span><br><span class="line">  - name: $&#123;NAME&#125;</span><br><span class="line">    image: nginx:1.14.2</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">      name: http-web</span><br><span class="line">      </span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: $&#123;secret_name&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>通过 ServiceAccount 配置</p>
<p> 每个 namespace 都有个默认的 <a href="/202305161451/" title="ServiceAccount">ServiceAccount</a>，namespace 中的所有 Pod 默认情况下都会关联到此 ServiceAccount，ServiceAccount 的配置中包含了 <code>Image pull secrets</code>，在 ServiceAccount 中添加的镜像拉取密钥，会自动添加到所有使用这个 ServiceAccount 的 Pod 中。因此，向 ServiceAccount 中添加镜像拉取密钥可以不必对每个 Pod 都单独进行镜像拉取密钥的配置。 <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[为服务账号添加 ImagePullSecrets](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)">[8]</span></a></sup></p>
<p> 执行命令 <code>kubectl edit serviceaccount default</code> 编辑默认的 ServiceAccount，删掉包含 <code>resourceVersion</code> 主键的行，添加包含 <code>imagePullSecrets:</code> 的行并保存文件</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2022-12-06T09:51:03Z&quot;</span><br><span class="line">  name: default</span><br><span class="line">  namespace: default</span><br><span class="line">  uid: b219bafc-e2f9-48bb-a9e4-6e0bfb4ab536</span><br><span class="line">imagePullSecrets:</span><br><span class="line">  - name: harbor1.1dergegh.com</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Pod-添加-hosts"><a href="#Pod-添加-hosts" class="headerlink" title="Pod 添加 hosts"></a>Pod 添加 hosts</h2><p>有时需要在启动 Pod 时为其 <code>/etc/hosts</code> 中添加解析，以覆盖对主机名的解析，此时可以通过 <code>PodSpec</code> 的 <code>HostAliases</code> 字段来 <strong>添加这些自定义条目</strong> <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[使用 HostAliases 向 Pod /etc/hosts 文件添加条目](https://kubernetes.io/zh-cn/docs/tasks/network/customize-hosts-file-for-pods/)">[4]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hostaliases-pod</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">  hostAliases:</span><br><span class="line">  - ip: &quot;127.0.0.1&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.local&quot;</span><br><span class="line">    - &quot;bar.local&quot;</span><br><span class="line">  - ip: &quot;10.1.2.3&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.remote&quot;</span><br><span class="line">    - &quot;bar.remote&quot;</span><br><span class="line">  containers:</span><br><span class="line">  - name: cat-hosts</span><br><span class="line">    image: busybox:1.28</span><br><span class="line">    command:</span><br><span class="line">    - cat</span><br><span class="line">    args:</span><br><span class="line">    - &quot;/etc/hosts&quot;</span><br></pre></td></tr></table></figure>

<h2 id="配置-Pod-中的时区和时间"><a href="#配置-Pod-中的时区和时间" class="headerlink" title="配置 Pod 中的时区和时间"></a>配置 Pod 中的时区和时间</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ssgeek/p/15192028.html">关于 OS 中的时间及时区的说明</a></p>
<p>通常情况下，我们的环境中，宿主机都是配置为 CST 时间（东八区），而使用的基础镜像中的默认时间都是 UTC 时间，而不是本地时间，通常需要确保系统中所有的时间格式一致，需将容器中的时间也修改为 CST 时间。</p>
<p>为此可使用以下方法中的一种来实现</p>
<ul>
<li><p>在 Dockerfile 中添加时区</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Set timezone</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span><br><span class="line">           &amp;&amp; echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone</span><br></pre></td></tr></table></figure></li>
<li><p>将时区文件挂载到 Pod 中</p>
<p>  在定义 Pod 上层控制器的时候，添加一个用于挂载时区的卷，挂载宿主机的时区文件</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">containers:</span><br><span class="line">- name: xxx</span><br><span class="line">  ...</span><br><span class="line">  volumeMounts:</span><br><span class="line">    - name: timezone</span><br><span class="line">      mountPath: /etc/localtime</span><br><span class="line">volumes:</span><br><span class="line">  - name: timezone</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /usr/share/zoneinfo/Asia/Shanghai</span><br></pre></td></tr></table></figure></li>
<li><p>通过环境变量定义时区</p>
<p>  在定义 Pod 上层控制器的时候，添加一个用于指定时区的环境变量</p>
<p>  <code>TZ</code> 环境变量用于设置时区。它由各种时间函数用于计算相对于全球标准时间 UTC（以前称为格林威治标准时间 GMT）的时间。格式由操作系统指定</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">containers:</span><br><span class="line">- name: xxx</span><br><span class="line">  ...</span><br><span class="line">  env:</span><br><span class="line">  - name: TZ</span><br><span class="line">    value: Asia/Shanghai</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="配置节点允许启动的最大-Pod-数"><a href="#配置节点允许启动的最大-Pod-数" class="headerlink" title="配置节点允许启动的最大 Pod 数"></a>配置节点允许启动的最大 Pod 数</h2><p>在 K8S 集群中，默认每个 Worker 节点最大可创建 110 个 Pod，实际可以根据节点资源情况调整范围。</p>
<p>在 Woker 节点上，可创建的最大的 Pod 数量是作为 Kubelet 的启动参数出现的，因此修改 Kubelet 服务的配置文件增加 <code>--max-pod</code> 参数即可。</p>
<p>修改 <code>kubelet</code> 服务的启动文件 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，添加以下环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;KUBELET_NODE_MAX_PODS=--max-pods=200&quot;</span><br></pre></td></tr></table></figure>
<p>将新加的环境变量追加到 <code>/usr/bin/kubelet</code> 中</p>
<figure class="highlight shell"><figcaption><span>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span></figcaption><table><tr><td class="code"><pre><span class="line"> Note: This dropin only works with kubeadm and kubelet v1.11+</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">Environment=&quot;KUBELET_EVICT_NODEFS_THRESHOLD_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;</span><br><span class="line">Environment=&quot;KUBELET_NODE_MAX_PODS=--max-pods=200&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that <span class="string">&quot;kubeadm init&quot;</span> and <span class="string">&quot;kubeadm join&quot;</span> generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span></span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that the user can use <span class="keyword">for</span> overrides of the kubelet args as a last resort. Preferably, the user should use</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">the .NodeRegistration.KubeletExtraArgs object <span class="keyword">in</span> the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span></span><br><span class="line">EnvironmentFile=-/etc/sysconfig/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICT_NODEFS_THRESHOLD_ARGS $KUBELET_NODE_MAX_PODS</span><br></pre></td></tr></table></figure>

<h2 id="重置集群"><a href="#重置集群" class="headerlink" title="重置集群"></a>重置集群</h2><p>要重置集群配置，可以参考以下步骤 <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[清理](https://v1-25.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down)
">[7]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm reset --cri-socket=unix:///var/run/cri-dockerd.sock</span></span><br><span class="line">[reset] Reading configuration from the cluster...</span><br><span class="line">[reset] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">W0428 15:31:57.928077   38272 reset.go:103] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: configmaps &quot;kubeadm-config&quot; not found</span><br><span class="line">W0428 15:31:57.928210   38272 preflight.go:55] [reset] WARNING: Changes made to this host by &#x27;kubeadm init&#x27; or &#x27;kubeadm join&#x27; will be reverted.</span><br><span class="line">[reset] Are you sure you want to proceed? [y/N]: y</span><br><span class="line">W0428 15:32:05.851830   38272 removeetcdmember.go:85] [reset] No kubeadm config, using etcd pod spec to get data directory</span><br><span class="line">[reset] Stopping the kubelet service</span><br><span class="line">[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;</span><br><span class="line">[reset] Deleting contents of directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]</span><br><span class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</span><br><span class="line">[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet]</span><br><span class="line"></span><br><span class="line">The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d</span><br><span class="line"></span><br><span class="line">The reset process does not reset or clean up iptables rules or IPVS tables.</span><br><span class="line">If you wish to reset iptables, you must do so manually by using the &quot;iptables&quot; command.</span><br><span class="line"></span><br><span class="line">If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)</span><br><span class="line">to reset your system&#x27;s IPVS tables.</span><br><span class="line"></span><br><span class="line">The reset process does not clean your kubeconfig files and you must remove them manually.</span><br><span class="line">Please, check the contents of the $HOME/.kube/config file.</span><br></pre></td></tr></table></figure>

<h2 id="部署-kube-state-metrics-组件"><a href="#部署-kube-state-metrics-组件" class="headerlink" title="部署 kube-state-metrics 组件"></a>部署 kube-state-metrics 组件</h2><p><code>kube-state-metrics</code> 是一个用于导出 Kubernetes 集群的资源状态指标的开源项目，通过监听 API Server 生成有关资源对象(如 <code>Deployment</code>、<code>Node</code>、<code>Pod</code>)的状态指标，需要注意的是 <code>kube-state-metrics</code> 只是简单的提供一个 <code>metrics</code> 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储，主要关注的是业务相关的一些元数据，比如 <code>Deployment</code>、<code>Pod</code>、<code>RepliSet</code> 的状态等指标。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/kubernetes/kube-state-metrics/releases">下载链接</a> 下载对应版本的部署文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">wget https://github.com/kubernetes/kube-state-metrics/archive/refs/tags/v2.6.0.tar.gz</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tar -xf v2.6.0.tar.gz</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> kube-state-metrics-2.6.0/examples/standard/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f cluster-role.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f cluster-role-binding.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f service-account.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f deployment.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f service.yaml</span></span><br></pre></td></tr></table></figure>
<p>参考以上步骤部署后，会生成 <code>kube-state-metrics</code> 所需要的资源：<code>ClusterRole</code>、<code>ClusterRoleBinding</code>、<code>ServiceAccount</code>、<code>Deployment</code>、<code>Service</code></p>
<p>部署成功后，查看生成的 <code>Service</code>，会看到是一个 <a href="https://csms.tech/202209241108/#无头服务（Headless-Services）">Headless 的服务</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -n kube-system</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">kube-dns             ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   180d</span><br><span class="line">kube-state-metrics   ClusterIP   None           &lt;none&gt;        8080/TCP,8081/TCP        16m</span><br><span class="line">metrics-server       ClusterIP   10.99.58.171   &lt;none&gt;        443/TCP                  10d</span><br></pre></td></tr></table></figure>

<p>登陆一个 Pod，尝试在 Pod 中访问 <code>kube-state-metrics</code> 提供 Metrics 的 url （<code>kube-state-metrics.kube-system.svc.cluster.local:8080/metrics</code>）。对应的 Service 的 FQDN 为 <code>kube-state-metrics.kube-system.svc.cluster.local</code>，查看其解析，根据 <code>Headless Service</code> 的原理，会解析成 <code>kube-state-metrics</code> 的 Pod 的 IP。正常情况下会获取到 <code>kube-state-metrics</code> 监控的指标数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ping kube-state-metrics.kube-system.svc.cluster.local</span></span><br><span class="line">PING kube-state-metrics.kube-system.svc.cluster.local (10.244.4.178) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10-244-4-178.kube-state-metrics.kube-system.svc.cluster.local (10.244.4.178): icmp_seq=1 ttl=62 time=0.160 ms</span><br><span class="line">64 bytes from 10-244-4-178.kube-state-metrics.kube-system.svc.cluster.local (10.244.4.178): icmp_seq=2 ttl=62 time=0.146 ms</span><br><span class="line">64 bytes from 10-244-4-178.kube-state-metrics.kube-system.svc.cluster.local (10.244.4.178): icmp_seq=3 ttl=62 time=0.143 ms</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl kube-state-metrics.kube-system.svc.cluster.local:8080/metrics</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_annotations gauge</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HELP kube_certificatesigningrequest_labels Kubernetes labels converted to Prometheus labels.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_labels gauge</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HELP kube_certificatesigningrequest_created Unix creation timestamp</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_created gauge</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HELP kube_certificatesigningrequest_condition The number of each certificatesigningrequest condition</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_condition gauge</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HELP kube_certificatesigningrequest_cert_length Length of the issued cert</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_cert_length gauge</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HELP kube_configmap_annotations Kubernetes annotations converted to Prometheus labels.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">TYPE kube_configmap_annotations gauge</span></span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;cattle-impersonation-system&quot;,configmap=&quot;kube-root-ca.crt&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;fleet-local&quot;,configmap=&quot;kube-root-ca.crt&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;telegram-stream&quot;,configmap=&quot;kube-root-ca.crt&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;cattle-system&quot;,configmap=&quot;admincreated&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;cattle-fleet-system&quot;,configmap=&quot;gitjob&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;cattle-system&quot;,configmap=&quot;forcesystemnamespaceassignment&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;kube-flannel&quot;,configmap=&quot;kube-flannel-cfg&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;p-76mvn&quot;,configmap=&quot;kube-root-ca.crt&quot;&#125; 1</span><br><span class="line">kube_configmap_annotations&#123;namespace=&quot;p-gqxm4&quot;,configmap=&quot;kube-root-ca.crt&quot;&#125; 1</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="集群证书过期后的处理步骤"><a href="#集群证书过期后的处理步骤" class="headerlink" title="集群证书过期后的处理步骤"></a>集群证书过期后的处理步骤</h2><p>假如集群 TLS 证书过期或者需要更新，可以参考以下步骤更新</p>
<ol>
<li><p>确定集群证书状态</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs check-expiration</span></span><br><span class="line">[check-expiration] Reading configuration from the cluster...</span><br><span class="line">[check-expiration] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration</span><br><span class="line"></span><br><span class="line">CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED</span><br><span class="line">admin.conf                 Sep 26, 2023 01:43 UTC   &lt;invalid&gt;                               no      </span><br><span class="line">apiserver                  Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">apiserver-etcd-client      Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">apiserver-kubelet-client   Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       ca                      no      </span><br><span class="line">controller-manager.conf    Sep 26, 2023 01:43 UTC   &lt;invalid&gt;                               no      </span><br><span class="line">etcd-healthcheck-client    Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">etcd-peer                  Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">etcd-server                Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       etcd-ca                 no      </span><br><span class="line">front-proxy-client         Sep 26, 2023 01:43 UTC   &lt;invalid&gt;       front-proxy-ca          no      </span><br><span class="line">scheduler.conf             Sep 26, 2023 01:43 UTC   &lt;invalid&gt;                               no      </span><br><span class="line"></span><br><span class="line">CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED</span><br><span class="line">ca                      Sep 23, 2032 01:43 UTC   8y              no      </span><br><span class="line">etcd-ca                 Sep 23, 2032 01:43 UTC   8y              no      </span><br><span class="line">front-proxy-ca          Sep 23, 2032 01:43 UTC   8y              no</span><br></pre></td></tr></table></figure>
</li>
<li><p>备份集群证书。<br> 备份 <code>/etc/kubernetes/</code> 目录，确保在任何出现问题的情况下都可以恢复</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo cp -r /etc/kubernetes /etc/kubernetes-backup</span><br></pre></td></tr></table></figure></li>
<li><p>在每个 Master 节点上更新证书</p>
<ul>
<li>更新单个证书。如果只想更新特定的证书，可以使用 <code>kubeadm alpha certs renew</code> 命令。例如，要更新 <code>apiserver</code> 的证书，可以执行： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo kubeadm certs renew apiserver</span><br></pre></td></tr></table></figure></li>
<li>更新所有证书 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubeadm certs renew all</span><br><span class="line">[renew] Reading configuration from the cluster...</span><br><span class="line">[renew] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">[renew] Error reading configuration from the Cluster. Falling back to default configuration</span><br><span class="line"></span><br><span class="line">certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed</span><br><span class="line">certificate for serving the Kubernetes API renewed</span><br><span class="line">certificate the apiserver uses to access etcd renewed</span><br><span class="line">certificate for the API server to connect to kubelet renewed</span><br><span class="line">certificate embedded in the kubeconfig file for the controller manager to use renewed</span><br><span class="line">certificate for liveness probes to healthcheck etcd renewed</span><br><span class="line">certificate for etcd nodes to communicate with each other renewed</span><br><span class="line">certificate for serving etcd renewed</span><br><span class="line">certificate for the front proxy client renewed</span><br><span class="line">certificate embedded in the kubeconfig file for the scheduler manager to use renewed</span><br><span class="line"></span><br><span class="line">Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>重启<strong>所有节点上的 <code>kubelet</code> 服务</strong>，以及 <code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>kube-apiserver</code></p>
</li>
<li><p>确定集群证书状态</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubeadm certs check-expiration</span></span><br><span class="line">[check-expiration] Reading configuration from the cluster...</span><br><span class="line">[check-expiration] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line"></span><br><span class="line">CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED</span><br><span class="line">admin.conf                 Sep 26, 2024 09:45 UTC   364d                                    no      </span><br><span class="line">apiserver                  Sep 26, 2024 09:45 UTC   364d            ca                      no      </span><br><span class="line">apiserver-etcd-client      Sep 26, 2024 09:45 UTC   364d            etcd-ca                 no      </span><br><span class="line">apiserver-kubelet-client   Sep 26, 2024 09:45 UTC   364d            ca                      no      </span><br><span class="line">controller-manager.conf    Sep 26, 2024 09:45 UTC   364d                                    no      </span><br><span class="line">etcd-healthcheck-client    Sep 26, 2024 09:45 UTC   364d            etcd-ca                 no      </span><br><span class="line">etcd-peer                  Sep 26, 2024 09:45 UTC   364d            etcd-ca                 no      </span><br><span class="line">etcd-server                Sep 26, 2024 09:45 UTC   364d            etcd-ca                 no      </span><br><span class="line">front-proxy-client         Sep 26, 2024 09:45 UTC   364d            front-proxy-ca          no      </span><br><span class="line">scheduler.conf             Sep 26, 2024 09:45 UTC   364d                                    no      </span><br><span class="line"></span><br><span class="line">CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED</span><br><span class="line">ca                      Sep 23, 2032 01:43 UTC   8y              no      </span><br><span class="line">etcd-ca                 Sep 23, 2032 01:43 UTC   8y              no      </span><br><span class="line">front-proxy-ca          Sep 23, 2032 01:43 UTC   8y              no</span><br></pre></td></tr></table></figure>
<blockquote>
<p><em><strong>如果集群证书更新后，Pod 内需要使用到集群证书，如果需要立即更新证书，需要重启节点上的 <code>kubelet</code> 服务，否则如果未重启节点上的 <code>kubelet</code> 服务，即使重启 Pod，Pod 使用的依旧是未更新的证书</strong></em></p>
</blockquote>
</li>
</ol>
<h2 id="基于公网的-Kubernetes-集群部署注意事项"><a href="#基于公网的-Kubernetes-集群部署注意事项" class="headerlink" title="基于公网的 Kubernetes 集群部署注意事项"></a>基于公网的 Kubernetes 集群部署注意事项</h2><p>以下部分列出在部署基于公网 IP（ <strong>如基于跨区域云主机，云主机上的网卡只有内网地址，未绑定公网 IP</strong> ）的集群时所需的注意事项， <strong>使用公网 IP 通信而非内网通信，会产生较高的延迟，可能会导致如 <code>etcd</code> 集群不稳定</strong> ，如非特殊场景，建议使用内网部署集群。</p>
<h3 id="etcd-集群使用公网-IP-通信"><a href="#etcd-集群使用公网-IP-通信" class="headerlink" title="etcd 集群使用公网 IP 通信"></a>etcd 集群使用公网 IP 通信</h3><p>要创建基于公网或者指定 IP 的 <code>etcd</code> 集群，可以使用以下配置 <strong>初始化 Kubernetes 集群</strong></p>
<figure class="highlight shell"><figcaption><span>k8s_kubeadm_init_config_template.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参考文档： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ , https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">未明确指定的配置，kubeadm 会使用默认值，默认值可使用命令 `kubeadm config <span class="built_in">print</span> init-defaults` 输出</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">  imagePullSerial: true</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">name: node</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">taints: null</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启用证书自动上传，证书密钥使用 `kubeadm init phase upload-certs --upload-certs` 生成</span></span><br><span class="line">certificateKey: &quot;&lt;你的证书密钥&gt;&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">apiServer: </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">apiserver 证书中的 SANs 列表</span></span><br><span class="line">  certSANs:</span><br><span class="line">    - &quot;apiserver.k8s.ops&quot;</span><br><span class="line">    - &quot;kubernetes&quot;</span><br><span class="line">    - &quot;kubernetes.default&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc&quot;</span><br><span class="line">    - &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    - &quot;&lt;master-1 主机名称&gt;&quot;</span><br><span class="line">    - &quot;&lt;master-2 主机名称&gt;&quot;  # 这里新增你的节点</span><br><span class="line"></span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">encryptionAlgorithm: RSA-2048</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增 Master 节点时，直接修改 `/etc/kubernetes/manifests/etcd.yaml`</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node name&gt;=https://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imageRepository: registry.k8s.io</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">proxy: &#123;&#125;</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controlPlaneEndpoint: apiserver.k8s.ops</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: JoinConfiguration</span><br><span class="line">nodeRegistration: </span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加为控制（Master）节点</span></span><br><span class="line">controlPlane: </span><br><span class="line">  localAPIEndpoint:</span><br><span class="line">    advertiseAddress: &lt;修改为 API Server 的监听 IP&gt;</span><br><span class="line">    bindPort: 6443</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">启用证书自动同步</span></span><br><span class="line">  certificateKey: &quot;&lt;你的证书密钥&gt;&quot;</span><br><span class="line"></span><br><span class="line">discovery:</span><br><span class="line">  bootstrapToken:</span><br><span class="line">    apiServerEndpoint: apiserver.k8s.ops:6443</span><br><span class="line">    token: &lt;你的 bootstrap token&gt;    # bootstrap token 和 CA Hash 使用命令 `kubeadm token create --print-join-command` 生成</span><br><span class="line">    caCertHashes:</span><br><span class="line">      - sha256:&lt;你的 CA 证书哈希&gt;</span><br></pre></td></tr></table></figure>
<p><strong>关键配置说明</strong> ：</p>
<ul>
<li><p><code>advertiseAddress</code> ： <code>kube-apiserver</code> 向其他节点通告的 IP， </p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: &lt;修改为 API Server 的公网 IP&gt;</span><br><span class="line">  bindPort: 6443</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>etcd</code> 配置，指定 <code>etcd</code> 节点的通告地址为 <strong>公网 IP</strong></p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta4</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.32.0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果需要自定义 etcd 监听地址，使用以下参数，如使用公网 IP，具体参数说明请参考 etcd 文档： https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增 Master 节点时，需直接修改 `/etc/kubernetes/manifests/etcd.yaml`</span></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">    - name: &quot;listen-peer-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2380&quot;</span><br><span class="line">    - name: &quot;listen-client-urls&quot;</span><br><span class="line">      value: &quot;https://0.0.0.0:2379&quot;</span><br><span class="line">    - name: &quot;advertise-client-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;公网 IP&gt;:2379&quot;</span><br><span class="line">    - name: &quot;initial-advertise-peer-urls&quot;</span><br><span class="line">      value: &quot;https://&lt;公网 IP&gt;:2380&quot;</span><br><span class="line">    - name: &quot;initial-cluster&quot;</span><br><span class="line">      value: &quot;&lt;master node name&gt;=https://&lt;公网 IP&gt;:2380&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="flanneld-Tunnel-IP-配置"><a href="#flanneld-Tunnel-IP-配置" class="headerlink" title="flanneld Tunnel IP 配置"></a>flanneld Tunnel IP 配置</h2><p>默认情况下，<code>flanneld</code> 使用 <code>kube subnet manager</code> 模式，即使用 Kubernetes API 作为其后端存储，获取节点以及其 Pod Subnet 信息。 <a href="https://csms.tech/202212020941/#Kubernetes-中-flannel-相关配置">Kubernetes 中 flannel 相关配置</a></p>
<p><strong><code>flanneld</code> 会从 Node 的 Annotations 中读取节点的 Tunnel IP 信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl describe node k8s-master-1</span></span><br><span class="line">Name:               k8s-master-1</span><br><span class="line">Roles:              control-plane</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=k8s-master-1</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">                    node-role.kubernetes.io/control-plane=</span><br><span class="line">Annotations:        flannel.alpha.coreos.com/backend-data: &#123;&quot;VNI&quot;:1,&quot;VtepMAC&quot;:&quot;32:11:b3:bd:d5:78&quot;&#125;</span><br><span class="line">                    flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">                    flannel.alpha.coreos.com/kube-subnet-manager: true</span><br><span class="line">                    flannel.alpha.coreos.com/public-ip: &lt;k8s-master-1 IP&gt;</span><br><span class="line">                    # flannel.alpha.coreos.com/node-public-ip=&quot;&lt;节点的公共 IPv4 地址&gt;&quot;</span><br><span class="line">                    # flannel.alpha.coreos.com/node-public-ipv6=&quot;&lt;节点的公共 IPv6 地址&gt;&quot;</span><br><span class="line">                    flannel.alpha.coreos.com/public-ip-overwrite: &lt;k8s-master-1 Public IP&gt;</span><br><span class="line">                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">                    node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键 Annotations（注释信息）包括:</p>
<ul>
<li><code>flannel.alpha.coreos.com/backend-type</code> : <code>Flanneld</code> 使用的后端（Backend）网络技术类型，默认为 <code>vxlan</code></li>
<li><code>flannel.alpha.coreos.com/kube-subnet-manager</code> ： 是否使用 Kubernetes API 作为 Flanneld 的后端存储，默认为 <code>True</code> ，即使用 <strong>kube subnet manager</strong></li>
<li><code>flannel.alpha.coreos.com/public-ip</code> ： 默认检测到的节点 <strong>出口 IP （如云服务器，即为其内网 IP），默认使用此 IP 作为本 Node 的 Tunnel 端 IP</strong></li>
<li><code>flannel.alpha.coreos.com/public-ip-overwrite</code> : <strong>当 Flanneld 使用的 IP（即 Tunnel 端 IP）没有绑定在本机网卡，典型例子为云主机场景（使用 NAT 实现公私网地址转换），Flanneld 需要使用公网地址通信时，可以使用此 Annotation 强制指定 Flannel Tunnel 的本端 IP 地址</strong></li>
<li><code>flannel.alpha.coreos.com/node-public-ip</code> ： <strong>如果节点的 Tunnel IP 已经配置在节点的某个网络接口上（例如节点有多个接口）时使用</strong></li>
</ul>
<p>在基于公网 IP 的集群中，默认情况下，云主机只绑定了内网 IP, <code>flanneld</code> 将使用节点的内网 IP 作为其 Tunnel 地址和其他节点通信，因内网地址和其他节点的地址不互通，会导致节点上的 Pod 之间无法通信。 <strong>为了使跨节点的 Pod 可以正常通信，需要使 <code>flanneld</code> 使用节点的公网 IP 作为 Tunnel 端 IP</strong></p>
<p>在此场景下， <strong>需要修改各个 Node 的配置，强制指定 <code>flanneld</code> 使用公网 IP 作为 Tunnel 端地址</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl edit node k8s-master-1</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VNI&quot;:1,&quot;VtepMAC&quot;:&quot;56:4d:28:fa:f3:ce&quot;&#125;&#x27;</span><br><span class="line">    flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">    flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">    flannel.alpha.coreos.com/public-ip: &lt;k8s-master-1 Public IP&gt;</span><br><span class="line">    flannel.alpha.coreos.com/public-ip-overwrite: &lt;k8s-master-1 Public IP&gt;</span><br><span class="line">    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">    node.alpha.kubernetes.io/ttl: &quot;0&quot;</span><br><span class="line">    volumes.kubernetes.io/controller-managed-attach-detach: &quot;true&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="kubelet-配置"><a href="#kubelet-配置" class="headerlink" title="kubelet 配置"></a>kubelet 配置</h2><p>默认情况下，<code>kubelet</code> 将使用本机绑定的 IP 向集群上报（注册）本节点的信息，在云主机的场景下，其通常为内网 IP 。 <strong><code>kubelet</code> 要求 <code>--node-ip</code> （上报&#x2F;注册的节点 IP） 必须存在于主机的网络接口中，否则会拒绝启动或更新节点状态。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME            STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                            KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">k8s-master-1    Ready    control-plane   3d16h   v1.32.3   111.111.112.35   &lt;none&gt;        Ubuntu 24.04.2 LTS                  6.11.0-19-generic            containerd://1.7.25</span><br><span class="line">k8s-master-2    Ready    control-plane   5d19h   v1.32.2   172.31.27.52     &lt;none&gt;        Rocky Linux 8.10 (Green Obsidian)   6.13.5-1.el8.elrepo.x86_64   containerd://1.6.32</span><br><span class="line">k8s-master-3    Ready    control-plane   4d21h   v1.32.3   172.31.36.21     &lt;none&gt;        CentOS Linux 7 (Core)               5.15.80-200.el7.x86_64       containerd://2.0.0</span><br></pre></td></tr></table></figure>

<p><strong>如果使用 <code>kubelet --node-ip</code> 配置公网 IP，而公网 IP 未在本地网卡绑定，<code>kubelet</code> 服务会报错</strong> ：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">journalctl -u kubelet | grep -i <span class="string">&quot;Node IP&quot;</span></span></span><br><span class="line">&quot;Failed to set some node status fields&quot; err=&quot;failed to validate nodeIP: node IP: \&quot;&lt;k8s-master-1 Public IP&gt;\&quot; not found in the host&#x27;s network interfaces&quot; node=&quot;k8s-master-1&quot;</span><br></pre></td></tr></table></figure>

<p>在此场景下，如果要在 Master 节点上连接到其他节点上的 Pod 或者查看其日志信息会报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pods -A -o wide</span></span><br><span class="line">NAMESPACE              NAME              READY   STATUS    RESTARTS  IP               NODE</span><br><span class="line">kube-system            netshoot-8672j    1/1     Running   0 17h     192.168.2.8      k8s-master-1</span><br><span class="line">kube-system            netshoot-gw5rt    1/1     Running   0 17h     192.168.1.4      k8s-master-2</span><br><span class="line">kube-system            netshoot-wflkr    1/1     Running   0 17h     192.168.0.8      k8s-master-3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it -n kube-system netshoot-8672j -- sh</span></span><br><span class="line">error: Internal error occurred: error sending request: Post &quot;https://172.31.36.21:10250/exec/kube-system/netshoot-8672j/netshoot?command=sh&amp;input=1&amp;output=1&amp;tty=1&quot;: dial tcp 172.31.36.21:10250: i/o timeout</span><br></pre></td></tr></table></figure>

<p><strong>此问题原因</strong> ： </p>
<ul>
<li><strong>节点注册的 IP 不可达</strong> ： <code>kubelet</code> 默认使用内网 IP 注册到 Kubernetes API Server。</li>
<li><strong><code>kube-apiserver</code> 的 IP 选择策略</strong> ： 默认优先使用 InternalIP，而非公网 IP（EIP）。</li>
</ul>
<p><strong>解决方案如下</strong> ：</p>
<ol>
<li><p><strong>在节点上绑定公网 IP</strong> 。因为 <code>kubelet</code> 会验证注册的 IP 地址是否绑定在本地网卡上，而云主机场景下的公网 IP 并未绑定到网卡，如果配置 <code>kubelet</code> 注册的 IP 为公网 IP，<code>kubelet</code> 会报错并继续使用内网 IP 注册，参考以下命令将公网 IP 绑定到本地网卡</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ip addr add 54.219.215.136/32 dev eth0</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>ip addr add</code> 只是临时添加 IP，重启服务器后会丢失。如果想永久生效，需要写入网络配置文件</p>
</blockquote>
</li>
<li><p><strong>配置 kube-apiserver 优先使用 ExternalIP</strong></p>
<p> <strong>修改控制平面节点的 <code>kube-apiserver</code> 启动参数，使其优先选择 ExternalIP</strong></p>
<ol>
<li>编辑 <code>kube-apiserver</code> 的 Manifest 文件（ <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> ），添加或修改 <code>--kubelet-preferred-address-types</code> 参数 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-apiserver</span><br><span class="line">    - --kubelet-preferred-address-types=ExternalIP,InternalIP,Hostname  # 将 ExternalIP 放在最前</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<strong>kube-apiserver 会自动重启（无需手动操作）</strong> 。</li>
</ol>
</li>
<li><p><strong>检查节点注册信息</strong></p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME            STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                            KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">k8s-master-1    Ready    control-plane   3d16h   v1.32.3   111.111.112.35   &lt;none&gt;        Ubuntu 24.04.2 LTS                  6.11.0-19-generic            containerd://1.7.25</span><br><span class="line">k8s-master-2    Ready    control-plane   5d19h   v1.32.2   111.111.112.36   &lt;none&gt;        Rocky Linux 8.10 (Green Obsidian)   6.13.5-1.el8.elrepo.x86_64   containerd://1.6.32</span><br><span class="line">k8s-master-3    Ready    control-plane   4d21h   v1.32.3   111.111.112.37   &lt;none&gt;        CentOS Linux 7 (Core)               5.15.80-200.el7.x86_64       containerd://2.0.0</span><br></pre></td></tr></table></figure>
<p>如果节点注册的公网 IP 不对，可修改对应节点的 <code>kubelet</code> 配置（<code>/var/lib/kubelet/config.yaml</code>），指定注册的公网 IP</p>
 <figure class="highlight shell"><figcaption><span>/var/lib/kubelet/config.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">nodeIP: 54.219.215.136</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/">Kubernetes 官网文档</a><br><a target="_blank" rel="noopener" href="https://github.com/Mirantis/cri-dockerd">cri-dockerd 安装链接</a><br><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844904148001882120">Centos7 集群部署k8s 版本v1.17.4及Dashboard </a><br><a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/tree/master/docs">kubernetes-dashboard 配置官网说明</a><br><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/">部署和访问 Kubernetes 仪表板（Dashboard）</a></p>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/#configure-the-kubelet-to-use-cri-dockerd">配置 kubelet 使用 cri-dockerd</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/#%E4%B8%BA-kube-apiserver-%E5%88%9B%E5%BB%BA%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8">为 kube-apiserver 创建负载均衡器</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line">在命令行上提供凭证来创建 Secret</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/network/customize-hosts-file-for-pods/">使用 HostAliases 向 Pod /etc/hosts 文件添加条目</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">Kubernetes Metrics Server</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://v1-25.docs.kubernetes.io/zh-cn/docs/reference/networking/ports-and-protocols/">端口和协议</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://v1-25.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down">清理</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">为服务账号添加 ImagePullSecrets</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/#%E9%85%8D%E7%BD%AE-kubelet-%E4%BD%BF%E7%94%A8-containerd-%E4%BD%9C%E4%B8%BA%E5%85%B6%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6">配置 kubelet 使用 containerd 作为其容器运行时</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/issues/4581">Kubeadm unknown service runtime.v1alpha2.RuntimeService</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/Mirantis/cri-dockerd">cri-dockerd 安装链接</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#manual-certs">Manual certificate distribution</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/architecture/cgroups/">About cgroup v2</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/">User Namespaces</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Deploy and Access the Kubernetes Dashboard</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="../tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i> Kubernetes</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="../202209051137/" rel="prev" title="docker compose 配置 django mysql 站点">
                  <i class="fa fa-chevron-left"></i> docker compose 配置 django mysql 站点
                </a>
            </div>
            <div class="post-nav-item">
                <a href="../202209131536/" rel="next" title="kubernetes 常用命令示例">
                  kubernetes 常用命令示例 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COSMOS</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="../js/comments.js"></script><script src="../js/utils.js"></script><script src="../js/motion.js"></script><script src="../js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="../js/third-party/search/local-search.js"></script>




  <script src="../js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fl9999","repo":"fl9999.github.io","client_id":"a11bf6f7860762b725b5","client_secret":"a99046105f8bddc72ec718d54dc3fd7f22070821","admin_user":"fl9999","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d41d8cd98f00b204e9800998ecf8427e"}</script>
<script src="../js/third-party/comments/gitalk.js"></script>

</body>
</html>
