<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16-next.png">
  <link rel="mask-icon" href="../images/logo.svg" color="#222">

<link rel="stylesheet" href="../css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"csms.tech","root":"/","images":"../images","scheme":"Gemini","darkmode":true,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeIn","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"../search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="../js/config.js"></script>

    <meta name="description" content="Kubernetes 官网文档 环境信息 Centos 7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.24.7 kubeadm-1.24.7 kubelet-1.24.7  kubernetes 环境安装前配置升级内核版本Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 安装配置">
<meta property="og:url" content="http://csms.tech/202209121102/index.html">
<meta property="og:site_name" content="L B T">
<meta property="og:description" content="Kubernetes 官网文档 环境信息 Centos 7 5.4.212-1 Docker 20.10.18 containerd.io-1.6.8 kubectl-1.24.7 kubeadm-1.24.7 kubelet-1.24.7  kubernetes 环境安装前配置升级内核版本Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.csms.tech/img_77.png">
<meta property="og:image" content="https://i.csms.tech/img_56.png">
<meta property="og:image" content="https://i.csms.tech/img_57.png">
<meta property="og:image" content="https://i.csms.tech/img_58.png">
<meta property="og:image" content="https://i.csms.tech/img_59.png">
<meta property="og:image" content="https://i.csms.tech/img_60.png">
<meta property="og:image" content="https://i.csms.tech/img_66.png">
<meta property="article:published_time" content="2022-09-12T03:03:09.000Z">
<meta property="article:modified_time" content="2023-05-16T07:52:11.000Z">
<meta property="article:author" content="COSMOS">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.csms.tech/img_77.png">


<link rel="canonical" href="http://csms.tech/202209121102/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://csms.tech/202209121102/","path":"202209121102/","title":"kubernetes 安装配置"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kubernetes 安装配置 | L B T</title>
  








  <noscript>
    <link rel="stylesheet" href="../css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">L B T</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记 录 过 去 的 经 验</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="../index.html" rel="section"><i class="fa fa-earth-americas fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="../categories/" rel="section"><i class="fa fa-folder-tree fa-fw"></i>总目录<span class="badge">34</span></a></li><li class="menu-item menu-item-linux"><a href="../categories/Linux" rel="section"><i class="fa fa-brands fa-linux fa-fw"></i>Linux</a></li><li class="menu-item menu-item-python"><a href="../categories/Python" rel="section"><i class="fa fa-brands fa-python fa-fw"></i>Python</a></li><li class="menu-item menu-item-docker"><a href="../categories/Docker" rel="section"><i class="fa fa-brands fa-docker fa-fw"></i>Docker</a></li><li class="menu-item menu-item-kubernetes"><a href="../categories/Kubernetes" rel="section"><i class="fa fa-dharmachakra fa-fw"></i>Kubernetes</a></li><li class="menu-item menu-item-tags"><a href="../tags/" rel="section"><i class="fa fa-tornado fa-fw"></i>标签<span class="badge">71</span></a></li><li class="menu-item menu-item-archives"><a href="../archives/" rel="section"><i class="fa fa-rectangle-list fa-fw"></i>列表<span class="badge">151</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章总目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">1.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubernetes-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%89%8D%E9%85%8D%E7%BD%AE"><span class="nav-number">2.</span> <span class="nav-text">kubernetes 环境安装前配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC"><span class="nav-number">2.1.</span> <span class="nav-text">升级内核版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E9%97%AD-SELinux"><span class="nav-number">2.2.</span> <span class="nav-text">关闭 SELinux</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%89%80%E6%9C%89%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8B%E9%97%B4%E5%85%B7%E6%9C%89%E5%AE%8C%E5%85%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5"><span class="nav-number">2.3.</span> <span class="nav-text">集群中所有计算机之间具有完全的网络连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A6%81%E6%AD%A2swap%E5%88%86%E5%8C%BA"><span class="nav-number">2.4.</span> <span class="nav-text">禁止swap分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D"><span class="nav-number">2.5.</span> <span class="nav-text">配置主机名</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA-kube-proxy-%E5%BC%80%E5%90%AF-ipvs"><span class="nav-number">2.6.</span> <span class="nav-text">为 kube-proxy 开启 ipvs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E5%8F%91-IPv4-%E5%B9%B6%E8%AE%A9-iptables-%E7%9C%8B%E5%88%B0%E6%A1%A5%E6%8E%A5%E6%B5%81%E9%87%8F"><span class="nav-number">2.7.</span> <span class="nav-text">转发 IPv4 并让 iptables 看到桥接流量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Docker-Engine"><span class="nav-number">2.8.</span> <span class="nav-text">安装 Docker Engine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-cri-dockerd"><span class="nav-number">2.9.</span> <span class="nav-text">安装 cri-dockerd</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE"><span class="nav-number">3.</span> <span class="nav-text">Kubernetes 安装配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-kubeadm%E3%80%81kubelet-%E5%92%8C-kubectl"><span class="nav-number">3.1.</span> <span class="nav-text">安装 kubeadm、kubelet 和 kubectl</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%8A%82%E7%82%B9"><span class="nav-number">3.2.</span> <span class="nav-text">初始化控制平面节点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%8D%95%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E9%9B%86%E7%BE%A4"><span class="nav-number">3.2.1.</span> <span class="nav-text">创建单控制平面集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E7%9A%84%E9%9B%86%E7%BE%A4"><span class="nav-number">3.2.2.</span> <span class="nav-text">创建高可用控制平面的集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-kube-flannel"><span class="nav-number">3.2.3.</span> <span class="nav-text">安装 kube-flannel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-dashboard"><span class="nav-number">4.</span> <span class="nav-text">安装 dashboard</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Kubernetes-Metrics-Server"><span class="nav-number">5.</span> <span class="nav-text">安装 Kubernetes Metrics Server</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF"><span class="nav-number">6.</span> <span class="nav-text">常见错误</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Found-multiple-CRI-endpoints-on-the-host"><span class="nav-number">6.1.</span> <span class="nav-text">Found multiple CRI endpoints on the host</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kube-flannel-%E7%8A%B6%E6%80%81%E4%B8%BA-CrashLoopBackOff"><span class="nav-number">6.2.</span> <span class="nav-text">kube-flannel 状态为 CrashLoopBackOff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master-%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81%E4%B8%BA-NotReady"><span class="nav-number">6.3.</span> <span class="nav-text">master 节点状态为 NotReady</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B9%8B%E5%A4%96%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8-kubectl-%E6%8A%A5%E9%94%99"><span class="nav-number">6.4.</span> <span class="nav-text">集群之外的服务器使用 kubectl 报错</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-number">6.5.</span> <span class="nav-text">kubelet 启动失败</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">7.</span> <span class="nav-text">其他常用配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">新增节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%86-CRI-%E7%94%B1-containerd-%E5%8F%98%E6%9B%B4%E4%B8%BA-Docker"><span class="nav-number">7.2.</span> <span class="nav-text">将 CRI 由 containerd 变更为 Docker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9-Service-%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%9A%84-nodePort-%E7%AB%AF%E5%8F%A3%E8%8C%83%E5%9B%B4"><span class="nav-number">7.3.</span> <span class="nav-text">修改 Service 可使用的 nodePort 端口范围</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%80%E5%90%AF-corndns-%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95"><span class="nav-number">7.4.</span> <span class="nav-text">开启 corndns 日志记录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0-Harbor-%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E7%9A%84%E8%AE%A4%E8%AF%81%E4%BF%A1%E6%81%AF"><span class="nav-number">7.5.</span> <span class="nav-text">添加 Harbor 私有镜像仓库的认证信息</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Pod-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E7%9A%84%E8%AE%A4%E8%AF%81%E4%BF%A1%E6%81%AF"><span class="nav-number">7.5.1.</span> <span class="nav-text">配置 Pod 拉取镜像的认证信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod-%E6%B7%BB%E5%8A%A0-hosts"><span class="nav-number">7.6.</span> <span class="nav-text">Pod 添加 hosts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Pod-%E4%B8%AD%E7%9A%84%E6%97%B6%E5%8C%BA%E5%92%8C%E6%97%B6%E9%97%B4"><span class="nav-number">7.7.</span> <span class="nav-text">配置 Pod 中的时区和时间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E8%8A%82%E7%82%B9%E5%85%81%E8%AE%B8%E5%90%AF%E5%8A%A8%E7%9A%84%E6%9C%80%E5%A4%A7-Pod-%E6%95%B0"><span class="nav-number">7.8.</span> <span class="nav-text">配置节点允许启动的最大 Pod 数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4"><span class="nav-number">7.9.</span> <span class="nav-text">重置集群</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">8.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%9A%E6%B3%A8"><span class="nav-number">9.</span> <span class="nav-text">脚注</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">COSMOS</p>
  <div class="site-description" itemprop="description">得 能 莫 忘</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="../archives/">
          <span class="site-state-item-count">151</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="../categories/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="../tags/">
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://csms.tech/202209121102/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="../images/avatar.gif">
      <meta itemprop="name" content="COSMOS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L B T">
      <meta itemprop="description" content="得 能 莫 忘">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kubernetes 安装配置 | L B T">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kubernetes 安装配置
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-12 11:03:09" itemprop="dateCreated datePublished" datetime="2022-09-12T11:03:09+08:00">2022-09-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-16 15:52:11" itemprop="dateModified" datetime="2023-05-16T15:52:11+08:00">2023-05-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">上层目录</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="../categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/">Kubernetes 官网文档</a></p>
<h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><ul>
<li>Centos 7 5.4.212-1</li>
<li>Docker 20.10.18</li>
<li>containerd.io-1.6.8</li>
<li>kubectl-1.24.7</li>
<li>kubeadm-1.24.7</li>
<li>kubelet-1.24.7</li>
</ul>
<h1 id="kubernetes-环境安装前配置"><a href="#kubernetes-环境安装前配置" class="headerlink" title="kubernetes 环境安装前配置"></a>kubernetes 环境安装前配置</h1><h2 id="升级内核版本"><a href="#升级内核版本" class="headerlink" title="升级内核版本"></a>升级内核版本</h2><p>Centos 7 默认的内核版本 3.10 在运行 kubernetes 时存在不稳定性，建议升级内核版本到新版本</p>
<a href="/202209140931/" title="Centos 7 升级内核">Centos 7 升级内核</a>

<h2 id="关闭-SELinux"><a href="#关闭-SELinux" class="headerlink" title="关闭 SELinux"></a>关闭 SELinux</h2><p>kubernetes 目前未实现对 SELinux 的支持，因此必须要关闭 SELinux</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo setenforce 0</span><br><span class="line">sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=disabled/&#x27; /etc/selinux/config</span><br></pre></td></tr></table></figure>

<h2 id="集群中所有计算机之间具有完全的网络连接"><a href="#集群中所有计算机之间具有完全的网络连接" class="headerlink" title="集群中所有计算机之间具有完全的网络连接"></a>集群中所有计算机之间具有完全的网络连接</h2><p>配置集群所有节点的防火墙，确保所有集群节点之间具有完全的网络连接。</p>
<ul>
<li>放通节点之间的通信</li>
<li>确保防火墙允许 <code>FORWARD</code> 链的流量<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [4:368]</span><br><span class="line"></span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes nodes</span></span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.5.58 -j ACCEPT</span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.5.68 -j ACCEPT</span><br><span class="line">-A INPUT -m comment --comment &quot;kubernetes nodes&quot; -s 172.31.0.230 -j ACCEPT</span><br><span class="line"></span><br><span class="line">-A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT -m comment --comment &quot;k8s ingress http,https&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">-A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT</span><br><span class="line">-A INPUT -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure></li>
</ul>
<span id="more"></span>

<p>集群通信 ( iptables ) 矩阵说明： <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[端口和协议](https://v1-25.docs.kubernetes.io/zh-cn/docs/reference/networking/ports-and-protocols/)">[6]</span></a></sup></p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>iptables table</th>
<th>ipables chain</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used by</th>
</tr>
</thead>
<tbody><tr>
<td>UDP</td>
<td>filter</td>
<td>INPUT</td>
<td>8472</td>
<td>flannel</td>
<td>network</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>6443</td>
<td>Kubernetes API server</td>
<td>ALL node</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver,etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10250</td>
<td>kubelet API</td>
<td>Control plane, Self ,kubectl exec</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>self</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>self</td>
</tr>
<tr>
<td>TCP</td>
<td>filter</td>
<td>INPUT</td>
<td>30000-32767</td>
<td>NodePortService</td>
<td>All</td>
</tr>
</tbody></table>
<ul>
<li>不同节点之间的 <code>Pod</code> 通信需要经过 <code>flannel</code> 的 <code>8472/udp</code> </li>
<li><code>nodePort</code> 类型的 <code>service</code> ，默认可用的 <code>nodePort</code> 端口范围为 <code>30000-32767</code>，根据实际情况配置</li>
</ul>
<p>若对网络安全要求较为严格，可在 master 节点使用以下防火墙规则，本示例中 CNI 对接的网络插件为 flannel，若使用其他网络插件，则根据插件要求放通对应端口。</p>
<p>本示例中 <code>192.168.142.8 - 10</code> 为 master 节点，<code>192.168.142.11 - 12</code> 为 worker 节点</p>
<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes master</span></span><br><span class="line">-A INPUT -s 192.168.142.8 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;kubernetes master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.9 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;kubernetes master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.10 -p tcp -m multiport --dports 6443,2379:2380,10250,10259,10257 -j ACCEPT -m comment --comment &quot;kubernetes master&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.11 -p tcp -m multiport --dports 6443,2379:2380,10250 -j ACCEPT -m comment --comment &quot;kubernetes master for k8s node&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p tcp -m multiport --dports 6443,2379:2380,10250 -j ACCEPT -m comment --comment &quot;kubernetes master for k8s node&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.8 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.11 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes node&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes node&quot;</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure>
<p>在 node 节点使用以下防火墙规则</p>
<figure class="highlight shell"><figcaption><span>/etc/sysconfig/iptables</span></figcaption><table><tr><td class="code"><pre><span class="line">*filter</span><br><span class="line">:INPUT DROP [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubernetes node</span></span><br><span class="line">-A INPUT -s 192.168.142.8 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;kubernetes node for master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;kubernetes node for master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p tcp -m multiport --dports 10250 -j ACCEPT -m comment --comment &quot;kubernetes node for master&quot;</span><br><span class="line"></span><br><span class="line">-A INPUT -s 192.168.142.8 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.9 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.10 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes master&quot;</span><br><span class="line">-A INPUT -s 192.168.142.11 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes node&quot;</span><br><span class="line">-A INPUT -s 192.168.142.12 -p udp --dport 8472 -j ACCEPT -m comment --comment &quot;flannel for kubernetes node&quot;</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure>

<h2 id="禁止swap分区"><a href="#禁止swap分区" class="headerlink" title="禁止swap分区"></a>禁止swap分区</h2><p>以下命令临时关闭 swap，要永久关闭 swap，修改配置文件 <code>/etc/fstab</code>，删除或注释其中 <code>swap</code> 相关的行。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>

<h2 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h2><p>节点之中不可以有重复的主机名、MAC 地址或 product_uuid</p>
<p>配置集群中的 3 台主机名分别为 <code>kubernetes1</code>，<code>kubernetes2</code>，<code>kubernetes3</code>，本示例中 <code>kubernetes1</code> 作为 master </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname kubernetes1</span><br></pre></td></tr></table></figure>

<p>添加主机名和 ip 解析到 <code>/etc/hosts</code> 文件</p>
<figure class="highlight shell"><figcaption><span>/etc/hosts</span></figcaption><table><tr><td class="code"><pre><span class="line">172.31.10.19 kubernetes1</span><br><span class="line">172.31.9.241 kubernetes2</span><br><span class="line">172.31.14.115 kubernetes3</span><br></pre></td></tr></table></figure>

<h2 id="为-kube-proxy-开启-ipvs"><a href="#为-kube-proxy-开启-ipvs" class="headerlink" title="为 kube-proxy 开启 ipvs"></a>为 kube-proxy 开启 ipvs</h2><p><a target="_blank" rel="noopener" href="https://blog.fleeto.us/post/iptables-or-ipvs/">kube-proxy 模式对比：iptables 还是 IPVS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1890887">kube-proxy中使用ipvs与iptables的比较</a></p>
<p>此配置为<strong>可选操作</strong>，在不启用 ipvs 模式的情况下，kube-proxy 会使用 iptables 模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules </span><br></pre></td></tr></table></figure>

<h2 id="转发-IPv4-并让-iptables-看到桥接流量"><a href="#转发-IPv4-并让-iptables-看到桥接流量" class="headerlink" title="转发 IPv4 并让 iptables 看到桥接流量"></a>转发 IPv4 并让 iptables 看到桥接流量</h2><p>以下操作需要在 kubernetes 集群中的所有节点操作  </p>
<p>通过运行 <code>lsmod | grep br_netfilter</code> 来验证 <code>br_netfilter</code> 模块是否已加载。Kubernetes 通过 <code>bridge-netfilter</code> 配置使 iptables 规则可以应用在 Linux Bridge 上，对 Linux 内核进行宿主机和容器之间的数据包的地址转换是必须的，否则 Pod 进行外部服务网络请求时会出现目标主机不可达或者连接拒绝等错误（host unreachable 或者 connection refused）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">lsmod | grep br_netfilter</span></span><br><span class="line">br_netfilter           22256  0 </span><br><span class="line">bridge                151336  1 br_netfilter</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>若要显式加载此模块，请运行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo modprobe overlay</span><br><span class="line">sudo modprobe br_netfilter</span><br></pre></td></tr></table></figure>


<p>为了让 Linux 节点的 iptables 能够正确查看桥接流量，请确认 <code>sysctl</code> 配置中的 <code>net.bridge.bridge-nf-call-iptables</code> 设置为 1</p>
<p>为配置永久生效，可以添加以下配置，<code>/etc/modules-load.d/k8s.conf</code> 中追加要加载的模块</p>
<figure class="highlight shell"><figcaption><span>/etc/modules-load.d/k8s.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">overlay</span><br><span class="line">br_netfilter</span><br></pre></td></tr></table></figure>

<p><code>/etc/sysctl.d/k8s.conf</code> 中追加内核参数</p>
<figure class="highlight shell"><figcaption><span>/etc/sysctl.d/k8s.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br></pre></td></tr></table></figure>

<p>执行以下命令重新载入 sysctl 参数而无需重启系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sysctl --system</span><br></pre></td></tr></table></figure>

<!-- more -->

<h2 id="安装-Docker-Engine"><a href="#安装-Docker-Engine" class="headerlink" title="安装 Docker Engine"></a>安装 Docker Engine</h2><p>以下操作需要在 kubernetes 集群中的所有节点操作<br>参考以下链接，在每个节点上安装 Docker Engine<br><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/centos/">Centos 安装 Docker Engine 官网参考文档</a></p>
<a href="/202208041317/" title="docker 安装及常用命令介绍">docker 安装及常用命令介绍</a>

<h2 id="安装-cri-dockerd"><a href="#安装-cri-dockerd" class="headerlink" title="安装 cri-dockerd"></a>安装 cri-dockerd</h2><p>Docker Engine 没有实现 CRI，因此 Kubernetes 无法直接使用 Docker Engine，需要先安装 cri-dockerd，以让 Kubernetes 可以通过 Kubernetes 的 CRI 操作 Docker。</p>
<p>以下操作需要在 kubernetes 集群中的所有节点操作 </p>
<p>按照源代码仓库中的说明安装 <a target="_blank" rel="noopener" href="https://github.com/Mirantis/cri-dockerd">cri-dockerd</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/Mirantis/cri-dockerd.git</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Run these commands as root</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##Install GO###</span></span></span><br><span class="line">wget https://storage.googleapis.com/golang/getgo/installer_linux</span><br><span class="line">chmod +x ./installer_linux</span><br><span class="line">./installer_linux</span><br><span class="line">source ~/.bash_profile</span><br><span class="line"></span><br><span class="line">cd cri-dockerd</span><br><span class="line">mkdir bin</span><br><span class="line">go build -o bin/cri-dockerd</span><br><span class="line">mkdir -p /usr/local/bin</span><br><span class="line">install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd</span><br><span class="line">cp -a packaging/systemd/* /etc/systemd/system</span><br><span class="line">sed -i -e &#x27;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 使用 iptables 替换 firewalld</span></span></span><br><span class="line">sed -i -e &#x27;s,firewalld.service,iptables.service,&#x27; /etc/systemd/system/cri-docker.service</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable --now cri-docker.service</span><br><span class="line">systemctl enable --now cri-docker.socket</span><br></pre></td></tr></table></figure>

<p>对于 <code>cri-dockerd</code>，默认情况下，CRI 套接字是 <code>/run/cri-dockerd.sock</code></p>
<h1 id="Kubernetes-安装配置"><a href="#Kubernetes-安装配置" class="headerlink" title="Kubernetes 安装配置"></a>Kubernetes 安装配置</h1><h2 id="安装-kubeadm、kubelet-和-kubectl"><a href="#安装-kubeadm、kubelet-和-kubectl" class="headerlink" title="安装 kubeadm、kubelet 和 kubectl"></a>安装 kubeadm、kubelet 和 kubectl</h2><p>需要在每台机器上安装以下的软件包：</p>
<ul>
<li><p><code>kubeadm</code> ： 用来初始化集群的指令。</p>
</li>
<li><p><code>kubelet</code> ： 在集群中的每个节点上用来启动 Pod 和容器等。</p>
</li>
<li><p><code>kubectl</code> ： 用来与集群通信的命令行工具。</p>
</li>
</ul>
<p>添加 <code>yum</code> 源</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">exclude=kubelet kubeadm kubectl</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>安装软件包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y kubelet-1.24.7 kubeadm-1.24.7 kubectl-1.24.7 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<p>启动服务并配置开机启动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl enable --now kubelet</span><br></pre></td></tr></table></figure>
<p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 <code>kubeadm</code> 指令的死循环。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl status kubelet</span></span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /usr/lib/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br><span class="line">   Active: activating (auto-restart) (Result: exit-code) since Mon 2022-09-12 14:35:58 CST; 7s ago</span><br><span class="line">     Docs: https://kubernetes.io/docs/</span><br><span class="line">  Process: 2056 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 2056 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Sep 12 14:35:58 ip-172-31-14-115.us-west-1.compute.internal systemd[1]: Unit kubelet.service entered failed state.</span><br><span class="line">Sep 12 14:35:58 ip-172-31-14-115.us-west-1.compute.internal systemd[1]: kubelet.service failed.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="初始化控制平面节点"><a href="#初始化控制平面节点" class="headerlink" title="初始化控制平面节点"></a>初始化控制平面节点</h2><h3 id="创建单控制平面集群"><a href="#创建单控制平面集群" class="headerlink" title="创建单控制平面集群"></a>创建单控制平面集群</h3><p>控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server （命令行工具 kubectl 与之通信）。</p>
<ol>
<li><p>初始化控制平面节点</p>
<p> 要初始化控制平面节点，请在 master 节点上（<code>kubernetes1</code>）运行以下命令，<a href="(https://csms.tech%3C!--swig%EF%BF%BC94--%3E#%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4)">命令参数说明</a>：</p>
 <figure class="highlight shell"><figcaption><span>kubernetes1</span></figcaption><table><tr><td class="code"><pre><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
<ul>
<li><code>--pod-network-cidr=10.244.0.0/16</code> 指定 pod 使用的网络段，后面配置网络（CNI）时配置的网段要和此处一致</li>
<li><code>--cri-socket=unix:///var/run/cri-dockerd.sock</code> 指定使用的 CRI 为 Docker</li>
</ul>
<p> 输出结果如下：<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.25.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubernetes1] and IPs [10.96.0.1 172.31.10.19]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [localhost kubernetes1] and IPs [172.31.10.19 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [localhost kubernetes1] and IPs [172.31.10.19 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 17.003297 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node kubernetes1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]</span><br><span class="line">[mark-control-plane] Marking the node kubernetes1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: 8ca35s.butdpihinkdczvqb</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, if you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">  beadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6e4fef99fa2407241e1a0e8ea652149 </span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p> 根据 <code>kubeadm init</code> 输出提示，配置 <code>kubectl</code> 需要的环境变量，root 用户执行以下命令<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure><br> 为永久生效，可将其添加到 <code>~/.bash_profile</code></p>
<p> 此时，执行以下命令查看集群节点信息<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-565d847f94-dc8tl                  0/1     Pending   0          5m42s</span><br><span class="line">kube-system   coredns-565d847f94-zqctg                  0/1     Pending   0          5m42s</span><br><span class="line">kube-system   etcd-kubernetes1                      1/1     Running   0          5m54s</span><br><span class="line">kube-system   kube-apiserver-kubernetes1            1/1     Running   0          5m53s</span><br><span class="line">kube-system   kube-controller-manager-kubernetes1   1/1     Running   0          5m54s</span><br><span class="line">kube-system   kube-proxy-6kwdx                          1/1     Running   0          5m43s</span><br><span class="line">kube-system   kube-scheduler-kubernetes1            1/1     Running   0          5m54s</span><br><span class="line"></span><br></pre></td></tr></table></figure><br> 其中，<code>coredns</code> 的 pod 处于 <code>Pending</code> 状态，是因为网络还没配置。</p>
<p> 因为 CRI 使用 docker，此时使用以下命令，可以查看到启动的所有容器<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker ps -a</span></span><br><span class="line">CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS     NAMES</span><br><span class="line">741a6d32b2cd   58a9a0c6d96f           &quot;/usr/local/bin/kube…&quot;   5 minutes ago   Up 5 minutes             k8s_kube-proxy_kube-proxy-6kwdx_kube-system_93101b10-7ee5-437c-a234-3e31edc7cfa9_0</span><br><span class="line">31509b3f06cc   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 5 minutes ago   Up 5 minutes             k8s_POD_kube-proxy-6kwdx_kube-system_93101b10-7ee5-437c-a234-3e31edc7cfa9_0</span><br><span class="line">fb3ec15950b6   bef2cf311509           &quot;kube-scheduler --au…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-scheduler_kube-scheduler-kubernetes1_kube-system_c455960b65afeadd009ff9ba9e7ab7b0_0</span><br><span class="line">333188677c01   4d2edfd10d3e           &quot;kube-apiserver --ad…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-apiserver_kube-apiserver-kubernetes1_kube-system_11596873d958a699a1b923df2333eaad_0</span><br><span class="line">4bdbf8689bbb   1a54c86c03a6           &quot;kube-controller-man…&quot;   6 minutes ago   Up 6 minutes             k8s_kube-controller-manager_kube-controller-manager-kubernetes1_kube-system_23ce2f60ac97b06bde25c1662e88e409_0</span><br><span class="line">a399d3484c17   a8a176a5d5d6           &quot;etcd --advertise-cl…&quot;   6 minutes ago   Up 6 minutes             k8s_etcd_etcd-kubernetes1_kube-system_84da44e552601c02573afe1dc1e3b0a2_0</span><br><span class="line">28aae0e41a7d   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-apiserver-kubernetes1_kube-system_11596873d958a699a1b923df2333eaad_0</span><br><span class="line">3f4f378ed731   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-scheduler-kubernetes1_kube-system_c455960b65afeadd009ff9ba9e7ab7b0_0</span><br><span class="line">eaa6d312a174   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_etcd-kubernetes1_kube-system_84da44e552601c02573afe1dc1e3b0a2_0</span><br><span class="line">707e84291ac2   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 6 minutes ago   Up 6 minutes             k8s_POD_kube-controller-manager-kubernetes1_kube-system_23ce2f60ac97b06bde25c1662e88e409_0</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
</li>
<li><p><a href="#%E5%AE%89%E8%A3%85-kube-flannel">安装 kube-flannel</a></p>
<p> 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。</p>
</li>
<li><p>将节点加入集群</p>
<p> 在 work 节点上执行以下命令加入集群</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6 \ </span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
<p> 加入集群成功后，在 master 上查看所有节点</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME              STATUS     ROLES           AGE   VERSION</span><br><span class="line">kubernetes1   Ready      control-plane   36m   v1.25.0</span><br><span class="line">kubernetes2   NotReady   &lt;none&gt;          21s   v1.25.0</span><br><span class="line">kubernetes3   NotReady   &lt;none&gt;          18s   v1.25.0</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="创建高可用控制平面的集群"><a href="#创建高可用控制平面的集群" class="headerlink" title="创建高可用控制平面的集群"></a>创建高可用控制平面的集群</h3><p>创建 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology"><code>堆叠（Stacked）etcd 拓扑</code></a> 的高可用控制平面集群</p>
<p><code>堆叠（Stacked）etcd 拓扑</code> 主要有以下特点：</p>
<ul>
<li><code>etcd</code> 分布式数据存储集群堆叠在 <code>kubeadm</code> 管理的控制平面节点上，作为控制平面的一个组件运行。</li>
<li>每个控制平面节点运行 <code>etcd</code>、 <code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。 <code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</li>
<li>每个控制平面节点创建一个本地 <code>etcd</code> 成员（member），这个 <code>etcd</code> 成员只与该节点的 <code>kube-apiserver</code> 通信。 这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</li>
<li>堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 <code>etcd</code> 成员和控制平面实例都将丢失， 并且冗余会受到影响。你可以通过添加更多控制平面节点来降低此风险。</li>
</ul>
<p><code>堆叠（Stacked）etcd 拓扑</code><br><img src="https://i.csms.tech/img_77.png"></p>
<p>为 kube-apiserver 创建负载均衡器，该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。 API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 :6443） 上进行的一个 TCP 检查。 <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[为 kube-apiserver 创建负载均衡器](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/#%E4%B8%BA-kube-apiserver-%E5%88%9B%E5%BB%BA%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8)">[2]</span></a></sup></p>
<p>此处假设 kube-apiserver 的负载均衡地址为 <code>kube-apiserver.my.com:6443</code>。</p>
<ol>
<li><p>初始化控制平面：</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock \</span><br><span class="line">             --control-plane-endpoint &quot;kube-apiserver.my.com:6443&quot; \</span><br><span class="line">             --upload-certs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。</p>
<p> 根据 <code>kubeadm init</code> 输出提示，配置 <code>kubectl</code> 需要的环境变量，root 用户执行以下命令</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure>
<p> 为永久生效，可将其添加到 <code>~/.bash_profile</code></p>
</li>
<li><p><a href="#%E5%AE%89%E8%A3%85-kube-flannel">安装 kube-flannel</a></p>
<p> 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。</p>
<p> 输入以下内容，并查看控制平面组件的 Pod 启动：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system -w</span><br></pre></td></tr></table></figure></li>
<li><p>其余控制平面节点上的操作</p>
<p> 执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给你的 <code>join</code> 命令。 在 CRI 是 <code>cri-dockerd</code> 的场景下，要添加 <code>--cri-socket=unix:///var/run/cri-dockerd.sock</code>。它看起来应该像这样：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv \</span><br><span class="line">             --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 \</span><br><span class="line">             --control-plane \</span><br><span class="line">             --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 \</span><br><span class="line">             --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>
</li>
<li><p>工作节点上的操作</p>
<p> 在工作节点上执行以下命令，添加工作节点到集群中。在 CRI 是 <code>cri-dockerd</code> 的场景下，要添加 <code>--cri-socket=unix:///var/run/cri-dockerd.sock</code>。它看起来应该像这样：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm join kube-apiserver.uat.148962587001:6443 \</span><br><span class="line">        --token 0nf24o.fb98ll5qkhpcxd70 \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:a5d589a3476777df757e38334b035a93811d94e75131e3d9cc1d7efad22fc793 \</span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="安装-kube-flannel"><a href="#安装-kube-flannel" class="headerlink" title="安装 kube-flannel"></a>安装 kube-flannel</h3><p>Kubernetes 安装时已经安装了网络相关驱动，位于 <code>/opt/cni/bin/flannel</code>，此时只需要根据相关配置文件生成 <code>kube-flannel</code> 的 pod 即可</p>
<p>请在 master 节点上（<code>kubernetes1</code>）运行以下命令创建 <code>kube-flannel</code> 相关 POD</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml</span></span><br><span class="line"></span><br><span class="line">namespace/kube-flannel created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.apps/kube-flannel-ds created</span><br></pre></td></tr></table></figure>
<p>使用默认的 <code>kube-flannel.yml</code>，默认的 Network 为 <code>10.244.0.0/16</code>，要变更默认网段，更改 <code>kube-flannel.yml</code> 中的以下内容即可：</p>
<figure class="highlight shell"><figcaption><span>kube-flannel.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">net-conf.json: |</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span><br><span class="line">    &quot;Backend&quot;: &#123;</span><br><span class="line">      &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此处的网段配置需要和 <a href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E8%8A%82%E7%82%B9">初始化集群时定义的 pod 网段</a> 保持一致</p>
<p>创建完成 <code>kube-flannel</code> 后，再次查看集群中的 pod 信息，可以看到 <code>coredns</code> 已经处于运行状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-rg969                     1/1     Running   0          20s</span><br><span class="line">kube-system    coredns-565d847f94-dc8tl                  0/1     Running   0          22m</span><br><span class="line">kube-system    coredns-565d847f94-zqctg                  0/1     Running   0          22m</span><br><span class="line">kube-system    etcd-kubernetes1                      1/1     Running   0          22m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1            1/1     Running   0          22m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1   1/1     Running   0          22m</span><br><span class="line">kube-system    kube-proxy-6kwdx                          1/1     Running   0          22m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1            1/1     Running   0          22m</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="安装-dashboard"><a href="#安装-dashboard" class="headerlink" title="安装 dashboard"></a>安装 dashboard</h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard">kubernetes-dashboard 项目地址</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure>

<p>修改 <code>recommended.yaml</code> 以下内容</p>
<figure class="highlight shell"><figcaption><span>recommended.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort     </span><br><span class="line">  ports:</span><br><span class="line">    - port: 443</span><br><span class="line">      targetPort: 8443</span><br><span class="line">      nodePort: 30443</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上修改主要是新加以下 2 行，配置对外的端口，可用范围为 30000-32767：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type: NodePort </span><br><span class="line">    nodePort: 30443</span><br></pre></td></tr></table></figure>
<p>以下 2 处新增配置 <code>nodeName: kubernetes1</code>，其中 <code>kubernetes1</code> 为 master 节点名称，可以通过 <code>kubectl get nodes</code> 查看</p>
<figure class="highlight shell"><figcaption><span>recommended.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  securityContext:</span><br><span class="line">    seccompProfile:</span><br><span class="line">      type: RuntimeDefault</span><br><span class="line">  nodeName: kubernetes1</span><br><span class="line">  containers:</span><br><span class="line">    - name: kubernetes-dashboard</span><br><span class="line">      image: kubernetesui/dashboard:v2.6.1</span><br><span class="line">      imagePullPolicy: Always</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">          protocol: TCP</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以上 2 处修改主要是配置 <code>kubernetes-dashboard</code> 运行在 master 节点上，否则可能运行在其他节点上，会因为网络问题导致 <code>kubernetes-dashboard</code> 无法正常启动，查看日志会报以下错误：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs -n kubernetes-dashboard kubernetes-dashboard-66c887f759-5rbbb</span></span><br><span class="line">2022/09/14 06:59:17 Starting overwatch</span><br><span class="line">2022/09/14 06:59:17 Using namespace: kubernetes-dashboard</span><br><span class="line">2022/09/14 06:59:17 Using in-cluster config to connect to apiserver</span><br><span class="line">2022/09/14 06:59:17 Using secret token for csrf signing</span><br><span class="line">2022/09/14 06:59:17 Initializing csrf token from kubernetes-dashboard-csrf secret</span><br><span class="line">panic: Get &quot;https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf&quot;: dial tcp 10.96.0.1:443: connect: no route to host</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键日志 ： <code>panic: Get &quot;https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf&quot;: dial tcp 10.96.0.1:443: connect: no route to host </code></p>
<p>以上报错也有可能是因为防火墙未放通各个 <code>service</code> 的 <code>CLUSTER-IP</code> 网段导致，可以在防火墙中放通相应网段</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get services -A</span></span><br><span class="line">NAMESPACE              NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">default                kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                  3h9m</span><br><span class="line">kube-system            kube-dns                    ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   3h9m</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.107.39.231   &lt;none&gt;        8000/TCP                 88m</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard        NodePort    10.101.165.61   &lt;none&gt;        443:30443/TCP            88m</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>iptables</code> 中放通对应网段</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -I INPUT 6 -s 10.0.0.0/8 -j ACCEPT</span><br></pre></td></tr></table></figure>

<p>使用修改后的配置文件 <code>recommended.yaml</code> 启动 <code>kubernetes-dashboard</code> pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f recommended.yaml</span></span><br><span class="line"></span><br><span class="line">namespace/kubernetes-dashboard created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">secret/kubernetes-dashboard-csrf created</span><br><span class="line">secret/kubernetes-dashboard-key-holder created</span><br><span class="line">configmap/kubernetes-dashboard-settings created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/dashboard-metrics-scraper created</span><br><span class="line">deployment.apps/dashboard-metrics-scraper created</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看 pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE</span><br><span class="line">kube-flannel           kube-flannel-ds-bdms5                        1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-flannel           kube-flannel-ds-kq7gz                        1/1     Running   1             19h</span><br><span class="line">kube-flannel           kube-flannel-ds-rg969                        1/1     Running   2 (17h ago)   19h</span><br><span class="line">kube-system            coredns-565d847f94-dc8tl                     1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            coredns-565d847f94-zqctg                     1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            etcd-kubernetes1                         1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-apiserver-kubernetes1               1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-controller-manager-kubernetes1      1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-proxy-6kwdx                             1/1     Running   2 (17h ago)   20h</span><br><span class="line">kube-system            kube-proxy-7lk7c                             1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-system            kube-proxy-rjr76                             1/1     Running   1 (17h ago)   19h</span><br><span class="line">kube-system            kube-scheduler-kubernetes1               1/1     Running   3 (17h ago)   20h</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-746f6b45bf-ndvbr   1/1     Running   0             40s</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-64f444d4f9-2mjdb        1/1     Running   0             40s</span><br></pre></td></tr></table></figure>

<p><code>kubernetes-dashboard</code> 运行正常后，在防火墙放通 <code>kubernetes-dashboard</code> 对外的端口（30443）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -I INPUT 6 -p tcp --dport 30443  -j ACCEPT</span><br></pre></td></tr></table></figure>

<p>浏览器通过访问 master 节点的公网 ip 地址和端口（<a href="https://ip:30443）">https://ip:30443）</a> ，可以打开 <code>kubernetes-dashboard</code> web 界面<br><img src="https://i.csms.tech/img_56.png"></p>
<p>此时要验证 Token。需要首先创建管理员用户，创建以下配置文件，文件命名为 <code>kubernetes-dashboard-adminuser.yaml</code>，<a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">参考文档</a></p>
<figure class="highlight shell"><figcaption><span>kubernetes-dashboard-adminuser.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以上配置创建了一个 <code>admin</code> 用户（用户名字随便起），赋予 <code>ClusterRoleBinding</code> 角色权限，关联到 <code>clusert-admin</code>（名称是固定的不能修改）。</p>
<p>根据此配置创建账号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f kubernetes-dashboard-adminuser.yaml</span></span><br><span class="line">serviceaccount/admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/admin created</span><br></pre></td></tr></table></figure>

<p>获取 <code>Bearer Token</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl -n kubernetes-dashboard create token admin</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6InUxMDNNUmZmU3BFenZYTEZjNjk2LUR0S1Q..</span><br></pre></td></tr></table></figure>
<p>将生成的 <code>Token</code> 输入浏览器进行验证，验证成功后可以登入 Dashboard<br><img src="https://i.csms.tech/img_57.png"></p>
<p>默认的 token 有效期很短，要修改 token 的有限时间，可以在登陆 Dashboard 后，编辑 <code>kubernetes-dashboard</code> 的 <code>Deployment</code><br><img src="https://i.csms.tech/img_58.png"></p>
<p>在 <code>spec:template:spec:containers:args</code> 下新增 <code>- &#39;--token-ttl=2592000&#39;</code><br><img src="https://i.csms.tech/img_59.png"></p>
<h1 id="安装-Kubernetes-Metrics-Server"><a href="#安装-Kubernetes-Metrics-Server" class="headerlink" title="安装 Kubernetes Metrics Server"></a>安装 Kubernetes Metrics Server</h1><p>安装 <code>Kubernetes Metrics Server</code> 可以支持使用 <code>kubectl top</code> 命令来查看集群使用的资源情况。 <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Kubernetes Metrics Server](https://github.com/kubernetes-sigs/metrics-server)">[5]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</span><br><span class="line">mv components.yaml kubernetes-mitrics-server.yaml</span><br><span class="line">kubectl apply -f kubernetes-mitrics-server.yaml</span><br></pre></td></tr></table></figure>
<p>部署后为了解决证书问题，可以临时配置不使用安全证书进行通信，修改 <code>metrics-server</code> 的 <code>Deployment</code>，在 <code>metrics-server</code> 启动时添加参数 <code>--kubelet-insecure-tls</code></p>
<figure class="highlight shell"><figcaption><span>kubernetes-mitrics-server.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - args:</span><br><span class="line">    - --cert-dir=/tmp</span><br><span class="line">    - --secure-port=4443</span><br><span class="line">    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><br><span class="line">    - --kubelet-use-node-status-port</span><br><span class="line">    - --metric-resolution=15s</span><br><span class="line">    - --kubelet-insecure-tls</span><br></pre></td></tr></table></figure>

<h1 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h1><h2 id="Found-multiple-CRI-endpoints-on-the-host"><a href="#Found-multiple-CRI-endpoints-on-the-host" class="headerlink" title="Found multiple CRI endpoints on the host"></a>Found multiple CRI endpoints on the host</h2><p><strong>错误场景</strong> ： 执行以下命令将节点加入集群时报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm <span class="built_in">join</span> 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042</span></span><br><span class="line"></span><br><span class="line">Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the &#x27;criSocket&#x27; field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>报错原因</strong> ： 在没有明确指定 Kubernetes 要使用的 CRI 情况下，会自动扫描主机上面安装的 CRI，如果出现多个可用的 CRI，会报错并提示确定使用哪个 CRI。</p>
<p><strong>解决方法</strong> ： 使用如下命令，指定要使用的 CRI</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join 172.31.10.19:6443 --token 8ca35s.butdpihinkdczvqb --discovery-token-ca-cert-hash sha256:b2793f9a6bea44a64640f99042f11c4ff6 \ </span><br><span class="line">        --cri-socket=unix:///var/run/cri-dockerd.sock</span><br></pre></td></tr></table></figure>

<h2 id="kube-flannel-状态为-CrashLoopBackOff"><a href="#kube-flannel-状态为-CrashLoopBackOff" class="headerlink" title="kube-flannel 状态为 CrashLoopBackOff"></a>kube-flannel 状态为 CrashLoopBackOff</h2><p><strong>错误场景</strong> ：<br><code>kube-flannel</code> 一直重启，状态为 <code>CrashLoopBackOff</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods --all-namespaces</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS              RESTARTS         AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-7q2hp                     0/1     CrashLoopBackOff    40 (3m39s ago)   3h4m</span><br><span class="line">kube-flannel   kube-flannel-ds-k8wd6                     0/1     CrashLoopBackOff    35 (53s ago)     76m</span><br><span class="line">kube-flannel   kube-flannel-ds-x6ck2                     0/1     CrashLoopBackOff    18 (106s ago)    69m</span><br><span class="line">kube-system    coredns-565d847f94-b4sgn                  0/1     ContainerCreating   0                3h40m</span><br><span class="line">kube-system    coredns-565d847f94-ml6k5                  0/1     ContainerCreating   0                3h40m</span><br><span class="line">kube-system    etcd-kubernetes1                      1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1            1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1   1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-proxy-9vwxl                          1/1     Running             0                76m</span><br><span class="line">kube-system    kube-proxy-qxsc7                          1/1     Running             0                69m</span><br><span class="line">kube-system    kube-proxy-v5msf                          1/1     Running             0                3h40m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1            1/1     Running             0                3h40m</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>排查步骤</strong> ：<br>查看日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs kube-flannel-ds-7q2hp -n kube-flannel</span></span><br><span class="line">Defaulted container &quot;kube-flannel&quot; out of: kube-flannel, install-cni-plugin (init), install-cni (init)</span><br><span class="line">I0913 06:42:19.799473       1 main.go:207] CLI flags config: &#123;etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true&#125;</span><br><span class="line">W0913 06:42:19.799563       1 client_config.go:614] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.</span><br><span class="line">I0913 06:42:19.903750       1 kube.go:120] Waiting 10m0s for node controller to sync</span><br><span class="line">I0913 06:42:19.903882       1 kube.go:401] Starting kube subnet manager</span><br><span class="line">I0913 06:42:20.903967       1 kube.go:127] Node controller sync successful</span><br><span class="line">I0913 06:42:20.903995       1 main.go:227] Created subnet manager: Kubernetes Subnet Manager - kubernetes1</span><br><span class="line">I0913 06:42:20.904004       1 main.go:230] Installing signal handlers</span><br><span class="line">I0913 06:42:20.904152       1 main.go:467] Found network config - Backend type: vxlan</span><br><span class="line">I0913 06:42:20.904195       1 match.go:206] Determining IP address of default interface</span><br><span class="line">I0913 06:42:20.904542       1 match.go:259] Using interface with name eth0 and address 172.31.10.19</span><br><span class="line">I0913 06:42:20.904570       1 match.go:281] Defaulting external address to interface address (172.31.10.19)</span><br><span class="line">I0913 06:42:20.904651       1 vxlan.go:138] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</span><br><span class="line">E0913 06:42:20.904962       1 main.go:330] Error registering network: failed to acquire lease: node &quot;kubernetes1&quot; pod cidr not assigned</span><br><span class="line">I0913 06:42:20.905100       1 main.go:447] Stopping shutdownHandler...</span><br><span class="line">W0913 06:42:20.905251       1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:402: watch of *v1.Node ended with: an error on the server (&quot;unable to decode an event from the watch stream: context canceled&quot;) has prevented the request from succeeding</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关键日志： <code>Error registering network: failed to acquire lease: node &quot;kubernetes1&quot; pod cidr not assigned</code></p>
<p><strong>问题原因</strong> ： worker 节点的 flannel 组件无法正常获取 podCIDR 的定义</p>
<p><strong>解决方法</strong> ： 编辑控制节点上的配置文件 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code>，在 <code>- command</code> 下添加以下内容：</p>
<figure class="highlight shell"><figcaption><span>/etc/kubernetes/manifests/kube-controller-manager.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line">- --allocate-node-cidrs=true</span><br><span class="line">- --cluster-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure>
<p>如果内容已存在的话，更改 cidr 的网段和 <a href="#%E5%AE%89%E8%A3%85-kube-flannel"><code>kube-flannel.yml</code> 中的 cidr</a> 一致</p>
<p>更改配置后，重启所有节点的 <code>kubelet</code> 服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<p>重新查看所有 pod 状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                      READY   STATUS    RESTARTS         AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-7q2hp                     1/1     Running   62 (2m20s ago)   4h43m</span><br><span class="line">kube-flannel   kube-flannel-ds-k8wd6                     1/1     Running   55 (4m33s ago)   175m</span><br><span class="line">kube-flannel   kube-flannel-ds-x6ck2                     1/1     Running   38 (2m43s ago)   168m</span><br><span class="line">kube-system    coredns-565d847f94-b4sgn                  0/1     Running   0                5h19m</span><br><span class="line">kube-system    coredns-565d847f94-ml6k5                  0/1     Running   0                5h19m</span><br><span class="line">kube-system    etcd-kubernetes1                      1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-apiserver-kubernetes1            1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-controller-manager-kubernetes1   1/1     Running   0                2m39s</span><br><span class="line">kube-system    kube-proxy-9vwxl                          1/1     Running   0                175m</span><br><span class="line">kube-system    kube-proxy-qxsc7                          1/1     Running   0                168m</span><br><span class="line">kube-system    kube-proxy-v5msf                          1/1     Running   0                5h19m</span><br><span class="line">kube-system    kube-scheduler-kubernetes1            1/1     Running   0                5h19m</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="master-节点状态为-NotReady"><a href="#master-节点状态为-NotReady" class="headerlink" title="master 节点状态为 NotReady"></a>master 节点状态为 NotReady</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">NAME              STATUS     ROLES                  AGE    VERSION</span><br><span class="line">k8s-work1         Ready      &lt;none&gt;                 99m    v1.21.2</span><br><span class="line">k8s-master        NotReady   control-plane,master   102m   v1.21.2</span><br><span class="line">k8s-work2         Ready      &lt;none&gt;                 99m    v1.21.2</span><br></pre></td></tr></table></figure>

<p>查看节点详细信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node k8s-master</span></span><br><span class="line"></span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message</span><br><span class="line">  ----             ------    -----------------                 ------------------                ------              -------</span><br><span class="line">  MemoryPressure   Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  DiskPressure     Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  PIDPressure      Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line">  Ready            Unknown   Tue, 11 Oct 2022 14:54:19 +0800   Tue, 11 Oct 2022 14:57:46 +0800   NodeStatusUnknown   Kubelet stopped posting node status.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看 <code>Pod</code> 状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -A</span></span><br><span class="line">NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-84s24                0/1     Pending   0          22m</span><br><span class="line">kube-flannel   kube-flannel-ds-qzd9g                1/1     Running   1          22m</span><br><span class="line">kube-flannel   kube-flannel-ds-sbtrr                1/1     Running   1          22m</span><br><span class="line">kube-system    coredns-558bd4d5db-8mbl5             1/1     Running   1          105m</span><br><span class="line">kube-system    coredns-558bd4d5db-gzrrx             1/1     Running   1          105m</span><br><span class="line">kube-system    etcd-k8s-master             1/1     Running   0          105m</span><br><span class="line">kube-system    kube-apiserver-k8s-master            1/1     Running   0          105m</span><br><span class="line">kube-system    kube-proxy-747cx                     1/1     Running   1          103m</span><br><span class="line">kube-system    kube-proxy-8bs8l                     1/1     Running   1          103m</span><br><span class="line">kube-system    kube-proxy-mvqjq                     1/1     Running   0          105m</span><br><span class="line">kube-system    kube-scheduler-k8s-master            1/1     Running   0          105m</span><br></pre></td></tr></table></figure>
<p>结果显示 <code>kube-flannel</code> 位于 master 上的 <code>Pod</code> 状态异常。</p>
<p>查看 <code>kubelet</code> 日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -f -u kubelet</span></span><br><span class="line"> E1011 16:28:53.898132     796 kubelet_node_status.go:93] &quot;Unable to register node with API server&quot; err=&quot;nodes \&quot;k8s-admin\&quot; is forbidden: node \&quot;k8s-master\&quot; is not allowed to modify node \&quot;k8s-admin\&quot;&quot; node=&quot;k8s-admin&quot;</span><br><span class="line"> E1011 16:28:53.900459     796 kubelet.go:2291] &quot;Error getting node&quot; err=&quot;node \&quot;k8s-admin\&quot; not found&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看主机名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">hostname status</span></span><br><span class="line"></span><br><span class="line">   Static hostname: k8s-master</span><br><span class="line">Transient hostname: k8s-admin</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: b1527a5456aab241a74a8a3dc31395c0</span><br><span class="line">           Boot ID: f8428003692349298cf2bb9efae8a664</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.76.1.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>根据 <code>kubelet</code> 日志，Kubenetes 节点名称和主机名不一致。修改节点主机名。</p>
<h2 id="集群之外的服务器使用-kubectl-报错"><a href="#集群之外的服务器使用-kubectl-报错" class="headerlink" title="集群之外的服务器使用 kubectl 报错"></a>集群之外的服务器使用 kubectl 报错</h2><p><strong>问题场景</strong>：</p>
<p>将集群的管理配置文件 (<code>/etc/kubernetes/admin.conf</code>) 拷贝到集群之外的服务器，并命名为指定文件 <code>~/.kube/config</code>，修改 <code>~/.kube/config</code> 中 <code>server</code> 的 IP 为 Kubernetes API Server 的实际 IP，使用 <code>kubectl</code> 命令时，报错 <code>Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 10.150.0.21, not </code>。</p>
<p><strong>问题原因</strong>：</p>
<p>报错表示，当使用安全端口 6443 访问 Kubernetes API Server 时，默认证书中的 DNS 包含了 API Server 服务的 CLUSTER-IP 和 服务器的 IP ，如果是云主机，则为云服务器的私有 IP，不包含其公网 IP，如果使用公网 IP 访问 6443 端口，会报此错误</p>
<p>通过以下命令，可以看到默认的 API Server 的 HTTPS 证书中包含的 DNS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> /etc/kubernetes/pki</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">openssl x509 -noout -text -<span class="keyword">in</span> apiserver.crt</span></span><br><span class="line">...</span><br><span class="line">    DNS:k8s-master, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:10.150.0.21</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><strong>解决方法</strong>：</p>
<ul>
<li><p>使用 <code>kubectl</code> 的命令行选项 <code>--insecure-skip-tls-verify</code> 可跳过证书验证。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 10.150.0.21, not 34.150.1.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes --insecure-skip-tls-verify</span></span><br><span class="line"></span><br><span class="line">NAME         STATUS   ROLES                  AGE    VERSION</span><br><span class="line">k8s-master   Ready    control-plane,master   6d5h   v1.21.2</span><br><span class="line">k8s-work1    Ready    &lt;none&gt;                 6d5h   v1.21.2</span><br><span class="line">k8s-work2    Ready    &lt;none&gt;                 6d5h   v1.21.2</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>修改 <code>~/.kube/config</code> 中 <code>server</code> 地址为证书中包含的 DNS 名称，如 <code>k8s-master</code>，并确保域名本地可解析</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">grep server .kube/config</span></span><br><span class="line">  server: https://k8s-master:6443</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes</span></span><br><span class="line">  NAME         STATUS   ROLES                  AGE    VERSION</span><br><span class="line">  k8s-master   Ready    control-plane,master   6d6h   v1.21.2</span><br><span class="line">  k8s-work1    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line">  k8s-work2    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新生成 API Server 证书</p>
<ol>
<li><p>备份当前证书</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /etc/kubernetes/pki</span><br><span class="line">mkdir /data/k8s/backup/pki</span><br><span class="line">mv apiserver.* /data/k8s/backup/pki/</span><br></pre></td></tr></table></figure></li>
<li><p>生成新的 API Server 证书</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"> $ </span><span class="language-bash">kubeadm init phase certs apiserver \</span></span><br><span class="line"><span class="language-bash">         --apiserver-advertise-address  10.150.0.21 \</span></span><br><span class="line"><span class="language-bash">         --apiserver-cert-extra-sans  10.96.0.1 \</span></span><br><span class="line"><span class="language-bash">         --apiserver-cert-extra-sans 34.150.1.1</span></span><br><span class="line">     </span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.21 34.150.1.1]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>10.150.0.21</code> - 为云主机的私有（内网） IP 地址，同时也是 <code>--apiserver-advertise-address</code>，必须要有</li>
<li><code>10.96.0.1</code> - 为 Kubernetes 集群中 API Server 对应的 Service 的 ClusterIP，必须要有</li>
<li><code>34.150.1.1</code> - 为云主机的公网（弹性） IP，是本次要添加的 IP</li>
</ul>
<p>如果还有其他 IP，可以参照格式 <code>--apiserver-cert-extra-sans 34.150.1.1</code> 添加。</p>
<p> 命令执行成功后，会生成新的证书。<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ls apiserver*</span><br><span class="line">   apiserver.crt              apiserver-etcd-client.key  apiserver-kubelet-client.crt</span><br><span class="line">   apiserver-etcd-client.crt  apiserver.key              apiserver-kubelet-client.key</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>重启 kubelet 服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证</p>
<p> 在远端服务器执行以下命令验证效果 </p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ rm -rf .kube/cache/</span><br><span class="line">     </span><br><span class="line">$ grep server .kube/config </span><br><span class="line">  server: https://34.150.1.1:6443</span><br><span class="line">     </span><br><span class="line">$ kubectl get nodes</span><br><span class="line">   NAME                  STATUS   ROLES                  AGE    VERSION</span><br><span class="line">   k8s-master   Ready    control-plane,master   6d6h   v1.21.2</span><br><span class="line">   k8s-work1    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line">   k8s-work2    Ready    &lt;none&gt;                 6d6h   v1.21.2</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="kubelet-启动失败"><a href="#kubelet-启动失败" class="headerlink" title="kubelet 启动失败"></a>kubelet 启动失败</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">journalctl -u kubelet</span></span><br><span class="line">&quot;command failed&quot; err=&quot;failed to run Kubelet: running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swas contained: [Filename\t\t\t\tType\t\tSize\tUsed\tPriority /dev/dm-1</span><br></pre></td></tr></table></figure>
<p>根据 kubelet 服务日志，导致失败的原因为系统启用了 swap，关闭 swap，重新启动正常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.9G        306M        120M        8.7M        1.5G        1.4G</span><br><span class="line">Swap:          3.9G        2.0M        3.9G</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">swapoff -a</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.9G        350M         71M        9.0M        1.5G        1.4G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>

<h1 id="其他常用配置"><a href="#其他常用配置" class="headerlink" title="其他常用配置"></a>其他常用配置</h1><h2 id="新增节点"><a href="#新增节点" class="headerlink" title="新增节点"></a>新增节点</h2><p>要为集群新增 worker 节点，参考以下步骤</p>
<ol>
<li>加入前准备工作<br><a href="#kubernetes-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%89%8D%E9%85%8D%E7%BD%AE">参考安装步骤，配置新节点</a></li>
<li>在集群的 master 节点执行以下命令，获取加入集群的命令<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm token create --print-join-command</span></span><br><span class="line">kubeadm join cluster:6443 --token cuxvrexi24aejb --discovery-token-ca-cert-hash sha256:9e5b71bc392</span><br></pre></td></tr></table></figure></li>
<li>在新节点上执行加入集群的命令 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubeadm join cluster:6443 --token cuxvrexi24aejb --discovery-token-ca-cert-hash sha256:9e5b71bc392</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="将-CRI-由-containerd-变更为-Docker"><a href="#将-CRI-由-containerd-变更为-Docker" class="headerlink" title="将 CRI 由 containerd 变更为 Docker"></a>将 CRI 由 <code>containerd</code> 变更为 <code>Docker</code></h2><p>编辑 <code>/var/lib/kubelet/kubeadm-flags.env</code> 文件，在该文件中可以添加 <code>kubelet</code> 启动参数，将 <code>--container-runtime-endpoint</code> 标志，设置为 <code>unix:///var/run/cri-dockerd.sock</code> <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[配置 kubelet 使用 cri-dockerd](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/#configure-the-kubelet-to-use-cri-dockerd)">[1]</span></a></sup></p>
<figure class="highlight shell"><figcaption><span>/var/lib/kubelet/kubeadm-flags.env</span></figcaption><table><tr><td class="code"><pre><span class="line">KUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.8&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>kubeadm</code> 工具将节点上的套接字存储为控制面上 <code>Node</code> 对象的注解。 要为每个被影响的节点更改此套接字：</p>
<ol>
<li>编辑 Node 对象的 YAML 表示：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/path/to/admin.conf kubectl edit no &lt;NODE_NAME&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li><code>/path/to/admin.conf</code> ：指向 <code>kubectl</code> 配置文件 <code>admin.conf</code> 的路径；</li>
<li><code>&lt;NODE_NAME&gt;</code> ：你要修改的节点的名称。</li>
</ul>
<ol start="2">
<li>将 <code>kubeadm.alpha.kubernetes.io/cri-socket</code> 标志更改为 <code>unix:///var/run/cri-dockerd.sock</code>；</li>
</ol>
<p>配置完成后，重启 kubelet</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet </span><br></pre></td></tr></table></figure>

<p>查看 node 使用的 CRI</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes -o wide</span></span><br><span class="line">NAME              STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">kubernetes1   Ready    control-plane   7d2h   v1.25.0   172.31.5.58    &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br><span class="line">kubernetes2   Ready    &lt;none&gt;          7d1h   v1.25.0   172.31.5.68    &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br><span class="line">kubernetes3   Ready    &lt;none&gt;          7d1h   v1.25.0   172.31.0.230   &lt;none&gt;        CentOS Linux 7 (Core)   5.4.212-1.el7.elrepo.x86_64   docker://20.10.18</span><br></pre></td></tr></table></figure>

<h2 id="修改-Service-可使用的-nodePort-端口范围"><a href="#修改-Service-可使用的-nodePort-端口范围" class="headerlink" title="修改 Service 可使用的 nodePort 端口范围"></a>修改 Service 可使用的 nodePort 端口范围</h2><p>默认情况下，<code>Service</code> 中可使用的 <code>nodePort</code> 端口的默认范围为 <code>30000-32767</code>，要修改此配置，参考以下步骤。</p>
<p>Master 节点上编辑 kube-apiserver 的 <code>Pod</code> 配置文件 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>，在 <code>.spec.containers.command</code> 下添加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- --service-node-port-range=1-65535</span><br></pre></td></tr></table></figure>
<p><img src="https://i.csms.tech/img_60.png"></p>
<p><code>apiserver</code> 是以静态 <code>Pod</code> 的形式运行，<code>/etc/kubernetes/manifests</code> 目录下是所有静态 <code>Pod</code> 文件的定义，<code>kubelet</code> 会监控该目录下文件的变动，只要发生变化，<code>Pod</code> 就会重建，响应相应的改动。所以我们修改 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> 文件，添加 <code>nodePort</code> 范围参数后会自动生效，无需进行其他操作</p>
<p><strong>高可用场景下，所有 Master 节点上都要修改，否则可能遇到部分时候依然报错： <code>nodePort: Invalid value: 65500: provided port is not in the valid range. The range of valid ports is 30000-32767</code></strong></p>
<h2 id="开启-corndns-日志记录"><a href="#开启-corndns-日志记录" class="headerlink" title="开启 corndns 日志记录"></a>开启 corndns 日志记录</h2><p>默认的 coredns 配置没有开启日志插件，这导致 kubernetes 集群中一些 dns 解析超时问题难以定位。要打开 coredns 的日志功能，可以通过以下命令开启日志功能</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl edit configmap -n kube-system  coredns</span><br></pre></td></tr></table></figure>
<p>添加以下配置：<br><img src="https://i.csms.tech/img_66.png"></p>
<p>接下来我们再使用命令查看日志，就可以看到 dns 解析的记录，无需重启 coredns</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl logs -f -n kube-system coredns-558bd4d5db-z6mst</span></span><br><span class="line">...</span><br><span class="line">[INFO] 10.244.2.23:39830 - 37988 &quot;A IN raw.githubusercontent.com.google.internal. udp 59 false 512&quot; NXDOMAIN qr,aa,rd,ra 164 0.000065843s</span><br><span class="line">[INFO] 10.244.2.23:56581 - 52144 &quot;A IN raw.githubusercontent.com. udp 43 false 512&quot; NOERROR qr,aa,rd,ra 207 0.000133489s</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="添加-Harbor-私有镜像仓库的认证信息"><a href="#添加-Harbor-私有镜像仓库的认证信息" class="headerlink" title="添加 Harbor 私有镜像仓库的认证信息"></a>添加 Harbor 私有镜像仓库的认证信息</h2><p>在命令行上提供凭证来创建 Secret <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[在命令行上提供凭证来创建 Secret](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line)">[3]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry $&#123;secretname&#125; \</span><br><span class="line">  --namespace default</span><br><span class="line">  --docker-server=https://index.docker.io/v1/ \</span><br><span class="line">  --docker-username=&lt;你的用户名&gt; \</span><br><span class="line">  --docker-password=&lt;你的密码&gt; \</span><br><span class="line">  --docker-email=&lt;你的邮箱地址&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>&lt;your-registry-server&gt;</code> 是你的私有 Docker 仓库全限定域名（FQDN）。 DockerHub 使用 <code>https://index.docker.io/v1/</code>。</li>
<li><code>&lt;your-name&gt;</code> 是你的 Docker 用户名。</li>
<li><code>&lt;your-pword&gt;</code> 是你的 Docker 密码。</li>
<li><code>&lt;your-email&gt;</code> 是你的 Docker 邮箱。</li>
</ul>
<p>这样你就成功地将集群中的 Docker 凭证设置为名为 <code>$&#123;secretname&#125;</code> 的 Secret。</p>
<p>Secret 属于 namespace 级别的资源，不能跨 namespace 使用。</p>
<p>检查创建的 Secret</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret <span class="variable">$&#123;secretname&#125;</span> --output=yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: eyJhdXRocfX19</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2022-11-04T08:55:51Z&quot;</span><br><span class="line">  name: $&#123;secretname&#125;</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;1679647&quot;</span><br><span class="line">  uid: a780ac6d-9525-4620-a171-b818021cc6ca</span><br><span class="line">type: kubernetes.io/dockerconfigjson</span><br></pre></td></tr></table></figure>

<p><code>.dockerconfigjson</code> 字段的值是 Docker 凭证的 base64 表示。要了解 <code>dockerconfigjson</code> 字段中的内容，请将 Secret 数据转换为可读格式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret <span class="variable">$&#123;secretname&#125;</span> --output=<span class="string">&quot;jsonpath=&#123;.data.\.dockerconfigjson&#125;&quot;</span> | <span class="built_in">base64</span> --decode</span></span><br><span class="line">&#123;&quot;auths&quot;:&#123;&quot;https://harbor1.my.com&quot;:&#123;&quot;username&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;password&quot;,&quot;email&quot;:&quot;docker@q.com&quot;,&quot;auth&quot;:&quot;YWRdsUQ==&quot;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="配置-Pod-拉取镜像的认证信息"><a href="#配置-Pod-拉取镜像的认证信息" class="headerlink" title="配置 Pod 拉取镜像的认证信息"></a>配置 Pod 拉取镜像的认证信息</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get secret -n frtg</span></span><br><span class="line">NAME                   TYPE                             DATA   AGE</span><br><span class="line">harbor1.1dergegh.com   kubernetes.io/dockerconfigjson   1      157d</span><br></pre></td></tr></table></figure>

<p>在 namespace 中配置了镜像仓库的 Secret 后，可以使用以下方法配置 Pod 拉取镜像时的认证信息</p>
<ol>
<li><p>在 Pod 的配置中使用 <code>imagePullSecrets</code> 指令，此种方式需要在每个 Pod 的配置中添加</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: $&#123;NAME&#125;</span><br><span class="line">  namespace: $&#123;NAMESPACE&#125;</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  containers:</span><br><span class="line">  - name: $&#123;NAME&#125;</span><br><span class="line">    image: nginx:1.14.2</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">      name: http-web</span><br><span class="line">      </span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: $&#123;secret_name&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>通过 ServiceAccount 配置</p>
<p> 每个 namespace 都有个默认的 <a href="/202305161451/" title="ServiceAccount">ServiceAccount</a>，namespace 中的所有 Pod 默认情况下都会关联到此 ServiceAccount，ServiceAccount 的配置中包含了 <code>Image pull secrets</code>，在 ServiceAccount 中添加的镜像拉取密钥，会自动添加到所有使用这个 ServiceAccount 的 Pod 中。因此，向 ServiceAccount 中添加镜像拉取密钥可以不必对每个 Pod 都单独进行镜像拉取密钥的配置。 <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[为服务账号添加 ImagePullSecrets](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)">[8]</span></a></sup></p>
<p> 执行命令 <code>kubectl edit serviceaccount default</code> 编辑默认的 ServiceAccount，删掉包含 <code>resourceVersion</code> 主键的行，添加包含 <code>imagePullSecrets:</code> 的行并保存文件</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2022-12-06T09:51:03Z&quot;</span><br><span class="line">  name: default</span><br><span class="line">  namespace: default</span><br><span class="line">  uid: b219bafc-e2f9-48bb-a9e4-6e0bfb4ab536</span><br><span class="line">imagePullSecrets:</span><br><span class="line">  - name: harbor1.1dergegh.com</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Pod-添加-hosts"><a href="#Pod-添加-hosts" class="headerlink" title="Pod 添加 hosts"></a>Pod 添加 hosts</h2><p>有时需要在启动 Pod 时为其 <code>/etc/hosts</code> 中添加解析，以覆盖对主机名的解析，此时可以通过 <code>PodSpec</code> 的 <code>HostAliases</code> 字段来 <strong>添加这些自定义条目</strong> <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[使用 HostAliases 向 Pod /etc/hosts 文件添加条目](https://kubernetes.io/zh-cn/docs/tasks/network/customize-hosts-file-for-pods/)">[4]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hostaliases-pod</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">  hostAliases:</span><br><span class="line">  - ip: &quot;127.0.0.1&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.local&quot;</span><br><span class="line">    - &quot;bar.local&quot;</span><br><span class="line">  - ip: &quot;10.1.2.3&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.remote&quot;</span><br><span class="line">    - &quot;bar.remote&quot;</span><br><span class="line">  containers:</span><br><span class="line">  - name: cat-hosts</span><br><span class="line">    image: busybox:1.28</span><br><span class="line">    command:</span><br><span class="line">    - cat</span><br><span class="line">    args:</span><br><span class="line">    - &quot;/etc/hosts&quot;</span><br></pre></td></tr></table></figure>

<h2 id="配置-Pod-中的时区和时间"><a href="#配置-Pod-中的时区和时间" class="headerlink" title="配置 Pod 中的时区和时间"></a>配置 Pod 中的时区和时间</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ssgeek/p/15192028.html">关于 OS 中的时间及时区的说明</a></p>
<p>通常情况下，我们的环境中，宿主机都是配置为 CST 时间（东八区），而使用的基础镜像中的默认时间都是 UTC 时间，而不是本地时间，通常需要确保系统中所有的时间格式一致，需将容器中的时间也修改为 CST 时间。</p>
<p>为此可使用以下方法中的一种来实现</p>
<ul>
<li><p>在 Dockerfile 中添加时区</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Set timezone</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span><br><span class="line">           &amp;&amp; echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone</span><br></pre></td></tr></table></figure></li>
<li><p>将时区文件挂载到 Pod 中</p>
<p>  在定义 Pod 上层控制器的时候，添加一个用于挂载时区的卷，挂载宿主机的时区文件</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">containers:</span><br><span class="line">- name: xxx</span><br><span class="line">  ...</span><br><span class="line">  volumeMounts:</span><br><span class="line">    - name: timezone</span><br><span class="line">      mountPath: /etc/localtime</span><br><span class="line">volumes:</span><br><span class="line">  - name: timezone</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /usr/share/zoneinfo/Asia/Shanghai</span><br></pre></td></tr></table></figure></li>
<li><p>通过环境变量定义时区</p>
<p>  在定义pod上层控制器的时候，添加一个用于指定时区的环境变量</p>
<p>  <code>TZ</code> 环境变量用于设置时区。它由各种时间函数用于计算相对于全球标准时间 UTC（以前称为格林威治标准时间 GMT）的时间。格式由操作系统指定</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">containers:</span><br><span class="line">- name: xxx</span><br><span class="line">  ...</span><br><span class="line">  env:</span><br><span class="line">  - name: TZ</span><br><span class="line">    value: Asia/Shanghai</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="配置节点允许启动的最大-Pod-数"><a href="#配置节点允许启动的最大-Pod-数" class="headerlink" title="配置节点允许启动的最大 Pod 数"></a>配置节点允许启动的最大 Pod 数</h2><p>在 K8S 集群中，默认每个 Worker 节点最大可创建 110 个 Pod，实际可以根据节点资源情况调整范围。</p>
<p>在 Woker 节点上，可创建的最大的 Pod 数量是作为 Kubelet 的启动参数出现的，因此修改 Kubelet 服务的配置文件增加 <code>--max-pod</code> 参数即可。</p>
<p>修改 <code>kubelet</code> 服务的启动文件 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，添加以下环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;KUBELET_NODE_MAX_PODS=--max-pods=200&quot;</span><br></pre></td></tr></table></figure>
<p>将新加的环境变量追加到 <code>/usr/bin/kubelet</code> 中</p>
<figure class="highlight shell"><figcaption><span>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span></figcaption><table><tr><td class="code"><pre><span class="line"> Note: This dropin only works with kubeadm and kubelet v1.11+</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">Environment=&quot;KUBELET_EVICT_NODEFS_THRESHOLD_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;</span><br><span class="line">Environment=&quot;KUBELET_NODE_MAX_PODS=--max-pods=200&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that <span class="string">&quot;kubeadm init&quot;</span> and <span class="string">&quot;kubeadm join&quot;</span> generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span></span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that the user can use <span class="keyword">for</span> overrides of the kubelet args as a last resort. Preferably, the user should use</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">the .NodeRegistration.KubeletExtraArgs object <span class="keyword">in</span> the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span></span><br><span class="line">EnvironmentFile=-/etc/sysconfig/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICT_NODEFS_THRESHOLD_ARGS $KUBELET_NODE_MAX_PODS</span><br></pre></td></tr></table></figure>

<h2 id="重置集群"><a href="#重置集群" class="headerlink" title="重置集群"></a>重置集群</h2><p>要重置集群配置，可以参考以下步骤 <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[清理](https://v1-25.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down)
">[7]</span></a></sup></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubeadm reset --cri-socket=unix:///var/run/cri-dockerd.sock</span></span><br><span class="line">[reset] Reading configuration from the cluster...</span><br><span class="line">[reset] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br><span class="line">W0428 15:31:57.928077   38272 reset.go:103] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: configmaps &quot;kubeadm-config&quot; not found</span><br><span class="line">W0428 15:31:57.928210   38272 preflight.go:55] [reset] WARNING: Changes made to this host by &#x27;kubeadm init&#x27; or &#x27;kubeadm join&#x27; will be reverted.</span><br><span class="line">[reset] Are you sure you want to proceed? [y/N]: y</span><br><span class="line">W0428 15:32:05.851830   38272 removeetcdmember.go:85] [reset] No kubeadm config, using etcd pod spec to get data directory</span><br><span class="line">[reset] Stopping the kubelet service</span><br><span class="line">[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;</span><br><span class="line">[reset] Deleting contents of directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]</span><br><span class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</span><br><span class="line">[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet]</span><br><span class="line"></span><br><span class="line">The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d</span><br><span class="line"></span><br><span class="line">The reset process does not reset or clean up iptables rules or IPVS tables.</span><br><span class="line">If you wish to reset iptables, you must do so manually by using the &quot;iptables&quot; command.</span><br><span class="line"></span><br><span class="line">If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)</span><br><span class="line">to reset your system&#x27;s IPVS tables.</span><br><span class="line"></span><br><span class="line">The reset process does not clean your kubeconfig files and you must remove them manually.</span><br><span class="line">Please, check the contents of the $HOME/.kube/config file.</span><br></pre></td></tr></table></figure>

<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/">Kubernetes 官网文档</a><br><a target="_blank" rel="noopener" href="https://github.com/Mirantis/cri-dockerd">cri-dockerd 安装链接</a><br><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844904148001882120">Centos7 集群部署k8s 版本v1.17.4及Dashboard </a><br><a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/tree/master/docs">kubernetes-dashboard 配置官网说明</a><br><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/">部署和访问 Kubernetes 仪表板（Dashboard）</a></p>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/#configure-the-kubelet-to-use-cri-dockerd">配置 kubelet 使用 cri-dockerd</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/#%E4%B8%BA-kube-apiserver-%E5%88%9B%E5%BB%BA%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8">为 kube-apiserver 创建负载均衡器</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line">在命令行上提供凭证来创建 Secret</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/network/customize-hosts-file-for-pods/">使用 HostAliases 向 Pod /etc/hosts 文件添加条目</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">Kubernetes Metrics Server</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://v1-25.docs.kubernetes.io/zh-cn/docs/reference/networking/ports-and-protocols/">端口和协议</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://v1-25.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down">清理</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">为服务账号添加 ImagePullSecrets</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="../tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i> Kubernetes</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="../202209051137/" rel="prev" title="docker compose 配置 django mysql 站点">
                  <i class="fa fa-chevron-left"></i> docker compose 配置 django mysql 站点
                </a>
            </div>
            <div class="post-nav-item">
                <a href="../202209131536/" rel="next" title="kubernetes 常用命令示例">
                  kubernetes 常用命令示例 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COSMOS</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="../js/comments.js"></script><script src="../js/utils.js"></script><script src="../js/motion.js"></script><script src="../js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="../js/third-party/search/local-search.js"></script>




  <script src="../js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fl9999","repo":"fl9999.github.io","client_id":"a11bf6f7860762b725b5","client_secret":"a99046105f8bddc72ec718d54dc3fd7f22070821","admin_user":"fl9999","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d41d8cd98f00b204e9800998ecf8427e"}</script>
<script src="../js/third-party/comments/gitalk.js"></script>

</body>
</html>
